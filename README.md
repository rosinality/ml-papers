# ML Papers
## Table of contents
1. [3d generative model](#3d-generative-model)
1. [activation](#activation)
1. [active learning](#active-learning)
1. [adaptation](#adaptation)
1. [adapter](#adapter)
1. [adversarial training](#adversarial-training)
1. [antialiasing](#antialiasing)
1. [asr](#asr)
1. [attention](#attention)
1. [audio generation](#audio-generation)
1. [audio source separation](#audio-source-separation)
1. [augmentation](#augmentation)
1. [autoregressive model](#autoregressive-model)
1. [backbone](#backbone)
1. [bayesian](#bayesian)
1. [bert](#bert)
1. [bias](#bias)
1. [calibration](#calibration)
1. [causality](#causality)
1. [channel attention](#channel-attention)
1. [chat](#chat)
1. [classificiation](#classificiation)
1. [computation](#computation)
1. [continual learning](#continual-learning)
1. [contrastive learning](#contrastive-learning)
1. [convolution](#convolution)
1. [dataset](#dataset)
1. [ddpm](#ddpm)
1. [decoding](#decoding)
1. [deep prior](#deep-prior)
1. [detr](#detr)
1. [dewarping](#dewarping)
1. [dialog](#dialog)
1. [differentiable operator](#differentiable-operator)
1. [differentiable tree](#differentiable-tree)
1. [discrete vae](#discrete-vae)
1. [disentangle](#disentangle)
1. [distillation](#distillation)
1. [distributed training](#distributed-training)
1. [domain adaptation](#domain-adaptation)
1. [dropout](#dropout)
1. [efficient attention](#efficient-attention)
1. [embedding](#embedding)
1. [end2end](#end2end)
1. [energy based model](#energy-based-model)
1. [ensemble](#ensemble)
1. [federated learning](#federated-learning)
1. [few shot](#few-shot)
1. [finetuning](#finetuning)
1. [flow](#flow)
1. [fpn](#fpn)
1. [gan](#gan)
1. [gan inversion](#gan-inversion)
1. [generalization](#generalization)
1. [generative model](#generative-model)
1. [graph](#graph)
1. [hallucination](#hallucination)
1. [hypernetwork](#hypernetwork)
1. [hyperparameter](#hyperparameter)
1. [identifiability](#identifiability)
1. [image editing](#image-editing)
1. [image generation](#image-generation)
1. [img2img](#img2img)
1. [implicit model](#implicit-model)
1. [implicit representation](#implicit-representation)
1. [instance segmentation](#instance-segmentation)
1. [interpolation](#interpolation)
1. [knowledge base](#knowledge-base)
1. [language generation](#language-generation)
1. [language model](#language-model)
1. [layout](#layout)
1. [lightweight](#lightweight)
1. [line](#line)
1. [lm](#lm)
1. [local attention](#local-attention)
1. [loss](#loss)
1. [loss surface](#loss-surface)
1. [matting](#matting)
1. [memory](#memory)
1. [meta learning](#meta-learning)
1. [metric](#metric)
1. [metric learning](#metric-learning)
1. [mixture of experts](#mixture-of-experts)
1. [mixup](#mixup)
1. [mlm](#mlm)
1. [multimodal](#multimodal)
1. [multimodal generation](#multimodal-generation)
1. [multitask](#multitask)
1. [nas](#nas)
1. [nerf](#nerf)
1. [neural computer](#neural-computer)
1. [neural ode](#neural-ode)
1. [neural rendering](#neural-rendering)
1. [nlp](#nlp)
1. [nmt](#nmt)
1. [noise](#noise)
1. [non autoregressive](#non-autoregressive)
1. [norm free](#norm-free)
1. [normalization](#normalization)
1. [object detection](#object-detection)
1. [ocr](#ocr)
1. [open set recognition](#open-set-recognition)
1. [optimization](#optimization)
1. [optimizer](#optimizer)
1. [oriented object detection](#oriented-object-detection)
1. [out of distribution](#out-of-distribution)
1. [panoptic segmentation](#panoptic-segmentation)
1. [perceptual loss](#perceptual-loss)
1. [point cloud](#point-cloud)
1. [pooling](#pooling)
1. [pose](#pose)
1. [positional encoding](#positional-encoding)
1. [practice](#practice)
1. [pretraining](#pretraining)
1. [probabilistic model](#probabilistic-model)
1. [prompt](#prompt)
1. [pruning](#pruning)
1. [qa](#qa)
1. [reasoning](#reasoning)
1. [regularization](#regularization)
1. [reinforcement learning](#reinforcement-learning)
1. [rendering](#rendering)
1. [representation](#representation)
1. [resampling](#resampling)
1. [restoration](#restoration)
1. [retrieval](#retrieval)
1. [review](#review)
1. [robustness](#robustness)
1. [saliency](#saliency)
1. [salient object detection](#salient-object-detection)
1. [scale](#scale)
1. [score](#score)
1. [self supervised](#self-supervised)
1. [self supervised discovery](#self-supervised-discovery)
1. [semantic factor](#semantic-factor)
1. [semantic segmentation](#semantic-segmentation)
1. [semi supervised learning](#semi-supervised-learning)
1. [sgld](#sgld)
1. [singing voice synthesis](#singing-voice-synthesis)
1. [single image](#single-image)
1. [speech](#speech)
1. [state space model](#state-space-model)
1. [structure learning](#structure-learning)
1. [style transfer](#style-transfer)
1. [stylegan](#stylegan)
1. [super resolution](#super-resolution)
1. [table](#table)
1. [text generation](#text-generation)
1. [tokenizer](#tokenizer)
1. [topic model](#topic-model)
1. [topology](#topology)
1. [tracking](#tracking)
1. [training](#training)
1. [transducer](#transducer)
1. [transfer](#transfer)
1. [transformer](#transformer)
1. [tropical geometry](#tropical-geometry)
1. [tts](#tts)
1. [uncertainty](#uncertainty)
1. [unsupervised img2img](#unsupervised-img2img)
1. [unsupervised nmt](#unsupervised-nmt)
1. [vae](#vae)
1. [video](#video)
1. [video transformer](#video-transformer)
1. [vision](#vision)
1. [vision language](#vision-language)
1. [vision transformer](#vision-transformer)
1. [visual grounding](#visual-grounding)
1. [vit](#vit)
1. [vocoder](#vocoder)
1. [weak supervision](#weak-supervision)
1. [uncategorized](#uncategorized)
## 3d generative model
1. [211220 3D-aware Image Synthesis via Learning Structural and Textural Representations](papers/2021/211220%203D-aware%20Image%20Synthesis%20via%20Learning%20Structural%20and%20Textural%20Representations)
## activation
1. [201019 Smooth activations and reproducibility in deep networks](papers/2020/201019%20Smooth%20activations%20and%20reproducibility%20in%20deep%20networks) #stability
## active learning
1. [200630 Similarity Search for Efficient Active Learning and Search of Rare](papers/2020/200630%20Similarity%20Search%20for%20Efficient%20Active%20Learning%20and%20Search%20of%20Rare)
1. [210729 Batch Active Learning at Scale](papers/2021/210729%20Batch%20Active%20Learning%20at%20Scale)
## adaptation
1. [200129 Side-Tuning](papers/2020/200129%20Side-Tuning)
1. [200130 Once for All](papers/2020/200130%20Once%20for%20All) #deploy
## adapter
1. [210608 Compacter](papers/2021/210608%20Compacter)
## adversarial training
1. [200130 Adversarial Examples Improve Image Recognition](papers/2020/200130%20Adversarial%20Examples%20Improve%20Image%20Recognition)
1. [200625 Smooth Adversarial Training](papers/2020/200625%20Smooth%20Adversarial%20Training)
## antialiasing
1. [201120 An Effective Anti-Aliasing Approach for Residual Networks](papers/2020/201120%20An%20Effective%20Anti-Aliasing%20Approach%20for%20Residual%20Networks)
1. [201128 Truly shift-invariant convolutional neural networks](papers/2020/201128%20Truly%20shift-invariant%20convolutional%20neural%20networks)
## asr
1. [200220 Imputer](papers/2020/200220%20Imputer) #non-autoregressive #ctc
1. [200506 RNN-T Models Fail to Generalize to Out-of-Domain Audio](papers/2020/200506%20RNN-T%20Models%20Fail%20to%20Generalize%20to%20Out-of-Domain%20Audio) #transducer #out_of_distribution #domain #regularization
1. [200510 Listen Attentively, and Spell Once](papers/2020/200510%20Listen%20Attentively%2C%20and%20Spell%20Once) #non-autoregressive
1. [200516 Large scale weakly and semi-supervised learning for low-resource video ASR](papers/2020/200516%20Large%20scale%20weakly%20and%20semi-supervised%20learning%20for%20low-resource%20video%20ASR) #weak_supervision #semi_supervised_learning
1. [200516 Reducing Spelling Inconsistencies in Code-Switching ASR using](papers/2020/200516%20Reducing%20Spelling%20Inconsistencies%20in%20Code-Switching%20ASR%20using) #ctc
1. [200516 Spike-Triggered Non-Autoregressive Transformer for End-to-End Speech Recognition](papers/2020/200516%20Spike-Triggered%20Non-Autoregressive%20Transformer%20for%20End-to-End%20Speech%20Recognition) #non-autoregressive
1. [200518 Attention-based Transducer for Online Speech Recognition](papers/2020/200518%20Attention-based%20Transducer%20for%20Online%20Speech%20Recognition) #transducer
1. [200518 Iterative Pseudo-Labeling for Speech Recognition](papers/2020/200518%20Iterative%20Pseudo-Labeling%20for%20Speech%20Recognition)
1. [200519 Distilling Knowledge from Ensembles of Acoustic Models for Joint CTC-Attention End-to-End Speech Recognition](papers/2020/200519%20Distilling%20Knowledge%20from%20Ensembles%20of%20Acoustic%20Models%20for%20Joint%20CTC-Attention%20End-to-End%20Speech%20Recognition) #ctc
1. [200519 Improved Noisy Student Training for Automatic Speech Recognition](papers/2020/200519%20Improved%20Noisy%20Student%20Training%20for%20Automatic%20Speech%20Recognition) #semi_supervised_learning
1. [200729 Developing RNN-T Models Surpassing High-Performance Hybrid Models with](papers/2020/200729%20Developing%20RNN-T%20Models%20Surpassing%20High-Performance%20Hybrid%20Models%20with) #rnn_t
1. [201021 FastEmit](papers/2020/201021%20FastEmit) #transducer #decoding
1. [201027 CASS-NAT](papers/2020/201027%20CASS-NAT) #non-autoregressive
1. [201125 Streaming end-to-end multi-talker speech recognition](papers/2020/201125%20Streaming%20end-to-end%20multi-talker%20speech%20recognition) #transducer
1. [210524 Unsupervised Speech Recognition](papers/2021/210524%20Unsupervised%20Speech%20Recognition) #unsupervised_training
1. [210608 SpeechBrain](papers/2021/210608%20SpeechBrain)
1. [210827 Injecting Text in Self-Supervised Speech Pretraining](papers/2021/210827%20Injecting%20Text%20in%20Self-Supervised%20Speech%20Pretraining) #self_supervised
1. [211012 Word Order Does Not Matter For Speech Recognition](papers/2021/211012%20Word%20Order%20Does%20Not%20Matter%20For%20Speech%20Recognition) #weak_supervision
1. [211030 Pseudo-Labeling for Massively Multilingual Speech Recognition](papers/2021/211030%20Pseudo-Labeling%20for%20Massively%20Multilingual%20Speech%20Recognition) #semi_supervised_learning #multilingual
1. [211210 Building a great multi-lingual teacher with sparsely-gated mixture of experts for speech recognition](papers/2021/211210%20Building%20a%20great%20multi-lingual%20teacher%20with%20sparsely-gated%20mixture%20of%20experts%20for%20speech%20recognition) #moe
## attention
1. [200122 Object Contextual Representations](papers/2020/200122%20Object%20Contextual%20Representations) #semantic_segmentation
1. [200129 Empirical Attention](papers/2020/200129%20Empirical%20Attention)
1. [200130 Axial Attention](papers/2020/200130%20Axial%20Attention) #generative_model
1. [200130 Criss-Cross Attention](papers/2020/200130%20Criss-Cross%20Attention) #semantic_segmentation
1. [200212 Capsules with Inverted Dot-Product Attention Routing](papers/2020/200212%20Capsules%20with%20Inverted%20Dot-Product%20Attention%20Routing) #capsule
1. [200219 Tree-structured Attention with Hierarchical Accumulation](papers/2020/200219%20Tree-structured%20Attention%20with%20Hierarchical%20Accumulation) #parse
1. [200226 Sparse Sinkhorn Attention](papers/2020/200226%20Sparse%20Sinkhorn%20Attention) #sparse_attention
1. [200317 Axial-DeepLab](papers/2020/200317%20Axial-DeepLab) #panoptic_segmentation
1. [200404 Neural Architecture Search for Lightweight Non-Local Networks](papers/2020/200404%20Neural%20Architecture%20Search%20for%20Lightweight%20Non-Local%20Networks)
1. [200421 Attention is Not Only a Weight](papers/2020/200421%20Attention%20is%20Not%20Only%20a%20Weight) #bert
1. [200423 Self-Attention Attribution](papers/2020/200423%20Self-Attention%20Attribution) #bert
1. [200428 Exploring Self-attention for Image Recognition](papers/2020/200428%20Exploring%20Self-attention%20for%20Image%20Recognition)
1. [200510 CTC-synchronous Training for Monotonic Attention Model](papers/2020/200510%20CTC-synchronous%20Training%20for%20Monotonic%20Attention%20Model) #asr #ctc
1. [200516 Streaming Transformer-based Acoustic Models Using Self-attention with Augmented Memory](papers/2020/200516%20Streaming%20Transformer-based%20Acoustic%20Models%20Using%20Self-attention%20with%20Augmented%20Memory) #asr #memory
1. [200519 Normalized Attention Without Probability Cage](papers/2020/200519%20Normalized%20Attention%20Without%20Probability%20Cage)
1. [200519 Staying True to Your Word](papers/2020/200519%20Staying%20True%20to%20Your%20Word)
1. [200626 Object-Centric Learning with Slot Attention](papers/2020/200626%20Object-Centric%20Learning%20with%20Slot%20Attention)
1. [201119 On the Dynamics of Training Attention Models](papers/2020/201119%20On%20the%20Dynamics%20of%20Training%20Attention%20Models) #training
1. [210223 Linear Transformers Are Secretly Fast Weight Memory Systems](papers/2021/210223%20Linear%20Transformers%20Are%20Secretly%20Fast%20Weight%20Memory%20Systems) #linear_attention #efficient_attention
1. [210225 LazyFormer](papers/2021/210225%20LazyFormer) #bert
1. [210517 Pay Attention to MLPs](papers/2021/210517%20Pay%20Attention%20to%20MLPs) #mlp
1. [210524 Self-Attention Networks Can Process Bounded Hierarchical Languages](papers/2021/210524%20Self-Attention%20Networks%20Can%20Process%20Bounded%20Hierarchical%20Languages) #nlp
1. [210607 Refiner](papers/2021/210607%20Refiner) #vit
1. [210705 What Makes for Hierarchical Vision Transformer](papers/2021/210705%20What%20Makes%20for%20Hierarchical%20Vision%20Transformer) #vit #mlp #local_attention
1. [210826 Train Short, Test Long](papers/2021/210826%20Train%20Short%2C%20Test%20Long) #positional_encoding
## audio generation
1. [220220 It's Raw! Audio Generation with State-Space Models](papers/2022/220220%20It%27s%20Raw%21%20Audio%20Generation%20with%20State-Space%20Models)
## audio source separation
1. [211019 The Cocktail Fork Problem](papers/2021/211019%20The%20Cocktail%20Fork%20Problem)
## augmentation
1. [200122 FixMatch](papers/2020/200122%20FixMatch) #semi_supervised_learning #manifold #mixup
1. [200220 Affinity and Diversity](papers/2020/200220%20Affinity%20and%20Diversity)
1. [200621 AdvAug](papers/2020/200621%20AdvAug) #mixup #nlp #adversarial_training
1. [200710 Meta-Learning Requires Meta-Augmentation](papers/2020/200710%20Meta-Learning%20Requires%20Meta-Augmentation) #metalearning
1. [201117 Sequence-Level Mixed Sample Data Augmentation](papers/2020/201117%20Sequence-Level%20Mixed%20Sample%20Data%20Augmentation) #nlp
1. [201213 Simple Copy-Paste is a Strong Data Augmentation Method for Instance](papers/2020/201213%20Simple%20Copy-Paste%20is%20a%20Strong%20Data%20Augmentation%20Method%20for%20Instance) #instance_segmentation
1. [201214 Improving Panoptic Segmentation at All Scales](papers/2020/201214%20Improving%20Panoptic%20Segmentation%20at%20All%20Scales) #panoptic_segmentation
1. [210318 AlignMix](papers/2021/210318%20AlignMix) #mixup
1. [210318 TrivialAugment](papers/2021/210318%20TrivialAugment)
1. [210429 Ensembling with Deep Generative Views](papers/2021/210429%20Ensembling%20with%20Deep%20Generative%20Views) #ensemble #gan_inversion
## autoregressive model
1. [200129 Semi Autorgressive Training](papers/2020/200129%20Semi%20Autorgressive%20Training)
1. [201027 Scaling Laws for Autoregressive Generative Modeling](papers/2020/201027%20Scaling%20Laws%20for%20Autoregressive%20Generative%20Modeling) #scale
1. [211216 Characterizing and addressing the issue of oversmoothing in neural autoregressive sequence modeling](papers/2021/211216%20Characterizing%20and%20addressing%20the%20issue%20of%20oversmoothing%20in%20neural%20autoregressive%20sequence%20modeling)
## backbone
1. [190724 MixNet](papers/2019/190724%20MixNet) #convolution
1. [200123 Antialiasing](papers/2020/200123%20Antialiasing) #invariance
1. [200128 Attentive Normalization](papers/2020/200128%20Attentive%20Normalization)
1. [200128 IBN-Net](papers/2020/200128%20IBN-Net)
1. [200128 Selective Kernel](papers/2020/200128%20Selective%20Kernel)
1. [200128 SpineNet](papers/2020/200128%20SpineNet)
1. [200128 Squeeze-Excitation](papers/2020/200128%20Squeeze-Excitation)
1. [200128 Switchable Normalization](papers/2020/200128%20Switchable%20Normalization)
1. [200128 Switchable Whitening](papers/2020/200128%20Switchable%20Whitening)
1. [200129 Assembled Techniques](papers/2020/200129%20Assembled%20Techniques) #regularization
1. [200129 DenseNet](papers/2020/200129%20DenseNet)
1. [200129 Dual Path Networks](papers/2020/200129%20Dual%20Path%20Networks)
1. [200129 HarDNet](papers/2020/200129%20HarDNet)
1. [200129 PyramidNet](papers/2020/200129%20PyramidNet)
1. [200129 SelecSLS](papers/2020/200129%20SelecSLS)
1. [200129 ShuffleNet V2](papers/2020/200129%20ShuffleNet%20V2) #efficiency
1. [200129 VoVNet](papers/2020/200129%20VoVNet)
1. [200130 FishNet](papers/2020/200130%20FishNet)
1. [200130 HRNet](papers/2020/200130%20HRNet)
1. [200130 MixConv](papers/2020/200130%20MixConv) #convolution
1. [200330 Designing Network Design Spaces](papers/2020/200330%20Designing%20Network%20Design%20Spaces) #hypernetwork
1. [200330 TResNet](papers/2020/200330%20TResNet) #antialiasing
1. [200419 ResNeSt](papers/2020/200419%20ResNeSt)
1. [200630 Deep Isometric Learning for Visual Recognition](papers/2020/200630%20Deep%20Isometric%20Learning%20for%20Visual%20Recognition) #normalization #resnet #cnn #norm_free
1. [200712 PSConv](papers/2020/200712%20PSConv) #cnn #multiscale
1. [201015 HS-ResNet](papers/2020/201015%20HS-ResNet) #multiscale
1. [201221 FcaNet](papers/2020/201221%20FcaNet) #channel_attention
1. [210226 Transformer in Transformer](papers/2021/210226%20Transformer%20in%20Transformer) #vision_transformer
1. [210304 Barlow Twins](papers/2021/210304%20Barlow%20Twins) #self_supervised #contrastive_learning
1. [210310 Involution](papers/2021/210310%20Involution) #convolution #attention
1. [210312 Revisiting ResNets](papers/2021/210312%20Revisiting%20ResNets) #resnet
1. [210317 Learning to Resize Images for Computer Vision Tasks](papers/2021/210317%20Learning%20to%20Resize%20Images%20for%20Computer%20Vision%20Tasks) #resizing
1. [210331 EfficientNetV2](papers/2021/210331%20EfficientNetV2)
1. [210408 SI-Score](papers/2021/210408%20SI-Score) #robustness #vision_transformer
1. [210505 RepMLP](papers/2021/210505%20RepMLP) #mlp
1. [210506 Do You Even Need Attention](papers/2021/210506%20Do%20You%20Even%20Need%20Attention) #mlp
1. [210510 ResMLP](papers/2021/210510%20ResMLP) #mlp
1. [210617 Layer Folding](papers/2021/210617%20Layer%20Folding) #efficiency #pruning
1. [210628 Early Convolutions Help Transformers See Better](papers/2021/210628%20Early%20Convolutions%20Help%20Transformers%20See%20Better) #cnn #vit
1. [210718 AS-MLP](papers/2021/210718%20AS-MLP) #mlp
1. [210726 Contextual Transformer Networks for Visual Recognition](papers/2021/210726%20Contextual%20Transformer%20Networks%20for%20Visual%20Recognition)
1. [211014 Non-deep Networks](papers/2021/211014%20Non-deep%20Networks)
1. [211018 HRFormer](papers/2021/211018%20HRFormer) #vit
1. [211227 Augmenting Convolutional networks with attention-based aggregation](papers/2021/211227%20Augmenting%20Convolutional%20networks%20with%20attention-based%20aggregation) #vit #cnn
1. [220110 A ConvNet for the 2020s](papers/2022/220110%20A%20ConvNet%20for%20the%202020s) #cnn #vit
1. [220313 Scaling Up Your Kernels to 31x31](papers/2022/220313%20Scaling%20Up%20Your%20Kernels%20to%2031x31)
1. [220318 Three things everyone should know about Vision Transformers](papers/2022/220318%20Three%20things%20everyone%20should%20know%20about%20Vision%20Transformers) #vit
## bayesian
1. [200207 Bayes Posterior](papers/2020/200207%20Bayes%20Posterior)
1. [200210 Liberty or Depth](papers/2020/200210%20Liberty%20or%20Depth) #mean_field
1. [200220 Neural Bayes](papers/2020/200220%20Neural%20Bayes) #representation #clustering
1. [200514 Efficient and Scalable Bayesian Neural Nets with Rank-1 Factors](papers/2020/200514%20Efficient%20and%20Scalable%20Bayesian%20Neural%20Nets%20with%20Rank-1%20Factors) #ensemble #variational_inference
## bert
1. [200305 What the [MASK]](papers/2020/200305%20What%20the%20%5BMASK%5D)
1. [200405 FastBERT](papers/2020/200405%20FastBERT) #distillation #lightweight
1. [200408 DynaBERT](papers/2020/200408%20DynaBERT) #distillation #pruning
1. [200412 XtremeDistil](papers/2020/200412%20XtremeDistil) #distillation #lightweight
1. [200427 DeeBERT](papers/2020/200427%20DeeBERT) #lightweight
1. [200518 Audio ALBERT](papers/2020/200518%20Audio%20ALBERT) #audio #representation
1. [200601 Amnesic Probing](papers/2020/200601%20Amnesic%20Probing)
1. [200608 On the Stability of Fine-tuning BERT](papers/2020/200608%20On%20the%20Stability%20of%20Fine-tuning%20BERT) #finetuning
1. [200610 Revisiting Few-sample BERT Fine-tuning](papers/2020/200610%20Revisiting%20Few-sample%20BERT%20Fine-tuning) #finetuning
1. [210906 An Empirical Study on Few-shot Knowledge Probing for Pretrained Language Models](papers/2021/210906%20An%20Empirical%20Study%20on%20Few-shot%20Knowledge%20Probing%20for%20Pretrained%20Language%20Models) #few_shot #knowledge_base #prompt
1. [210907 Beyond Preserved Accuracy](papers/2021/210907%20Beyond%20Preserved%20Accuracy) #lightweight #distillation
## bias
1. [200519 Identifying Statistical Bias in Dataset Replication](papers/2020/200519%20Identifying%20Statistical%20Bias%20in%20Dataset%20Replication)
1. [201202 Learning from others' mistakes](papers/2020/201202%20Learning%20from%20others%27%20mistakes) #product_of_experts
## calibration
1. [200221 Calibrating Deep Neural Networks using Focal Loss](papers/2020/200221%20Calibrating%20Deep%20Neural%20Networks%20using%20Focal%20Loss) #loss
1. [200223 Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks](papers/2020/200223%20Being%20Bayesian%2C%20Even%20Just%20a%20Bit%2C%20Fixes%20Overconfidence%20in%20ReLU%20Networks) #bayesian
1. [200620 Regression Prior Networks](papers/2020/200620%20Regression%20Prior%20Networks)
1. [210730 Soft Calibration Objectives for Neural Networks](papers/2021/210730%20Soft%20Calibration%20Objectives%20for%20Neural%20Networks)
## causality
1. [200518 An Analysis of the Adaptation Speed of Causal Models](papers/2020/200518%20An%20Analysis%20of%20the%20Adaptation%20Speed%20of%20Causal%20Models)
## channel attention
1. [200129 GCNet](papers/2020/200129%20GCNet)
## chat
1. [200630 PLATO-2](papers/2020/200630%20PLATO-2) #text_gen #chatbot
## classificiation
1. [220107 Generalized Category Discovery](papers/2022/220107%20Generalized%20Category%20Discovery) #open_set_recognition
## computation
1. [200213 Training Large Neural Networks with Constant Memory using a New Execution Algorithm](papers/2020/200213%20Training%20Large%20Neural%20Networks%20with%20Constant%20Memory%20using%20a%20New%20Execution%20Algorithm)
1. [201204 Nimble](papers/2020/201204%20Nimble)
## continual learning
1. [201124 Energy-Based Models for Continual Learning](papers/2020/201124%20Energy-Based%20Models%20for%20Continual%20Learning) #energy_based_model
1. [211103 One Pass ImageNet](papers/2021/211103%20One%20Pass%20ImageNet) #online_learning
## contrastive learning
1. [200213 A Simple Framework for Contrastive Learning of Visual Representations](papers/2020/200213%20A%20Simple%20Framework%20for%20Contrastive%20Learning%20of%20Visual%20Representations) #augmentation
1. [200309 Improved Baselines with Momentum Contrastive Learning](papers/2020/200309%20Improved%20Baselines%20with%20Momentum%20Contrastive%20Learning)
1. [200423 Supervised Contrastive Learning](papers/2020/200423%20Supervised%20Contrastive%20Learning) #metric_learning
1. [200511 Prototypical Contrastive Learning of Unsupervised Representations](papers/2020/200511%20Prototypical%20Contrastive%20Learning%20of%20Unsupervised%20Representations)
1. [200520 What Makes for Good Views for Contrastive Learning](papers/2020/200520%20What%20Makes%20for%20Good%20Views%20for%20Contrastive%20Learning)
1. [200613 Bootstrap your own latent](papers/2020/200613%20Bootstrap%20your%20own%20latent)
1. [200630 Debiased Contrastive Learning](papers/2020/200630%20Debiased%20Contrastive%20Learning)
1. [200730 Contrastive Learning for Unpaired Image-to-Image Translation](papers/2020/200730%20Contrastive%20Learning%20for%20Unpaired%20Image-to-Image%20Translation) #img2img
1. [200803 LoCo](papers/2020/200803%20LoCo)
1. [201020 BYOL works even without batch statistics](papers/2020/201020%20BYOL%20works%20even%20without%20batch%20statistics)
1. [201109 Towards Domain-Agnostic Contrastive Learning](papers/2020/201109%20Towards%20Domain-Agnostic%20Contrastive%20Learning) #mixup #multimodal
1. [201116 AdCo](papers/2020/201116%20AdCo) #adversarial_training
1. [201117 Dense Contrastive Learning for Self-Supervised Visual Pre-Training](papers/2020/201117%20Dense%20Contrastive%20Learning%20for%20Self-Supervised%20Visual%20Pre-Training)
1. [201119 Heterogeneous Contrastive Learning](papers/2020/201119%20Heterogeneous%20Contrastive%20Learning)
1. [201119 Propagate Yourself](papers/2020/201119%20Propagate%20Yourself)
1. [201121 Run Away From your Teacher](papers/2020/201121%20Run%20Away%20From%20your%20Teacher)
1. [201123 Boosting Contrastive Self-Supervised Learning with False Negative](papers/2020/201123%20Boosting%20Contrastive%20Self-Supervised%20Learning%20with%20False%20Negative)
1. [201126 Beyond Single Instance Multi-view Unsupervised Representation Learning](papers/2020/201126%20Beyond%20Single%20Instance%20Multi-view%20Unsupervised%20Representation%20Learning) #self_supervised #mixup
1. [201126 How Well Do Self-Supervised Models Transfer](papers/2020/201126%20How%20Well%20Do%20Self-Supervised%20Models%20Transfer) #self_supervised #transfer
1. [201127 Self-EMD](papers/2020/201127%20Self-EMD)
1. [201201 Towards Good Practices in Self-supervised Representation Learning](papers/2020/201201%20Towards%20Good%20Practices%20in%20Self-supervised%20Representation%20Learning) #self_supervised
1. [201204 Seed the Views](papers/2020/201204%20Seed%20the%20Views) #mixup
1. [201212 Contrastive Learning for Label-Efficient Semantic Segmentation](papers/2020/201212%20Contrastive%20Learning%20for%20Label-Efficient%20Semantic%20Segmentation) #semantic_segmentation
1. [201221 Online Bag-of-Visual-Words Generation for Unsupervised Representation](papers/2020/201221%20Online%20Bag-of-Visual-Words%20Generation%20for%20Unsupervised%20Representation) #self_supervised #discrete_vae
1. [201226 Spatial Contrastive Learning for Few-Shot Classification](papers/2020/201226%20Spatial%20Contrastive%20Learning%20for%20Few-Shot%20Classification) #few_shot #attention
1. [210325 Rethinking Self-Supervised Learning](papers/2021/210325%20Rethinking%20Self-Supervised%20Learning) #training
1. [210405 An Empirical Study of Training Self-Supervised Vision Transformers](papers/2021/210405%20An%20Empirical%20Study%20of%20Training%20Self-Supervised%20Vision%20Transformers) #vision_transformer
1. [210426 Multimodal Contrastive Training for Visual Representation Learning](papers/2021/210426%20Multimodal%20Contrastive%20Training%20for%20Visual%20Representation%20Learning) #multimodal
1. [210429 A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning](papers/2021/210429%20A%20Large-Scale%20Study%20on%20Unsupervised%20Spatiotemporal%20Representation%20Learning) #video
1. [210429 Emerging Properties in Self-Supervised Vision Transformers](papers/2021/210429%20Emerging%20Properties%20in%20Self-Supervised%20Vision%20Transformers) #saliency #vision_transformer #representation
1. [210429 With a Little Help from My Friends](papers/2021/210429%20With%20a%20Little%20Help%20from%20My%20Friends) #knn
1. [210510 Self-Supervised Learning with Swin Transformers](papers/2021/210510%20Self-Supervised%20Learning%20with%20Swin%20Transformers) #vision_transformer
1. [210511 VICReg](papers/2021/210511%20VICReg)
1. [210517 Divide and Contrast](papers/2021/210517%20Divide%20and%20Contrast) #self_supervised #dataset #distillation
1. [210601 Exploring the Diversity and Invariance in Yourself for Visual Pre-Training Task](papers/2021/210601%20Exploring%20the%20Diversity%20and%20Invariance%20in%20Yourself%20for%20Visual%20Pre-Training%20Task)
1. [211018 Understanding Dimensional Collapse in Contrastive Self-supervised Learning](papers/2021/211018%20Understanding%20Dimensional%20Collapse%20in%20Contrastive%20Self-supervised%20Learning)
## convolution
1. [200316 SlimConv](papers/2020/200316%20SlimConv)
1. [210429 Decoupled Dynamic Filter Networks](papers/2021/210429%20Decoupled%20Dynamic%20Filter%20Networks)
## dataset
1. [200509 Building a Manga Dataset](papers/2020/200509%20Building%20a%20Manga%20Dataset)
1. [201130 Image Quality Assessment for Perceptual Image Restoration](papers/2020/201130%20Image%20Quality%20Assessment%20for%20Perceptual%20Image%20Restoration) #score
1. [201201 Weakly-Supervised Arbitrary-Shaped Text Detection with](papers/2020/201201%20Weakly-Supervised%20Arbitrary-Shaped%20Text%20Detection%20with) #ocr #weak_supervision
1. [210601 Comparing Test Sets with Item Response Theory](papers/2021/210601%20Comparing%20Test%20Sets%20with%20Item%20Response%20Theory)
1. [210907 Datasets](papers/2021/210907%20Datasets)
1. [210927 PASS](papers/2021/210927%20PASS)
1. [211103 LAION-400M](papers/2021/211103%20LAION-400M)
## ddpm
1. [200619 Denoising Diffusion Probabilistic Models](papers/2020/200619%20Denoising%20Diffusion%20Probabilistic%20Models)
1. [201214 Learning Energy-Based Models by Diffusion Recovery Likelihood](papers/2020/201214%20Learning%20Energy-Based%20Models%20by%20Diffusion%20Recovery%20Likelihood) #energy_based_model
1. [210506 DiffSinger](papers/2021/210506%20DiffSinger) #singing_voice_synthesis
1. [210511 Diffusion Models Beat GANs on Image Synthesis](papers/2021/210511%20Diffusion%20Models%20Beat%20GANs%20on%20Image%20Synthesis)
1. [210528 Gotta Go Fast When Generating Data with Score-Based Models](papers/2021/210528%20Gotta%20Go%20Fast%20When%20Generating%20Data%20with%20Score-Based%20Models)
1. [210531 On Fast Sampling of Diffusion Probabilistic Models](papers/2021/210531%20On%20Fast%20Sampling%20of%20Diffusion%20Probabilistic%20Models)
1. [210607 Learning to Efficiently Sample from Diffusion Probabilistic Models](papers/2021/210607%20Learning%20to%20Efficiently%20Sample%20from%20Diffusion%20Probabilistic%20Models)
1. [210610 Cascaded Diffusion Models for High Fidelity Image Generation](papers/2021/210610%20Cascaded%20Diffusion%20Models%20for%20High%20Fidelity%20Image%20Generation)
1. [210610 Score-based Generative Modeling in Latent Space](papers/2021/210610%20Score-based%20Generative%20Modeling%20in%20Latent%20Space)
1. [210612 D2C](papers/2021/210612%20D2C)
1. [210701 Variational Diffusion Models](papers/2021/210701%20Variational%20Diffusion%20Models)
1. [210802 SDEdit](papers/2021/210802%20SDEdit)
1. [210819 ImageBART](papers/2021/210819%20ImageBART) #vq #autoregressive_model
1. [211129 Blended Diffusion for Text-driven Editing of Natural Images](papers/2021/211129%20Blended%20Diffusion%20for%20Text-driven%20Editing%20of%20Natural%20Images) #clip #image_editing
1. [211130 Diffusion Autoencoders](papers/2021/211130%20Diffusion%20Autoencoders)
1. [211220 GLIDE](papers/2021/211220%20GLIDE) #multimodal
1. [211220 High-Resolution Image Synthesis with Latent Diffusion Models](papers/2021/211220%20High-Resolution%20Image%20Synthesis%20with%20Latent%20Diffusion%20Models) #vae #vq
1. [220201 Progressive Distillation for Fast Sampling of Diffusion Models](papers/2022/220201%20Progressive%20Distillation%20for%20Fast%20Sampling%20of%20Diffusion%20Models) #distillation
1. [220316 Dual Diffusion Implicit Bridges for Image-to-Image Translation](papers/2022/220316%20Dual%20Diffusion%20Implicit%20Bridges%20for%20Image-to-Image%20Translation)
## decoding
1. [200516 Layer-Wise Cross-View Decoding for Sequence-to-Sequence Learning](papers/2020/200516%20Layer-Wise%20Cross-View%20Decoding%20for%20Sequence-to-Sequence%20Learning)
1. [200601 Cascaded Text Generation with Markov Transformers](papers/2020/200601%20Cascaded%20Text%20Generation%20with%20Markov%20Transformers) #text_generation
1. [210608 FastSeq](papers/2021/210608%20FastSeq)
## deep prior
1. [200408 Deep Manifold Prior](papers/2020/200408%20Deep%20Manifold%20Prior)
## detr
1. [210813 Conditional DETR for Fast Training Convergence](papers/2021/210813%20Conditional%20DETR%20for%20Fast%20Training%20Convergence)
## dewarping
1. [211025 DocTr](papers/2021/211025%20DocTr)
1. [211028 DocScanner](papers/2021/211028%20DocScanner)
## dialog
1. [200129 Meena](papers/2020/200129%20Meena) #NLP
1. [210715 Beyond Goldfish Memory](papers/2021/210715%20Beyond%20Goldfish%20Memory)
1. [220120 LaMDA](papers/2022/220120%20LaMDA)
## differentiable operator
1. [200220 Fast Differentiable Sorting and Ranking](papers/2020/200220%20Fast%20Differentiable%20Sorting%20and%20Ranking)
## differentiable tree
1. [200218 The Tree Ensemble Layer](papers/2020/200218%20The%20Tree%20Ensemble%20Layer)
## discrete vae
1. [200518 Robust Training of Vector Quantized Bottleneck Models](papers/2020/200518%20Robust%20Training%20of%20Vector%20Quantized%20Bottleneck%20Models)
## disentangle
1. [200130 ID-GAN](papers/2020/200130%20ID-GAN) #GAN
1. [200130 MixNMatch](papers/2020/200130%20MixNMatch) #conditional_generative_model
1. [200515 Face Identity Disentanglement via Latent Space Mapping](papers/2020/200515%20Face%20Identity%20Disentanglement%20via%20Latent%20Space%20Mapping)
## distillation
1. [200129 Learning by Cheating](papers/2020/200129%20Learning%20by%20Cheating)
1. [200209 Understanding and Improving Knowledge Distillation](papers/2020/200209%20Understanding%20and%20Improving%20Knowledge%20Distillation)
1. [200210 Subclass Distillation](papers/2020/200210%20Subclass%20Distillation)
1. [200219 Knapsack Pruning with Inner Distillation](papers/2020/200219%20Knapsack%20Pruning%20with%20Inner%20Distillation) #pruning #lightweight
1. [200221 Residual Knowledge Distillation](papers/2020/200221%20Residual%20Knowledge%20Distillation)
1. [200309 Knowledge distillation via adaptive instance normalization](papers/2020/200309%20Knowledge%20distillation%20via%20adaptive%20instance%20normalization) #normalization
1. [200521 Why distillation helps](papers/2020/200521%20Why%20distillation%20helps) #calibration
1. [200629 An EM Approach to Non-autoregressive Conditional Sequence Generation](papers/2020/200629%20An%20EM%20Approach%20to%20Non-autoregressive%20Conditional%20Sequence%20Generation) #non-autoregressive
1. [200701 Go Wide, Then Narrow](papers/2020/200701%20Go%20Wide%2C%20Then%20Narrow) #lightweight
1. [200702 Interactive Knowledge Distillation](papers/2020/200702%20Interactive%20Knowledge%20Distillation)
1. [210726 Text is Text, No Matter What](papers/2021/210726%20Text%20is%20Text%2C%20No%20Matter%20What) #multitask
## distributed training
1. [210510 GSPMD](papers/2021/210510%20GSPMD)
## domain adaptation
1. [200526 Keep it Simple](papers/2020/200526%20Keep%20it%20Simple)
## dropout
1. [200701 On Dropout, Overfitting, and Interaction Effects in Deep Neural Networks](papers/2020/200701%20On%20Dropout%2C%20Overfitting%2C%20and%20Interaction%20Effects%20in%20Deep%20Neural%20Networks)
## efficient attention
1. [200410 Longformer](papers/2020/200410%20Longformer)
1. [200412 ProFormer](papers/2020/200412%20ProFormer)
1. [200605 Masked Language Modeling for Proteins via Linearly Scalable Long-Context](papers/2020/200605%20Masked%20Language%20Modeling%20for%20Proteins%20via%20Linearly%20Scalable%20Long-Context)
1. [200608 Linformer](papers/2020/200608%20Linformer)
1. [210324 Finetuning Pretrained Transformers into RNNs](papers/2021/210324%20Finetuning%20Pretrained%20Transformers%20into%20RNNs)
1. [210505 Beyond Self-attention](papers/2021/210505%20Beyond%20Self-attention)
1. [210510 Poolingformer](papers/2021/210510%20Poolingformer)
1. [210603 Luna](papers/2021/210603%20Luna)
1. [210623 Stable, Fast and Accurate](papers/2021/210623%20Stable%2C%20Fast%20and%20Accurate)
1. [210705 Long-Short Transformer](papers/2021/210705%20Long-Short%20Transformer) #local_attention
1. [210712 Combiner](papers/2021/210712%20Combiner) #sparse_attention #local_attention
1. [210725 H-Transformer-1D](papers/2021/210725%20H-Transformer-1D)
1. [211210 Self-attention Does Not Need $O(n^2)$ Memory](papers/2021/211210%20Self-attention%20Does%20Not%20Need%20%24O%28n%5E2%29%24%20Memory)
## embedding
1. [200424 All Word Embeddings from One Embedding](papers/2020/200424%20All%20Word%20Embeddings%20from%20One%20Embedding)
1. [200717 A Unifying Perspective on Neighbor Embeddings along the](papers/2020/200717%20A%20Unifying%20Perspective%20on%20Neighbor%20Embeddings%20along%20the)
1. [210907 Rare Words Degenerate All Words](papers/2021/210907%20Rare%20Words%20Degenerate%20All%20Words)
## end2end
1. [200605 End-to-End Adversarial Text-to-Speech](papers/2020/200605%20End-to-End%20Adversarial%20Text-to-Speech) #tts
1. [200608 FastSpeech 2](papers/2020/200608%20FastSpeech%202) #tts
1. [201106 Wave-Tacotron](papers/2020/201106%20Wave-Tacotron) #tts
1. [210716 Autonomy 2.0](papers/2021/210716%20Autonomy%202.0)
1. [211215 SPTS](papers/2021/211215%20SPTS)
## energy based model
1. [200504 How to Train Your Energy-Based Model for Regression](papers/2020/200504%20How%20to%20Train%20Your%20Energy-Based%20Model%20for%20Regression)
## ensemble
1. [200217 BatchEnsemble](papers/2020/200217%20BatchEnsemble)
## federated learning
1. [210415 See through Gradients](papers/2021/210415%20See%20through%20Gradients)
## few shot
1. [200228 AdarGCN](papers/2020/200228%20AdarGCN) #graph
1. [210608 Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks](papers/2021/210608%20Parameter-efficient%20Multi-task%20Fine-tuning%20for%20Transformers%20via%20Shared%20Hypernetworks) #adapter #multitask
1. [210910 LibFewShot](papers/2021/210910%20LibFewShot)
## finetuning
1. [200214 AutoLR](papers/2020/200214%20AutoLR) #pruning
1. [200426 Masking as an Efficient Alternative to Finetuning for Pretrained](papers/2020/200426%20Masking%20as%20an%20Efficient%20Alternative%20to%20Finetuning%20for%20Pretrained)
1. [200709 Sample-based Regularization](papers/2020/200709%20Sample-based%20Regularization) #transfer
## flow
1. [200220 Regularized Autoencoders via Relaxed Injective Probability Flow](papers/2020/200220%20Regularized%20Autoencoders%20via%20Relaxed%20Injective%20Probability%20Flow)
1. [200227 Woodbury Transformations for Deep Generative Flows](papers/2020/200227%20Woodbury%20Transformations%20for%20Deep%20Generative%20Flows)
## fpn
1. [200122 CARAFE](papers/2020/200122%20CARAFE) #resampling
1. [200129 Mixture FPN](papers/2020/200129%20Mixture%20FPN)
1. [200506 Scale-Equalizing Pyramid Convolution for Object Detection](papers/2020/200506%20Scale-Equalizing%20Pyramid%20Convolution%20for%20Object%20Detection)
1. [201201 Dynamic Feature Pyramid Networks for Object Detection](papers/2020/201201%20Dynamic%20Feature%20Pyramid%20Networks%20for%20Object%20Detection)
1. [201202 Dual Refinement Feature Pyramid Networks for Object Detection](papers/2020/201202%20Dual%20Refinement%20Feature%20Pyramid%20Networks%20for%20Object%20Detection)
1. [201202 Parallel Residual Bi-Fusion Feature Pyramid Network for Accurate](papers/2020/201202%20Parallel%20Residual%20Bi-Fusion%20Feature%20Pyramid%20Network%20for%20Accurate)
1. [201225 Implicit Feature Pyramid Network for Object Detection](papers/2020/201225%20Implicit%20Feature%20Pyramid%20Network%20for%20Object%20Detection) #equilibrium_model #implicit_model
## gan
1. [170629 Do GANs actually learn the distribution](papers/2017/170629%20Do%20GANs%20actually%20learn%20the%20distribution)
1. [191022 MelGAN](papers/2019/191022%20MelGAN) #tts
1. [200129 Adversarial Lipschitz Regularization](papers/2020/200129%20Adversarial%20Lipschitz%20Regularization)
1. [200129 GAN generalization metric](papers/2020/200129%20GAN%20generalization%20metric)
1. [200129 OneGAN](papers/2020/200129%20OneGAN)
1. [200130 AttentionGAN](papers/2020/200130%20AttentionGAN) #attention #img2img
1. [200130 Evaluation metrics of GAN](papers/2020/200130%20Evaluation%20metrics%20of%20GAN) #metric #evaluation #generative_model
1. [200130 Local GAN](papers/2020/200130%20Local%20GAN) #attention
1. [200130 Noise Robust GAN](papers/2020/200130%20Noise%20Robust%20GAN) #robustness
1. [200130 Small-GAN](papers/2020/200130%20Small-GAN)
1. [200130 Smoothness and Stability in GANs](papers/2020/200130%20Smoothness%20and%20Stability%20in%20GANs)
1. [200206 Unbalanced GANs](papers/2020/200206%20Unbalanced%20GANs) #vae
1. [200210 Unsupervised Discovery of Interpretable Directions in the GAN Latent](papers/2020/200210%20Unsupervised%20Discovery%20of%20Interpretable%20Directions%20in%20the%20GAN%20Latent) #semantic_factor
1. [200211 Improved Consistency Regularization for GANs](papers/2020/200211%20Improved%20Consistency%20Regularization%20for%20GANs) #augmentation #consistency_regularization
1. [200211 Smoothness and Stability in GANs](papers/2020/200211%20Smoothness%20and%20Stability%20in%20GANs) #regularization
1. [200212 Image-to-Image Translation with Text Guidance](papers/2020/200212%20Image-to-Image%20Translation%20with%20Text%20Guidance) #multimodal #multimodal_generation #img2img
1. [200212 Real or Not Real, that is the Question](papers/2020/200212%20Real%20or%20Not%20Real%2C%20that%20is%20the%20Question)
1. [200214 Top-k Training of GANs](papers/2020/200214%20Top-k%20Training%20of%20GANs) #regularization
1. [200220 The Benefits of Pairwise Discriminators for Adversarial Training](papers/2020/200220%20The%20Benefits%20of%20Pairwise%20Discriminators%20for%20Adversarial%20Training) #regularization
1. [200223 GANHopper](papers/2020/200223%20GANHopper) #img2img
1. [200224 When Relation Networks meet GANs](papers/2020/200224%20When%20Relation%20Networks%20meet%20GANs) #regularization
1. [200225 Freeze the Discriminator](papers/2020/200225%20Freeze%20the%20Discriminator) #finetuning #transfer
1. [200226 On Leveraging Pretrained GANs for Generation with Limited Data](papers/2020/200226%20On%20Leveraging%20Pretrained%20GANs%20for%20Generation%20with%20Limited%20Data) #finetuning #transfer
1. [200227 Topology Distance](papers/2020/200227%20Topology%20Distance) #topology #score
1. [200228 A U-Net Based Discriminator for Generative Adversarial Networks](papers/2020/200228%20A%20U-Net%20Based%20Discriminator%20for%20Generative%20Adversarial%20Networks)
1. [200304 Creating High Resolution Images with a Latent Adversarial Generator](papers/2020/200304%20Creating%20High%20Resolution%20Images%20with%20a%20Latent%20Adversarial%20Generator) #generative_model #super_resolution
1. [200308 Perceptual Image Super-Resolution with Progressive Adversarial Network](papers/2020/200308%20Perceptual%20Image%20Super-Resolution%20with%20Progressive%20Adversarial%20Network) #super_resolution
1. [200312 Your GAN is Secretly an Energy-based Model and You Should use Discriminator Driven Latent Sampling](papers/2020/200312%20Your%20GAN%20is%20Secretly%20an%20Energy-based%20Model%20and%20You%20Should%20use%20Discriminator%20Driven%20Latent%20Sampling) #energy_based_model #sampling
1. [200317 Blur, Noise, and Compression Robust Generative Adversarial Networks](papers/2020/200317%20Blur%2C%20Noise%2C%20and%20Compression%20Robust%20Generative%20Adversarial%20Networks) #noise
1. [200318 OpenGAN](papers/2020/200318%20OpenGAN) #metric_learning
1. [200325 Improved Techniques for Training Single-Image GANs](papers/2020/200325%20Improved%20Techniques%20for%20Training%20Single-Image%20GANs) #single_image
1. [200326 Image Generation Via Minimizing Fréchet Distance in Discriminator Feature Space](papers/2020/200326%20Image%20Generation%20Via%20Minimizing%20Fr%C3%A9chet%20Distance%20in%20Discriminator%20Feature%20Space)
1. [200402 Controllable Orthogonalization in Training DNNs](papers/2020/200402%20Controllable%20Orthogonalization%20in%20Training%20DNNs) #regularization
1. [200404 Feature Quantization Improves GAN Training](papers/2020/200404%20Feature%20Quantization%20Improves%20GAN%20Training) #discrete_vae
1. [200405 Discriminator Contrastive Divergence](papers/2020/200405%20Discriminator%20Contrastive%20Divergence)
1. [200407 Inclusive GAN](papers/2020/200407%20Inclusive%20GAN)
1. [200408 Attentive Normalization for Conditional Image Generation](papers/2020/200408%20Attentive%20Normalization%20for%20Conditional%20Image%20Generation) #attention
1. [200504 Transforming and Projecting Images into Class-conditional Generative](papers/2020/200504%20Transforming%20and%20Projecting%20Images%20into%20Class-conditional%20Generative) #generative_model
1. [200518 Unconditional Audio Generation with Generative Adversarial Networks and Cycle Regularization](papers/2020/200518%20Unconditional%20Audio%20Generation%20with%20Generative%20Adversarial%20Networks%20and%20Cycle%20Regularization) #audio_generation
1. [200519 CIAGAN](papers/2020/200519%20CIAGAN)
1. [200519 Regularization Methods for Generative Adversarial Networks](papers/2020/200519%20Regularization%20Methods%20for%20Generative%20Adversarial%20Networks) #review #regularization
1. [200604 Image Augmentations for GAN Training](papers/2020/200604%20Image%20Augmentations%20for%20GAN%20Training) #augmentation
1. [200611 Training Generative Adversarial Networks with Limited Data](papers/2020/200611%20Training%20Generative%20Adversarial%20Networks%20with%20Limited%20Data) #augmentation
1. [200618 Differentiable Augmentation for Data-Efficient GAN Training](papers/2020/200618%20Differentiable%20Augmentation%20for%20Data-Efficient%20GAN%20Training) #augmentation
1. [200618 Diverse Image Generation via Self-Conditioned GANs](papers/2020/200618%20Diverse%20Image%20Generation%20via%20Self-Conditioned%20GANs) #generative_model
1. [200630 PriorGAN](papers/2020/200630%20PriorGAN)
1. [200708 InfoMax-GAN](papers/2020/200708%20InfoMax-GAN) #regularization
1. [200713 Closed-Form Factorization of Latent Semantics in GANs](papers/2020/200713%20Closed-Form%20Factorization%20of%20Latent%20Semantics%20in%20GANs) #semantic_factor
1. [200729 Instance Selection for GANs](papers/2020/200729%20Instance%20Selection%20for%20GANs)
1. [200729 VocGAN](papers/2020/200729%20VocGAN) #vocoder
1. [200730 Rewriting a Deep Generative Model](papers/2020/200730%20Rewriting%20a%20Deep%20Generative%20Model)
1. [200804 Open-Edit](papers/2020/200804%20Open-Edit) #image_editing
1. [200807 Improving the Speed and Quality of GAN by Adversarial Training](papers/2020/200807%20Improving%20the%20Speed%20and%20Quality%20of%20GAN%20by%20Adversarial%20Training) #robustness
1. [201028 Training Generative Adversarial Networks by Solving Ordinary](papers/2020/201028%20Training%20Generative%20Adversarial%20Networks%20by%20Solving%20Ordinary) #neural_ode
1. [201109 Learning Semantic-aware Normalization for Generative Adversarial Networks](papers/2020/201109%20Learning%20Semantic-aware%20Normalization%20for%20Generative%20Adversarial%20Networks) #normalization
1. [201109 Towards a Better Global Loss Landscape of GANs](papers/2020/201109%20Towards%20a%20Better%20Global%20Loss%20Landscape%20of%20GANs) #training
1. [201118 Style Intervention](papers/2020/201118%20Style%20Intervention) #semantic_factor
1. [201124 Adversarial Generation of Continuous Images](papers/2020/201124%20Adversarial%20Generation%20of%20Continuous%20Images) #implicit_representation
1. [201125 How to train your conditional GAN](papers/2020/201125%20How%20to%20train%20your%20conditional%20GAN) #img2img #generative_model
1. [201125 Omni-GAN](papers/2020/201125%20Omni-GAN) #generative_model
1. [201127 Image Generators with Conditionally-Independent Pixel Synthesis](papers/2020/201127%20Image%20Generators%20with%20Conditionally-Independent%20Pixel%20Synthesis) #implicit_representation
1. [201201 Refining Deep Generative Models via Discriminator Gradient Flow](papers/2020/201201%20Refining%20Deep%20Generative%20Models%20via%20Discriminator%20Gradient%20Flow) #sampling
1. [201201 pi-GAN](papers/2020/201201%20pi-GAN) #implicit_representation
1. [201203 Self-labeled Conditional GANs](papers/2020/201203%20Self-labeled%20Conditional%20GANs) #unsupervised_training
1. [201204 A Note on Data Biases in Generative Models](papers/2020/201204%20A%20Note%20on%20Data%20Biases%20in%20Generative%20Models) #bias #generative_model
1. [201208 You Only Need Adversarial Supervision for Semantic Image Synthesis](papers/2020/201208%20You%20Only%20Need%20Adversarial%20Supervision%20for%20Semantic%20Image%20Synthesis) #img2img
1. [210227 Ultra-Data-Efficient GAN Training](papers/2021/210227%20Ultra-Data-Efficient%20GAN%20Training) #augmentation #few_shot
1. [210317 Training GANs with Stronger Augmentations via Contrastive Discriminator](papers/2021/210317%20Training%20GANs%20with%20Stronger%20Augmentations%20via%20Contrastive%20Discriminator) #contrastive_learning #augmentation
1. [210318 Drop the GAN](papers/2021/210318%20Drop%20the%20GAN) #single_image #generative_model #patch
1. [210330 Dual Contrastive Loss and Attention for GANs](papers/2021/210330%20Dual%20Contrastive%20Loss%20and%20Attention%20for%20GANs) #contrastive_learning
1. [210401 Partition-Guided GANs](papers/2021/210401%20Partition-Guided%20GANs)
1. [210407 Regularizing Generative Adversarial Networks under Limited Data](papers/2021/210407%20Regularizing%20Generative%20Adversarial%20Networks%20under%20Limited%20Data) #regularization
1. [210408 InfinityGAN](papers/2021/210408%20InfinityGAN)
1. [210413 DatasetGAN](papers/2021/210413%20DatasetGAN) #few_shot
1. [210413 Few-shot Image Generation via Cross-domain Correspondence](papers/2021/210413%20Few-shot%20Image%20Generation%20via%20Cross-domain%20Correspondence) #img2img #generative_model #few_shot
1. [210414 Aligning Latent and Image Spaces to Connect the Unconnectable](papers/2021/210414%20Aligning%20Latent%20and%20Image%20Spaces%20to%20Connect%20the%20Unconnectable)
1. [210415 GANcraft](papers/2021/210415%20GANcraft) #nerf
1. [210422 On Buggy Resizing Libraries and Surprising Subtleties in FID Calculation](papers/2021/210422%20On%20Buggy%20Resizing%20Libraries%20and%20Surprising%20Subtleties%20in%20FID%20Calculation) #antialiasing
1. [210426 EigenGAN](papers/2021/210426%20EigenGAN) #semantic_factor
1. [210608 Data-Efficient Instance Generation from Instance Discrimination](papers/2021/210608%20Data-Efficient%20Instance%20Generation%20from%20Instance%20Discrimination) #contrastive_learning
1. [210614 Improved Transformer for High-Resolution GANs](papers/2021/210614%20Improved%20Transformer%20for%20High-Resolution%20GANs) #transformer #efficient_training
1. [210623 Alias-Free Generative Adversarial Networks](papers/2021/210623%20Alias-Free%20Generative%20Adversarial%20Networks) #antialiasing
1. [210910 Instance-Conditioned GAN](papers/2021/210910%20Instance-Conditioned%20GAN)
1. [210927 WarpedGANSpace](papers/2021/210927%20WarpedGANSpace)
1. [211017 AE-StyleGAN](papers/2021/211017%20AE-StyleGAN) #gan_inversion
1. [211101 Projected GANs Converge Faster](papers/2021/211101%20Projected%20GANs%20Converge%20Faster)
1. [211215 Efficient Geometry-aware 3D Generative Adversarial Networks](papers/2021/211215%20Efficient%20Geometry-aware%203D%20Generative%20Adversarial%20Networks) #nerf
1. [211216 GRAM](papers/2021/211216%20GRAM) #3d_generative_model #nerf
1. [220201 StyleGAN-XL](papers/2022/220201%20StyleGAN-XL)
1. [220219 Truncated Diffusion Probabilistic Models](papers/2022/220219%20Truncated%20Diffusion%20Probabilistic%20Models) #generative_model #ddpm
1. [220224 Self-Distilled StyleGAN](papers/2022/220224%20Self-Distilled%20StyleGAN)
1. [220311 The Role of ImageNet Classes in Fréchet Inception Distance](papers/2022/220311%20The%20Role%20of%20ImageNet%20Classes%20in%20Fr%C3%A9chet%20Inception%20Distance)
1. [220314 InsetGAN for Full-Body Image Generation](papers/2022/220314%20InsetGAN%20for%20Full-Body%20Image%20Generation) #pose
1. [220414 Any-resolution Training for High-resolution Image Synthesis](papers/2022/220414%20Any-resolution%20Training%20for%20High-resolution%20Image%20Synthesis)
## gan inversion
1. [200330 Exploiting Deep Generative Prior for Versatile Image Restoration and](papers/2020/200330%20Exploiting%20Deep%20Generative%20Prior%20for%20Versatile%20Image%20Restoration%20and) #perceptual_loss
1. [200331 In-Domain GAN Inversion for Real Image Editing](papers/2020/200331%20In-Domain%20GAN%20Inversion%20for%20Real%20Image%20Editing)
1. [200703 Collaborative Learning for Faster StyleGAN Embedding](papers/2020/200703%20Collaborative%20Learning%20for%20Faster%20StyleGAN%20Embedding)
1. [220223 Near Perfect GAN Inversion](papers/2022/220223%20Near%20Perfect%20GAN%20Inversion)
## generalization
1. [200130 Fantastic Generalization Measures](papers/2020/200130%20Fantastic%20Generalization%20Measures)
1. [200225 Rethinking Bias-Variance Trade-off for Generalization of Neural Networks](papers/2020/200225%20Rethinking%20Bias-Variance%20Trade-off%20for%20Generalization%20of%20Neural%20Networks)
## generative model
1. [190325 Implicit Generative and Generalization in Energy-Based Models](papers/2019/190325%20Implicit%20Generative%20and%20Generalization%20in%20Energy-Based%20Models) #energy_based_model
1. [200129 Controlling Generative Model](papers/2020/200129%20Controlling%20Generative%20Model)
1. [200129 Deep Automodulator](papers/2020/200129%20Deep%20Automodulator)
1. [200129 Frechet Joint Distance](papers/2020/200129%20Frechet%20Joint%20Distance)
1. [200129 Spot CNN generated image](papers/2020/200129%20Spot%20CNN%20generated%20image)
1. [200130 BIVA](papers/2020/200130%20BIVA)
1. [200130 Glow](papers/2020/200130%20Glow) #flow
1. [200130 IGEBM](papers/2020/200130%20IGEBM) #energy_based_model
1. [200130 Neural Spline Flows](papers/2020/200130%20Neural%20Spline%20Flows) #flow
1. [200130 VQ-VAE-2](papers/2020/200130%20VQ-VAE-2) #autoregressive_model
1. [200217 Augmented Normalizing Flows](papers/2020/200217%20Augmented%20Normalizing%20Flows) #flow
1. [200313 Semantic Pyramid for Image Generation](papers/2020/200313%20Semantic%20Pyramid%20for%20Image%20Generation) #perceptual_loss #image_editing
1. [200616 Improved Techniques for Training Score-Based Generative Models](papers/2020/200616%20Improved%20Techniques%20for%20Training%20Score-Based%20Generative%20Models) #ncsn
1. [201117 DeepNAG](papers/2020/201117%20DeepNAG)
1. [201126 Score-Based Generative Modeling through Stochastic Differential](papers/2020/201126%20Score-Based%20Generative%20Modeling%20through%20Stochastic%20Differential) #ddpm
1. [201202 Improved Contrastive Divergence Training of Energy Based Models](papers/2020/201202%20Improved%20Contrastive%20Divergence%20Training%20of%20Energy%20Based%20Models) #energy_based_model
1. [201204 Few-shot Image Generation with Elastic Weight Consolidation](papers/2020/201204%20Few-shot%20Image%20Generation%20with%20Elastic%20Weight%20Consolidation) #few_shot #continual_learning
1. [201209 Positional Encoding as Spatial Inductive Bias in GANs](papers/2020/201209%20Positional%20Encoding%20as%20Spatial%20Inductive%20Bias%20in%20GANs) #positional_encoding
1. [201224 Soft-IntroVAE](papers/2020/201224%20Soft-IntroVAE) #vae
1. [210223 Zero-Shot Text-to-Image Generation](papers/2021/210223%20Zero-Shot%20Text-to-Image%20Generation) #discrete_vae #autoregressive_model #multimodal
1. [210302 Fixing Data Augmentation to Improve Adversarial Robustness](papers/2021/210302%20Fixing%20Data%20Augmentation%20to%20Improve%20Adversarial%20Robustness) #ddpm #augmentation
1. [210305 Fixing Data Augmentation to Improve Adversarial Robustness 2](papers/2021/210305%20Fixing%20Data%20Augmentation%20to%20Improve%20Adversarial%20Robustness%202) #robustness #augmentation #ddpm
1. [210318 Few-shot Semantic Image  Synthesis Using StyleGAN Prior](papers/2021/210318%20Few-shot%20Semantic%20Image%20%20Synthesis%20Using%20StyleGAN%20Prior) #stylegan #few_shot
1. [210824 SimVLM](papers/2021/210824%20SimVLM) #vision-language
1. [211015 MaGNET](papers/2021/211015%20MaGNET) #sampling
1. [220208 MaskGIT](papers/2022/220208%20MaskGIT) #autoregressive_model #non-autoregressive #vq
## graph
1. [200129 Multi-Graph Transformer](papers/2020/200129%20Multi-Graph%20Transformer)
## hallucination
1. [210413 The Curious Case of Hallucinations in Neural Machine Translation](papers/2021/210413%20The%20Curious%20Case%20of%20Hallucinations%20in%20Neural%20Machine%20Translation) #mt
## hypernetwork
1. [200722 WeightNet](papers/2020/200722%20WeightNet) #channel_attention
## hyperparameter
1. [200425 Learning to Guide Random Search](papers/2020/200425%20Learning%20to%20Guide%20Random%20Search)
1. [200521 HyperSTAR](papers/2020/200521%20HyperSTAR)
## identifiability
1. [200701 On Linear Identifiability of Learned Representations](papers/2020/200701%20On%20Linear%20Identifiability%20of%20Learned%20Representations)
## image editing
1. [200515 Semantic Photo Manipulation with a Generative Image Prior](papers/2020/200515%20Semantic%20Photo%20Manipulation%20with%20a%20Generative%20Image%20Prior)
1. [201123 HistoGAN](papers/2020/201123%20HistoGAN)
1. [210318 Using latent space regression to analyze and leverage compositionality](papers/2021/210318%20Using%20latent%20space%20regression%20to%20analyze%20and%20leverage%20compositionality)
## image generation
1. [200426 Disentangled Image Generation Through Structured Noise Injection](papers/2020/200426%20Disentangled%20Image%20Generation%20Through%20Structured%20Noise%20Injection)
## img2img
1. [200130 FUNIT](papers/2020/200130%20FUNIT)
1. [200305 SketchyCOCO](papers/2020/200305%20SketchyCOCO)
1. [200315 GMM-UNIT](papers/2020/200315%20GMM-UNIT) #multimodal_generation
1. [200319 High-Resolution Daytime Translation Without Domain Labels](papers/2020/200319%20High-Resolution%20Daytime%20Translation%20Without%20Domain%20Labels)
1. [200330 Semi-supervised Learning for Few-shot Image-to-Image Translation](papers/2020/200330%20Semi-supervised%20Learning%20for%20Few-shot%20Image-to-Image%20Translation) #semi_supervised_learning #few_shot
1. [200406 Rethinking Spatially-Adaptive Normalization](papers/2020/200406%20Rethinking%20Spatially-Adaptive%20Normalization) #lightweight
1. [200409 TuiGAN](papers/2020/200409%20TuiGAN) #few_shot #single_image
1. [200419 TriGAN](papers/2020/200419%20TriGAN) #domain_adaptation
1. [200702 Deep Single Image Manipulation](papers/2020/200702%20Deep%20Single%20Image%20Manipulation) #single_image #image_editing
1. [200709 Improving Style-Content Disentanglement in Image-to-Image Translation](papers/2020/200709%20Improving%20Style-Content%20Disentanglement%20in%20Image-to-Image%20Translation) #disentangle
1. [200714 COCO-FUNIT](papers/2020/200714%20COCO-FUNIT)
1. [200715 Transformation Consistency Regularization- A Semi-Supervised Paradigm](papers/2020/200715%20Transformation%20Consistency%20Regularization-%20A%20Semi-Supervised%20Paradigm) #augmentation #semi_supervised_learning
1. [200723 TSIT](papers/2020/200723%20TSIT)
1. [200724 The Surprising Effectiveness of Linear Unsupervised Image-to-Image](papers/2020/200724%20The%20Surprising%20Effectiveness%20of%20Linear%20Unsupervised%20Image-to-Image)
1. [201203 CoCosNet v2](papers/2020/201203%20CoCosNet%20v2) #patch #pose
1. [201205 Spatially-Adaptive Pixelwise Networks for Fast Image Translation](papers/2020/201205%20Spatially-Adaptive%20Pixelwise%20Networks%20for%20Fast%20Image%20Translation) #implicit_representation
## implicit model
1. [200615 Multiscale Deep Equilibrium Models](papers/2020/200615%20Multiscale%20Deep%20Equilibrium%20Models)
## implicit representation
1. [210506 ACORN](papers/2021/210506%20ACORN) #positional_encoding
1. [211026 NeRV](papers/2021/211026%20NeRV)
1. [211122 Neural Fields in Visual Computing and Beyond](papers/2021/211122%20Neural%20Fields%20in%20Visual%20Computing%20and%20Beyond)
1. [220117 Instant Neural Graphics Primitives with a Multiresolution Hash Encoding](papers/2022/220117%20Instant%20Neural%20Graphics%20Primitives%20with%20a%20Multiresolution%20Hash%20Encoding)
## instance segmentation
1. [200129 BlendMask](papers/2020/200129%20BlendMask)
1. [200129 COCO 2018 Instance Segmentation](papers/2020/200129%20COCO%202018%20Instance%20Segmentation) #challenge
1. [200129 Deep Snake](papers/2020/200129%20Deep%20Snake)
1. [200130 PointRend](papers/2020/200130%20PointRend)
1. [200311 Conditional Convolutions for Instance Segmentation](papers/2020/200311%20Conditional%20Convolutions%20for%20Instance%20Segmentation)
1. [200313 PointINS](papers/2020/200313%20PointINS) #dynamic_conv
1. [200722 Deep Variational Instance Segmentation](papers/2020/200722%20Deep%20Variational%20Instance%20Segmentation)
1. [200730 LevelSet R-CNN](papers/2020/200730%20LevelSet%20R-CNN)
1. [201119 DCT-Mask](papers/2020/201119%20DCT-Mask)
1. [201119 Unifying Instance and Panoptic Segmentation with Dynamic Rank-1](papers/2020/201119%20Unifying%20Instance%20and%20Panoptic%20Segmentation%20with%20Dynamic%20Rank-1) #panoptic_segmentation #dynamic_conv
1. [201126 The Devil is in the Boundary](papers/2020/201126%20The%20Devil%20is%20in%20the%20Boundary)
1. [201129 End-to-End Video Instance Segmentation with Transformers](papers/2020/201129%20End-to-End%20Video%20Instance%20Segmentation%20with%20Transformers) #end2end #detr #video
1. [201203 BoxInst](papers/2020/201203%20BoxInst) #dataset #weak_supervision
1. [210503 ISTR](papers/2021/210503%20ISTR) #end2end
1. [210505 QueryInst](papers/2021/210505%20QueryInst) #end2end
1. [210604 SOLQ](papers/2021/210604%20SOLQ)
1. [210713 Per-Pixel Classification is Not All You Need for Semantic Segmentation](papers/2021/210713%20Per-Pixel%20Classification%20is%20Not%20All%20You%20Need%20for%20Semantic%20Segmentation) #panoptic_segmentation #semantic_segmentation #detr
## interpolation
1. [200804 Autoencoder Image Interpolation by Shaping the Latent Space](papers/2020/200804%20Autoencoder%20Image%20Interpolation%20by%20Shaping%20the%20Latent%20Space)
1. [211018 Learning in High Dimension Always Amounts to Extrapolation](papers/2021/211018%20Learning%20in%20High%20Dimension%20Always%20Amounts%20to%20Extrapolation) #extrapolation
## knowledge base
1. [200214 Scalable Neural Methods for Reasoning With a Symbolic Knowledge Base](papers/2020/200214%20Scalable%20Neural%20Methods%20for%20Reasoning%20With%20a%20Symbolic%20Knowledge%20Base)
## language generation
1. [200712 Do You Have the Right Scissors](papers/2020/200712%20Do%20You%20Have%20the%20Right%20Scissors)
1. [200729 Mirostat](papers/2020/200729%20Mirostat)
## language model
1. [200128 Scaling Laws for LM](papers/2020/200128%20Scaling%20Laws%20for%20LM)
1. [200205 K-Adapter](papers/2020/200205%20K-Adapter) #multitask #adapter
1. [200206 Consistency of a Recurrent Language Model With Respect to Incomplete](papers/2020/200206%20Consistency%20of%20a%20Recurrent%20Language%20Model%20With%20Respect%20to%20Incomplete) #decoding #hallucination #language_generation
1. [200222 Training Question Answering Models From Synthetic Data](papers/2020/200222%20Training%20Question%20Answering%20Models%20From%20Synthetic%20Data) #qa #bert
1. [200225 MiniLM](papers/2020/200225%20MiniLM) #distillation #lightweight
1. [200406 Sparse Text Generation](papers/2020/200406%20Sparse%20Text%20Generation) #language_generation #sampling
1. [200427 Recall and Learn](papers/2020/200427%20Recall%20and%20Learn) #finetuning #continual_learning
1. [200505 Stolen Probability](papers/2020/200505%20Stolen%20Probability)
1. [200516 MicroNet for Efficient Language Modeling](papers/2020/200516%20MicroNet%20for%20Efficient%20Language%20Modeling) #lightweight
1. [200518 Contextual Embeddings](papers/2020/200518%20Contextual%20Embeddings)
1. [201015 Fine-Tuning Pre-trained Language Model with Weak Supervision](papers/2020/201015%20Fine-Tuning%20Pre-trained%20Language%20Model%20with%20Weak%20Supervision) #transfer #weak_supervision
1. [201023 Rethinking embedding coupling in pre-trained language models](papers/2020/201023%20Rethinking%20embedding%20coupling%20in%20pre-trained%20language%20models) #regularization
1. [201201 How Can We Know When Language Models Know](papers/2020/201201%20How%20Can%20We%20Know%20When%20Language%20Models%20Know) #qa #calibration
1. [201228 Universal Sentence Representation Learning with Conditional Masked](papers/2020/201228%20Universal%20Sentence%20Representation%20Learning%20with%20Conditional%20Masked) #sentence_embedding #mlm
1. [210216 Non-Autoregressive Text Generation with Pre-trained Language Models](papers/2021/210216%20Non-Autoregressive%20Text%20Generation%20with%20Pre-trained%20Language%20Models) #non-autoregressive #text_generation
1. [210318 GPT Understands, Too](papers/2021/210318%20GPT%20Understands%2C%20Too) #finetuning #prompt
1. [210407 Revisiting Simple Neural Probabilistic Language Models](papers/2021/210407%20Revisiting%20Simple%20Neural%20Probabilistic%20Language%20Models)
1. [210420 Carbon Emissions and Large Neural Network Training](papers/2021/210420%20Carbon%20Emissions%20and%20Large%20Neural%20Network%20Training) #nlp
1. [210922 Recursively Summarizing Books with Human Feedback](papers/2021/210922%20Recursively%20Summarizing%20Books%20with%20Human%20Feedback) #summarization
## layout
1. [210601 Incorporating Visual Layout Structures for Scientific Text Classification](papers/2021/210601%20Incorporating%20Visual%20Layout%20Structures%20for%20Scientific%20Text%20Classification)
1. [210902 Skim-Attention](papers/2021/210902%20Skim-Attention)
1. [220418 LayoutLMv3](papers/2022/220418%20LayoutLMv3)
## lightweight
1. [200624 Neural Architecture Design for GPU-Efficient Networks](papers/2020/200624%20Neural%20Architecture%20Design%20for%20GPU-Efficient%20Networks)
1. [201124 MicroNet](papers/2020/201124%20MicroNet)
1. [210507 Pareto-Optimal Quantized ResNet Is Mostly 4-bit](papers/2021/210507%20Pareto-Optimal%20Quantized%20ResNet%20Is%20Mostly%204-bit) #quantization
## line
1. [210601 Towards Real-time and Light-weight Line Segment Detection](papers/2021/210601%20Towards%20Real-time%20and%20Light-weight%20Line%20Segment%20Detection)
## lm
1. [210524 StructuralLM](papers/2021/210524%20StructuralLM) #layout
1. [210524 True Few-Shot Learning with Language Models](papers/2021/210524%20True%20Few-Shot%20Learning%20with%20Language%20Models) #few_shot
1. [210528 ByT5](papers/2021/210528%20ByT5)
1. [210617 LoRA](papers/2021/210617%20LoRA) #adapter #finetuning
1. [210623 Charformer](papers/2021/210623%20Charformer) #tokenizer
1. [210714 Deduplicating Training Data Makes Language Models Better](papers/2021/210714%20Deduplicating%20Training%20Data%20Makes%20Language%20Models%20Better) #corpus
1. [210714 HTLM](papers/2021/210714%20HTLM)
1. [210811 DEMix Layers](papers/2021/210811%20DEMix%20Layers) #mixture_of_experts
1. [210813 Curriculum Learning](papers/2021/210813%20Curriculum%20Learning) #curriculum
1. [210816 On the Opportunities and Risks of Foundation Models](papers/2021/210816%20On%20the%20Opportunities%20and%20Risks%20of%20Foundation%20Models)
1. [210902 Do Prompt-Based Models Really Understand the Meaning of their Prompts](papers/2021/210902%20Do%20Prompt-Based%20Models%20Really%20Understand%20the%20Meaning%20of%20their%20Prompts) #prompt
1. [210903 Finetuned Language Models Are Zero-Shot Learners](papers/2021/210903%20Finetuned%20Language%20Models%20Are%20Zero-Shot%20Learners) #zero-shot
1. [210908 A Recipe For Arbitrary Text Style Transfer with Large Language Models](papers/2021/210908%20A%20Recipe%20For%20Arbitrary%20Text%20Style%20Transfer%20with%20Large%20Language%20Models) #prompt
1. [211011 Unsupervised Neural Machine Translation with Generative Language Models Only](papers/2021/211011%20Unsupervised%20Neural%20Machine%20Translation%20with%20Generative%20Language%20Models%20Only) #unsupervised_nmt
1. [211015 Multitask Prompted Training Enables Zero-Shot Task Generalization](papers/2021/211015%20Multitask%20Prompted%20Training%20Enables%20Zero-Shot%20Task%20Generalization) #zero-shot
1. [211016 Invariant Language Modeling](papers/2021/211016%20Invariant%20Language%20Modeling) #irm
1. [211016 MarkupLM](papers/2021/211016%20MarkupLM) #layout
1. [211016 Sharpness-Aware Minimization Improves Language Model Generalization](papers/2021/211016%20Sharpness-Aware%20Minimization%20Improves%20Language%20Model%20Generalization) #regularization
1. [211020 Shaking the foundations](papers/2021/211020%20Shaking%20the%20foundations) #causality
1. [211027 Training Verifiers to Solve Math Word Problems](papers/2021/211027%20Training%20Verifiers%20to%20Solve%20Math%20Word%20Problems)
1. [211213 GLaM](papers/2021/211213%20GLaM) #moe
1. [211220 Efficient Large Scale Language Modeling with Mixtures of Experts](papers/2021/211220%20Efficient%20Large%20Scale%20Language%20Modeling%20with%20Mixtures%20of%20Experts) #mixture_of_experts
1. [220210 Red Teaming Language Models with Language Models](papers/2022/220210%20Red%20Teaming%20Language%20Models%20with%20Language%20Models) #safety
1. [220213 A Contrastive Framework for Neural Text Generation](papers/2022/220213%20A%20Contrastive%20Framework%20for%20Neural%20Text%20Generation) #decoding
1. [220215 General-purpose, long-context autoregressive modeling with Perceiver AR](papers/2022/220215%20General-purpose%2C%20long-context%20autoregressive%20modeling%20with%20Perceiver%20AR) #efficient_attention #autoregressive_model
1. [220314 Efficient Language Modeling with Sparse all-MLP](papers/2022/220314%20Efficient%20Language%20Modeling%20with%20Sparse%20all-MLP) #mlp
1. [220329 Training Compute-Optimal Large Language Models](papers/2022/220329%20Training%20Compute-Optimal%20Large%20Language%20Models)
1. [220413 METRO](papers/2022/220413%20METRO)
1. [220414 GPT-NeoX-20B](papers/2022/220414%20GPT-NeoX-20B)
## local attention
1. [210323 Scaling Local Self-Attention for Parameter Efficient Visual Backbones](papers/2021/210323%20Scaling%20Local%20Self-Attention%20for%20Parameter%20Efficient%20Visual%20Backbones)
## loss
1. [200712 It Is Likely That Your Loss Should be a Likelihood](papers/2020/200712%20It%20Is%20Likely%20That%20Your%20Loss%20Should%20be%20a%20Likelihood)
## loss surface
1. [210225 Loss Surface Simplexes for Mode Connecting Volumes and Fast Ensembling](papers/2021/210225%20Loss%20Surface%20Simplexes%20for%20Mode%20Connecting%20Volumes%20and%20Fast%20Ensembling)
## matting
1. [200401 Background Matting](papers/2020/200401%20Background%20Matting)
1. [201123 Is a Green Screen Really Necessary for Real-Time Portrait Matting](papers/2020/201123%20Is%20a%20Green%20Screen%20Really%20Necessary%20for%20Real-Time%20Portrait%20Matting)
## memory
1. [200206 Product Kanerva Machines](papers/2020/200206%20Product%20Kanerva%20Machines)
## meta learning
1. [200221 Learning to Continually Learn](papers/2020/200221%20Learning%20to%20Continually%20Learn) #continual_learning
1. [200312 Online Fast Adaptation and Knowledge Accumulation](papers/2020/200312%20Online%20Fast%20Adaptation%20and%20Knowledge%20Accumulation)
1. [200401 Editable Neural Networks](papers/2020/200401%20Editable%20Neural%20Networks)
1. [200706 Meta-Learning Symmetries by Reparameterization](papers/2020/200706%20Meta-Learning%20Symmetries%20by%20Reparameterization) #group_equivariance
## metric
1. [211025 The Efficiency Misnomer](papers/2021/211025%20The%20Efficiency%20Misnomer)
## metric learning
1. [200319 A unifying mutual information view of metric learning](papers/2020/200319%20A%20unifying%20mutual%20information%20view%20of%20metric%20learning)
## mixture of experts
1. [220202 Unified Scaling Laws for Routed Language Models](papers/2022/220202%20Unified%20Scaling%20Laws%20for%20Routed%20Language%20Models)
## mixup
1. [201220 ResizeMix](papers/2020/201220%20ResizeMix)
1. [211228 LINDA](papers/2021/211228%20LINDA) #interpolation
## mlm
1. [200424 Probabilistically Masked Language Model Capable of Autoregressive Generation in Arbitrary Word Order](papers/2020/200424%20Probabilistically%20Masked%20Language%20Model%20Capable%20of%20Autoregressive%20Generation%20in%20Arbitrary%20Word%20Order) #language_generation
1. [210502 Larger-Scale Transformers for Multilingual Masked Language Modeling](papers/2021/210502%20Larger-Scale%20Transformers%20for%20Multilingual%20Masked%20Language%20Modeling) #multilingual #scale
1. [220216 Should You Mask 15% in Masked Language Modeling](papers/2022/220216%20Should%20You%20Mask%2015%25%20in%20Masked%20Language%20Modeling)
## multimodal
1. [200401 Pixel-BERT](papers/2020/200401%20Pixel-BERT)
1. [200513 INFOTABS](papers/2020/200513%20INFOTABS)
1. [200514 Behind the Scene](papers/2020/200514%20Behind%20the%20Scene)
1. [201130 Multimodal Pretraining Unmasked](papers/2020/201130%20Multimodal%20Pretraining%20Unmasked)
1. [210928 VideoCLIP](papers/2021/210928%20VideoCLIP) #video_transformer #retrieval
1. [211103 An Empirical Study of Training End-to-End Vision-and-Language Transformers](papers/2021/211103%20An%20Empirical%20Study%20of%20Training%20End-to-End%20Vision-and-Language%20Transformers) #vision-language
## multimodal generation
1. [211122 L-Verse](papers/2021/211122%20L-Verse)
1. [211124 NÜWA](papers/2021/211124%20N%C3%9CWA)
## multitask
1. [200508 Transforming task representations to perform novel tasks](papers/2020/200508%20Transforming%20task%20representations%20to%20perform%20novel%20tasks) #continual_learning
1. [200625 MTAdam](papers/2020/200625%20MTAdam)
1. [210825 Multi-Task Self-Training for Learning General Representations](papers/2021/210825%20Multi-Task%20Self-Training%20for%20Learning%20General%20Representations)
## nas
1. [200324 BigNAS](papers/2020/200324%20BigNAS)
1. [200326 Are Labels Necessary for Neural Architecture Search](papers/2020/200326%20Are%20Labels%20Necessary%20for%20Neural%20Architecture%20Search) #unsupervised_training
1. [200406 Network Adjustment](papers/2020/200406%20Network%20Adjustment)
1. [200412 FBNetV2](papers/2020/200412%20FBNetV2)
1. [200428 Angle-based Search Space Shrinking for Neural Architecture Search](papers/2020/200428%20Angle-based%20Search%20Space%20Shrinking%20for%20Neural%20Architecture%20Search)
1. [200506 Local Search is State of the Art for Neural Architecture Search](papers/2020/200506%20Local%20Search%20is%20State%20of%20the%20Art%20for%20Neural%20Architecture%20Search)
1. [200507 Noisy Differentiable Architecture Search](papers/2020/200507%20Noisy%20Differentiable%20Architecture%20Search)
1. [200602 FBNetV3](papers/2020/200602%20FBNetV3) #hyperparameter #training #swa
1. [200720 NSGANetV2](papers/2020/200720%20NSGANetV2)
## nerf
1. [201014 NeRF++](papers/2020/201014%20NeRF%2B%2B)
1. [201125 Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes](papers/2020/201125%20Neural%20Scene%20Flow%20Fields%20for%20Space-Time%20View%20Synthesis%20of%20Dynamic%20Scenes)
1. [201127 D-NeRF](papers/2020/201127%20D-NeRF)
1. [201203 Learned Initializations for Optimizing Coordinate-Based Neural](papers/2020/201203%20Learned%20Initializations%20for%20Optimizing%20Coordinate-Based%20Neural) #implicit_representation
1. [201203 pixelNeRF](papers/2020/201203%20pixelNeRF)
1. [201215 Object-Centric Neural Scene Rendering](papers/2020/201215%20Object-Centric%20Neural%20Scene%20Rendering)
1. [210225 IBRNet](papers/2021/210225%20IBRNet)
1. [210318 FastNeRF](papers/2021/210318%20FastNeRF)
1. [210318 GNeRF](papers/2021/210318%20GNeRF)
1. [210318 MVSNeRF](papers/2021/210318%20MVSNeRF)
1. [210318 NeMI](papers/2021/210318%20NeMI)
1. [210324 Mip-NeRF](papers/2021/210324%20Mip-NeRF)
1. [210325 KiloNeRF](papers/2021/210325%20KiloNeRF)
1. [210325 PlenOctrees for Real-time Rendering of Neural Radiance Fields](papers/2021/210325%20PlenOctrees%20for%20Real-time%20Rendering%20of%20Neural%20Radiance%20Fields)
1. [210706 Depth-supervised NeRF](papers/2021/210706%20Depth-supervised%20NeRF)
1. [210809 NeuralMVS](papers/2021/210809%20NeuralMVS)
1. [211019 CIPS-3D](papers/2021/211019%20CIPS-3D) #stylegan
1. [211129 Deblur-NeRF](papers/2021/211129%20Deblur-NeRF)
1. [211129 HDR-NeRF](papers/2021/211129%20HDR-NeRF)
1. [211129 Urban Radiance Fields](papers/2021/211129%20Urban%20Radiance%20Fields)
1. [211210 CityNeRF](papers/2021/211210%20CityNeRF)
## neural computer
1. [200720 Distributed Associative Memory Network with Memory Refreshing Loss](papers/2020/200720%20Distributed%20Associative%20Memory%20Network%20with%20Memory%20Refreshing%20Loss)
1. [211130 Show Your Work](papers/2021/211130%20Show%20Your%20Work)
## neural ode
1. [200207 How to train your neural ODE](papers/2020/200207%20How%20to%20train%20your%20neural%20ODE)
1. [200520 Neural Controlled Differential Equations](papers/2020/200520%20Neural%20Controlled%20Differential%20Equations)
1. [200708 Learning Differential Equations that are Easy to Solve](papers/2020/200708%20Learning%20Differential%20Equations%20that%20are%20Easy%20to%20Solve)
## neural rendering
1. [200226 Learning to Shadow Hand-drawn Sketches](papers/2020/200226%20Learning%20to%20Shadow%20Hand-drawn%20Sketches)
1. [200427 Neural Hair Rendering](papers/2020/200427%20Neural%20Hair%20Rendering)
1. [200506 CONFIG](papers/2020/200506%20CONFIG)
1. [201116 Stylized Neural Painting](papers/2020/201116%20Stylized%20Neural%20Painting)
1. [201119 Creative Sketch Generation](papers/2020/201119%20Creative%20Sketch%20Generation)
1. [201130 Animating Pictures with Eulerian Motion Fields](papers/2020/201130%20Animating%20Pictures%20with%20Eulerian%20Motion%20Fields) #single_image
1. [210319 Paint by Word](papers/2021/210319%20Paint%20by%20Word)
1. [210512 Enhancing Photorealism Enhancement](papers/2021/210512%20Enhancing%20Photorealism%20Enhancement)
1. [211013 ADOP](papers/2021/211013%20ADOP)
## nlp
1. [200518 (Re)construing Meaning in NLP](papers/2020/200518%20%28Re%29construing%20Meaning%20in%20NLP)
1. [200715 Towards Debiasing Sentence Representations](papers/2020/200715%20Towards%20Debiasing%20Sentence%20Representations) #bias
## nmt
1. [200207 A Multilingual View of Unsupervised Machine Translation](papers/2020/200207%20A%20Multilingual%20View%20of%20Unsupervised%20Machine%20Translation) #multilingual
1. [200427 Lexically Constrained Neural Machine Translation with Levenshtein Transformer](papers/2020/200427%20Lexically%20Constrained%20Neural%20Machine%20Translation%20with%20Levenshtein%20Transformer)
1. [200710 Learn to Use Future Information in Simultaneous Translation](papers/2020/200710%20Learn%20to%20Use%20Future%20Information%20in%20Simultaneous%20Translation) #simultaneous_translation
1. [201224 Why Neural Machine Translation Prefers Empty Outputs](papers/2020/201224%20Why%20Neural%20Machine%20Translation%20Prefers%20Empty%20Outputs) #hallucination
1. [211015 Breaking Down Multilingual Machine Translation](papers/2021/211015%20Breaking%20Down%20Multilingual%20Machine%20Translation) #multilingual
## noise
1. [201223 Noisy Labels Can Induce Good Representations](papers/2020/201223%20Noisy%20Labels%20Can%20Induce%20Good%20Representations) #representation
## non autoregressive
1. [200403 Aligned Cross Entropy for Non-Autoregressive Machine Translation](papers/2020/200403%20Aligned%20Cross%20Entropy%20for%20Non-Autoregressive%20Machine%20Translation)
1. [200415 Non-Autoregressive Machine Translation with Latent Alignments](papers/2020/200415%20Non-Autoregressive%20Machine%20Translation%20with%20Latent%20Alignments) #nmt #ctc
1. [200422 A Study of Non-autoregressive Model for Sequence Generation](papers/2020/200422%20A%20Study%20of%20Non-autoregressive%20Model%20for%20Sequence%20Generation)
1. [201022 Parallel Tacotron](papers/2020/201022%20Parallel%20Tacotron) #vae
1. [201025 Improved Mask-CTC for Non-Autoregressive End-to-End ASR](papers/2020/201025%20Improved%20Mask-CTC%20for%20Non-Autoregressive%20End-to-End%20ASR) #ctc
1. [201125 FBWave](papers/2020/201125%20FBWave) #vocoder #lightweight
1. [201207 EfficientTTS](papers/2020/201207%20EfficientTTS) #tts
1. [211213 Step-unrolled Denoising Autoencoders for Text Generation](papers/2021/211213%20Step-unrolled%20Denoising%20Autoencoders%20for%20Text%20Generation)
## norm free
1. [200310 ReZero is All You Need](papers/2020/200310%20ReZero%20is%20All%20You%20Need) #initialization
## normalization
1. [200122 Group Norm, Weight Standardization](papers/2020/200122%20Group%20Norm%2C%20Weight%20Standardization)
1. [200122 Moving Average Batch Normalization](papers/2020/200122%20Moving%20Average%20Batch%20Normalization)
1. [200122 StyleGAN 2](papers/2020/200122%20StyleGAN%202) #GAN
1. [200130 Rethinking Normalization](papers/2020/200130%20Rethinking%20Normalization)
1. [200130 Weight Standardization](papers/2020/200130%20Weight%20Standardization) #weight
1. [200224 Batch Normalization Biases Residual Blocks Towards the Identity Function](papers/2020/200224%20Batch%20Normalization%20Biases%20Residual%20Blocks%20Towards%20the%20Identity%20Function) #optimization #norm_free #initialization
1. [200306 TaskNorm](papers/2020/200306%20TaskNorm) #meta_learning
1. [200406 Evolving Normalization-Activation Layers](papers/2020/200406%20Evolving%20Normalization-Activation%20Layers) #nas #activation
1. [200427 A Batch Normalized Inference Network Keeps the KL Vanishing Away](papers/2020/200427%20A%20Batch%20Normalized%20Inference%20Network%20Keeps%20the%20KL%20Vanishing%20Away)
1. [201128 Batch Normalization with Enhanced Linear Transformation](papers/2020/201128%20Batch%20Normalization%20with%20Enhanced%20Linear%20Transformation)
1. [211026 Revisiting Batch Normalization](papers/2021/211026%20Revisiting%20Batch%20Normalization)
## object detection
1. [191118 Anchor-Free](papers/2019/191118%20Anchor-Free)
1. [191118 CenterMask](papers/2019/191118%20CenterMask) #instance_segmentation #backbone #1stage
1. [191121 EfficientDet](papers/2019/191121%20EfficientDet)
1. [200103 BlendMask](papers/2020/200103%20BlendMask) #instance_segmentation #1stage
1. [200122 SABL](papers/2020/200122%20SABL)
1. [200129 AP Loss](papers/2020/200129%20AP%20Loss) #loss
1. [200129 Backbone Reallocation for Detection](papers/2020/200129%20Backbone%20Reallocation%20for%20Detection) #backbone #nas
1. [200129 Dense RepPoints](papers/2020/200129%20Dense%20RepPoints)
1. [200129 DetNAS](papers/2020/200129%20DetNAS) #nas #backbone
1. [200129 IOU-aware single stage detector](papers/2020/200129%20IOU-aware%20single%20stage%20detector) #1stage
1. [200130 ATSS](papers/2020/200130%20ATSS) #anchor #retinanet #fcos
1. [200130 AutoAugment](papers/2020/200130%20AutoAugment) #augmentation #search
1. [200130 EfficientDet](papers/2020/200130%20EfficientDet) #fpn
1. [200130 Keypoint Triplet](papers/2020/200130%20Keypoint%20Triplet) #keypoint
1. [200130 Learning from Noisy Anchors](papers/2020/200130%20Learning%20from%20Noisy%20Anchors)
1. [200130 Multiple Anchor Learning](papers/2020/200130%20Multiple%20Anchor%20Learning) #anchor
1. [200130 Objects as Points](papers/2020/200130%20Objects%20as%20Points) #keypoint
1. [200130 Soft Anchor-Point](papers/2020/200130%20Soft%20Anchor-Point) #anchor
1. [200211 Object Detection as a Positive-Unlabeled Problem](papers/2020/200211%20Object%20Detection%20as%20a%20Positive-Unlabeled%20Problem) #positive_unlabled #dataset
1. [200212 Solving Missing-Annotation Object Detection with Background](papers/2020/200212%20Solving%20Missing-Annotation%20Object%20Detection%20with%20Background) #dataset #noise
1. [200218 Universal-RCNN](papers/2020/200218%20Universal-RCNN) #multi_dataset #graph
1. [200316 Frustratingly Simple Few-Shot Object Detection](papers/2020/200316%20Frustratingly%20Simple%20Few-Shot%20Object%20Detection) #few_shot
1. [200317 Revisiting the Sibling Head in Object Detector](papers/2020/200317%20Revisiting%20the%20Sibling%20Head%20in%20Object%20Detector)
1. [200319 Revisiting the Sibling Head in Object Detector](papers/2020/200319%20Revisiting%20the%20Sibling%20Head%20in%20Object%20Detector) #review
1. [200320 CentripetalNet](papers/2020/200320%20CentripetalNet) #keypoint
1. [200413 Dynamic R-CNN](papers/2020/200413%20Dynamic%20R-CNN)
1. [200423 YOLOv4](papers/2020/200423%20YOLOv4)
1. [200511 Scope Head for Accurate Localization in Object Detection](papers/2020/200511%20Scope%20Head%20for%20Accurate%20Localization%20in%20Object%20Detection)
1. [200526 End-to-End Object Detection with Transformers](papers/2020/200526%20End-to-End%20Object%20Detection%20with%20Transformers) #end2end #matching
1. [200603 DetectoRS](papers/2020/200603%20DetectoRS)
1. [200611 Rethinking Pre-training and Self-training](papers/2020/200611%20Rethinking%20Pre-training%20and%20Self-training) #semi_supervised_learning #transfer
1. [200706 LabelEnc](papers/2020/200706%20LabelEnc) #distillation
1. [200707 AutoAssign](papers/2020/200707%20AutoAssign) #anchor_free
1. [200714 AQD](papers/2020/200714%20AQD) #quantization
1. [200715 Probabilistic Anchor Assignment with IoU Prediction for Object Detection](papers/2020/200715%20Probabilistic%20Anchor%20Assignment%20with%20IoU%20Prediction%20for%20Object%20Detection) #anchor #1stage
1. [200716 RepPoints V2](papers/2020/200716%20RepPoints%20V2) #1stage #anchor_free
1. [200723 PP-YOLO](papers/2020/200723%20PP-YOLO) #tuning
1. [200723 The Devil is in Classification](papers/2020/200723%20The%20Devil%20is%20in%20Classification) #longtail
1. [200727 Corner Proposal Network for Anchor-free, Two-stage Object Detection](papers/2020/200727%20Corner%20Proposal%20Network%20for%20Anchor-free%2C%20Two-stage%20Object%20Detection) #anchor_free #2stage
1. [201116 Scaled-YOLOv4](papers/2020/201116%20Scaled-YOLOv4)
1. [201118 End-to-End Object Detection with Adaptive Clustering Transformer](papers/2020/201118%20End-to-End%20Object%20Detection%20with%20Adaptive%20Clustering%20Transformer) #detr #end2end #efficiency
1. [201121 Rethinking Transformer-based Set Prediction for Object Detection](papers/2020/201121%20Rethinking%20Transformer-based%20Set%20Prediction%20for%20Object%20Detection) #detr #end2end #efficiency
1. [201124 Sparse R-CNN](papers/2020/201124%20Sparse%20R-CNN)
1. [201128 Class-agnostic Object Detection](papers/2020/201128%20Class-agnostic%20Object%20Detection)
1. [201207 End-to-End Object Detection with Fully Convolutional Network](papers/2020/201207%20End-to-End%20Object%20Detection%20with%20Fully%20Convolutional%20Network) #end2end
1. [201223 SWA Object Detection](papers/2020/201223%20SWA%20Object%20Detection) #swa
1. [201227 Towards A Category-extended Object Detector without Relabeling or](papers/2020/201227%20Towards%20A%20Category-extended%20Object%20Detector%20without%20Relabeling%20or) #continual_learning
1. [210225 Simple multi-dataset detection](papers/2021/210225%20Simple%20multi-dataset%20detection) #multi_dataset
1. [210316 You Only Look One-level Feature](papers/2021/210316%20You%20Only%20Look%20One-level%20Feature)
1. [210325 USB](papers/2021/210325%20USB) #dataset
1. [210417 TransVG](papers/2021/210417%20TransVG) #visual_grounding
1. [210420 PP-YOLOv2](papers/2021/210420%20PP-YOLOv2) #yolo
1. [210426 MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding](papers/2021/210426%20MDETR%20--%20Modulated%20Detection%20for%20End-to-End%20Multi-Modal%20Understanding) #detr #visual_grounding
1. [210601 You Only Look at One Sequence](papers/2021/210601%20You%20Only%20Look%20at%20One%20Sequence) #vit
1. [210615 Dynamic Head](papers/2021/210615%20Dynamic%20Head) #attention
1. [210718 YOLOX](papers/2021/210718%20YOLOX) #yolo
1. [210728 SimROD](papers/2021/210728%20SimROD) #domain_adaptation #self_supervised
1. [210922 Pix2seq](papers/2021/210922%20Pix2seq) #detr #autoregressive_model
1. [210929 Localizing Objects with Self-Supervised Transformers and no Labels](papers/2021/210929%20Localizing%20Objects%20with%20Self-Supervised%20Transformers%20and%20no%20Labels) #self_supervised #self_supervised_discovery #salient_object_detection
1. [211101 PP-PicoDet](papers/2021/211101%20PP-PicoDet) #lightweight
1. [211122 Benchmarking Detection Transfer Learning with Vision Transformers](papers/2021/211122%20Benchmarking%20Detection%20Transfer%20Learning%20with%20Vision%20Transformers) #unsupervised_training #vit
1. [211123 Dynamic DETR](papers/2021/211123%20Dynamic%20DETR)
1. [211129 Sparse DETR](papers/2021/211129%20Sparse%20DETR) #detr
1. [220107 Detecting Twenty-thousand Classes using Image-level Supervision](papers/2022/220107%20Detecting%20Twenty-thousand%20Classes%20using%20Image-level%20Supervision) #weak_supervision
1. [220330 Exploring Plain Vision Transformer Backbones for Object Detection](papers/2022/220330%20Exploring%20Plain%20Vision%20Transformer%20Backbones%20for%20Object%20Detection) #vit #instance_segmentation
## ocr
1. [191231 LayoutLM](papers/2019/191231%20LayoutLM)
1. [200217 Text Perceptron](papers/2020/200217%20Text%20Perceptron)
1. [210415 Rethinking Text Line Recognition Models](papers/2021/210415%20Rethinking%20Text%20Line%20Recognition%20Models)
1. [220107 Data-Efficient Information Extraction from Form-Like Documents](papers/2022/220107%20Data-Efficient%20Information%20Extraction%20from%20Form-Like%20Documents) #information_extraction
1. [220328 Towards End-to-End Unified Scene Text Detection and Layout Analysis](papers/2022/220328%20Towards%20End-to-End%20Unified%20Scene%20Text%20Detection%20and%20Layout%20Analysis)
1. [220416 Pushing the Performance Limit of Scene Text Recognizer without Human Annotation](papers/2022/220416%20Pushing%20the%20Performance%20Limit%20of%20Scene%20Text%20Recognizer%20without%20Human%20Annotation)
## open set recognition
1. [211012 Open-Set Recognition](papers/2021/211012%20Open-Set%20Recognition)
## optimization
1. [200221 The Break-Even Point on Optimization Trajectories of Deep Neural Networks](papers/2020/200221%20The%20Break-Even%20Point%20on%20Optimization%20Trajectories%20of%20Deep%20Neural%20Networks) #loss #training
1. [200224 The Early Phase of Neural Network Training](papers/2020/200224%20The%20Early%20Phase%20of%20Neural%20Network%20Training)
1. [200227 Using a thousand optimization tasks to learn hyperparameter search strategies](papers/2020/200227%20Using%20a%20thousand%20optimization%20tasks%20to%20learn%20hyperparameter%20search%20strategies) #optimizer #hyperparameter
1. [200228 A Self-Tuning Actor-Critic Algorithm](papers/2020/200228%20A%20Self-Tuning%20Actor-Critic%20Algorithm) #reinforcement_learning #hyperparameter #meta_learning
1. [200316 Weak and Strong Gradient Directions](papers/2020/200316%20Weak%20and%20Strong%20Gradient%20Directions)
1. [200403 Gradient Centralization](papers/2020/200403%20Gradient%20Centralization) #training
1. [200508 An Investigation of Why Overparameterization Exacerbates Spurious](papers/2020/200508%20An%20Investigation%20of%20Why%20Overparameterization%20Exacerbates%20Spurious) #training
1. [200519 One Size Fits All](papers/2020/200519%20One%20Size%20Fits%20All)
## optimizer
1. [200130 LAMB](papers/2020/200130%20LAMB) #large_batch
1. [211006 8-bit Optimizers via Block-wise Quantization](papers/2021/211006%208-bit%20Optimizers%20via%20Block-wise%20Quantization)
## oriented object detection
1. [200129 Modulated Loss](papers/2020/200129%20Modulated%20Loss)
1. [200129 Oriented Objects as Middle Lines](papers/2020/200129%20Oriented%20Objects%20as%20Middle%20Lines)
## out of distribution
1. [200509 Generalizing Outside the Training Set](papers/2020/200509%20Generalizing%20Outside%20the%20Training%20Set)
1. [200519 Bridging the Gap Between Training and Inference for Spatio-Temporal Forecasting](papers/2020/200519%20Bridging%20the%20Gap%20Between%20Training%20and%20Inference%20for%20Spatio-Temporal%20Forecasting)
## panoptic segmentation
1. [200129 Bridge gap of traininfer Panoptic Segmentation](papers/2020/200129%20Bridge%20gap%20of%20traininfer%20Panoptic%20Segmentation)
1. [200130 Panoptic-DeepLab](papers/2020/200130%20Panoptic-DeepLab)
1. [200218 Towards Bounding-Box Free Panoptic Segmentation](papers/2020/200218%20Towards%20Bounding-Box%20Free%20Panoptic%20Segmentation) #box_free
1. [200404 Pixel Consensus Voting for Panoptic Segmentation](papers/2020/200404%20Pixel%20Consensus%20Voting%20for%20Panoptic%20Segmentation)
1. [200421 Panoptic-based Image Synthesis](papers/2020/200421%20Panoptic-based%20Image%20Synthesis) #neural_rendering
1. [201123 Scaling Wide Residual Networks for Panoptic Segmentation](papers/2020/201123%20Scaling%20Wide%20Residual%20Networks%20for%20Panoptic%20Segmentation) #scale
1. [201201 Fully Convolutional Networks for Panoptic Segmentation](papers/2020/201201%20Fully%20Convolutional%20Networks%20for%20Panoptic%20Segmentation) #dynamic_conv
1. [201201 MaX-DeepLab](papers/2020/201201%20MaX-DeepLab) #detr #end2end
1. [201202 Single-shot Path Integrated Panoptic Segmentation](papers/2020/201202%20Single-shot%20Path%20Integrated%20Panoptic%20Segmentation) #dynamic_conv
1. [210910 Panoptic Narrative Grounding](papers/2021/210910%20Panoptic%20Narrative%20Grounding) #visual_grounding
1. [211202 Masked-attention Mask Transformer for Universal Image Segmentation](papers/2021/211202%20Masked-attention%20Mask%20Transformer%20for%20Universal%20Image%20Segmentation) #detr
## perceptual loss
1. [200206 Image Fine-grained Inpainting](papers/2020/200206%20Image%20Fine-grained%20Inpainting) #inpainting
1. [200515 Enhancing Perceptual Loss with Adversarial Feature Matching for Super-Resolution](papers/2020/200515%20Enhancing%20Perceptual%20Loss%20with%20Adversarial%20Feature%20Matching%20for%20Super-Resolution)
1. [200626 A Loss Function for Generative Neural Networks Based on Watson's](papers/2020/200626%20A%20Loss%20Function%20for%20Generative%20Neural%20Networks%20Based%20on%20Watson%27s)
1. [201223 Focal Frequency Loss for Image Reconstruction and Synthesis](papers/2020/201223%20Focal%20Frequency%20Loss%20for%20Image%20Reconstruction%20and%20Synthesis) #loss
## point cloud
1. [220325 Point2Seq](papers/2022/220325%20Point2Seq)
## pooling
1. [200325 What Deep CNNs Benefit from Global Covariance Pooling](papers/2020/200325%20What%20Deep%20CNNs%20Benefit%20from%20Global%20Covariance%20Pooling)
1. [200330 Strip Pooling](papers/2020/200330%20Strip%20Pooling)
## pose
1. [200729 Unselfie](papers/2020/200729%20Unselfie) #inpainting
1. [210913 Pose with Style](papers/2021/210913%20Pose%20with%20Style)
## positional encoding
1. [200628 Rethinking Positional Encoding in Language Pre-training](papers/2020/200628%20Rethinking%20Positional%20Encoding%20in%20Language%20Pre-training)
1. [210408 Modulated Periodic Activations for Generalizable Local Functional](papers/2021/210408%20Modulated%20Periodic%20Activations%20for%20Generalizable%20Local%20Functional) #periodic_activation #implicit_representation
1. [210706 Rethinking Positional Encoding](papers/2021/210706%20Rethinking%20Positional%20Encoding)
## practice
1. [210630 Using AntiPatterns to avoid MLOps Mistakes](papers/2021/210630%20Using%20AntiPatterns%20to%20avoid%20MLOps%20Mistakes)
## pretraining
1. [190620 XLNet](papers/2019/190620%20XLNet) #language_model
1. [190729 RoBERTa](papers/2019/190729%20RoBERTa) #language_model
1. [200128 mBART](papers/2020/200128%20mBART) #machine_translation #nlp
1. [200129 ImageBERT](papers/2020/200129%20ImageBERT) #multimodal
1. [200129 LM Pretraining](papers/2020/200129%20LM%20Pretraining) #nlp
1. [200129 oLMpics](papers/2020/200129%20oLMpics) #language_model #nlp
1. [200130 RoBERTa](papers/2020/200130%20RoBERTa) #language_model #nlp #transformer
1. [200130 T5](papers/2020/200130%20T5) #nlp #transformer #seq2seq
1. [200130 ViLBERT](papers/2020/200130%20ViLBERT) #multimodal
1. [200210 Pre-training Tasks for Embedding-based Large-scale Retrieval](papers/2020/200210%20Pre-training%20Tasks%20for%20Embedding-based%20Large-scale%20Retrieval) #retrieval
1. [200217 Incorporating BERT into Neural Machine Translation](papers/2020/200217%20Incorporating%20BERT%20into%20Neural%20Machine%20Translation) #language_model #bert #nmt
1. [200219 CodeBERT](papers/2020/200219%20CodeBERT) #bert
1. [200228 UniLMv2](papers/2020/200228%20UniLMv2) #language_model
1. [200317 Calibration of Pre-trained Transformers](papers/2020/200317%20Calibration%20of%20Pre-trained%20Transformers) #calibration
1. [200405 Unsupervised Domain Clusters in Pretrained Language Models](papers/2020/200405%20Unsupervised%20Domain%20Clusters%20in%20Pretrained%20Language%20Models) #domain
1. [200412 Pre-training Text Representations as Meta Learning](papers/2020/200412%20Pre-training%20Text%20Representations%20as%20Meta%20Learning) #meta_learning #finetuning
1. [200413 Pretrained Transformers Improve Out-of-Distribution Robustness](papers/2020/200413%20Pretrained%20Transformers%20Improve%20Out-of-Distribution%20Robustness) #out_of_distribution
1. [200419 Are we pretraining it right](papers/2020/200419%20Are%20we%20pretraining%20it%20right) #multimodal
1. [200420 Adversarial Training for Large Neural Language Models](papers/2020/200420%20Adversarial%20Training%20for%20Large%20Neural%20Language%20Models) #adversarial_training #language_model #finetuning
1. [200420 MPNet](papers/2020/200420%20MPNet) #language_model
1. [200423 Don't Stop Pretraining](papers/2020/200423%20Don%27t%20Stop%20Pretraining) #domain
1. [200427 LightPAFF](papers/2020/200427%20LightPAFF) #distillation #finetuning
1. [200520 Pretraining with Contrastive Sentence Objectives Improves Discourse Performance of Language Models](papers/2020/200520%20Pretraining%20with%20Contrastive%20Sentence%20Objectives%20Improves%20Discourse%20Performance%20of%20Language%20Models) #contrastive_learning #sentence_embedding
1. [200610 MC-BERT](papers/2020/200610%20MC-BERT)
1. [200615 To Pretrain or Not to Pretrain](papers/2020/200615%20To%20Pretrain%20or%20Not%20to%20Pretrain) #nlp #finetuning
1. [200626 Pre-training via Paraphrasing](papers/2020/200626%20Pre-training%20via%20Paraphrasing) #retrieval
1. [200703 Language-agnostic BERT Sentence Embedding](papers/2020/200703%20Language-agnostic%20BERT%20Sentence%20Embedding) #embedding #multilingual
1. [200713 An Empirical Study on Robustness to Spurious Correlations using](papers/2020/200713%20An%20Empirical%20Study%20on%20Robustness%20to%20Spurious%20Correlations%20using) #nlp #multitask
1. [200715 InfoXLM](papers/2020/200715%20InfoXLM) #nlp #cross_lingual
1. [200804 Taking Notes on the Fly Helps BERT Pre-training](papers/2020/200804%20Taking%20Notes%20on%20the%20Fly%20Helps%20BERT%20Pre-training) #nlp
1. [201020 Pushing the Limits of Semi-Supervised Learning for Automatic Speech](papers/2020/201020%20Pushing%20the%20Limits%20of%20Semi-Supervised%20Learning%20for%20Automatic%20Speech) #semi_supervised_learning #asr
1. [201021 Self-training and Pre-training are Complementary for Speech Recognition](papers/2020/201021%20Self-training%20and%20Pre-training%20are%20Complementary%20for%20Speech%20Recognition) #self_supervised #asr
1. [201022 mT5](papers/2020/201022%20mT5) #language_model #multilingual
1. [201109 When Do You Need Billions of Words of Pretraining Data](papers/2020/201109%20When%20Do%20You%20Need%20Billions%20of%20Words%20of%20Pretraining%20Data) #language_model
1. [201117 UP-DETR](papers/2020/201117%20UP-DETR) #detr #end2end #object_detection
1. [201127 Progressively Stacking 2.0](papers/2020/201127%20Progressively%20Stacking%202.0) #efficiency
1. [201201 Pre-Trained Image Processing Transformer](papers/2020/201201%20Pre-Trained%20Image%20Processing%20Transformer) #contrastive_learning #vision_transformer #restoration
1. [201201 StructFormer](papers/2020/201201%20StructFormer) #parse #attention #mlm
1. [201227 Syntax-Enhanced Pre-trained Model](papers/2020/201227%20Syntax-Enhanced%20Pre-trained%20Model) #language_model #syntax
1. [210225 SparseBERT](papers/2021/210225%20SparseBERT) #attention #sparse_attention #bert
1. [210318 All NLP Tasks Are Generation Tasks](papers/2021/210318%20All%20NLP%20Tasks%20Are%20Generation%20Tasks) #language_model
1. [210324 Can Vision Transformers Learn without Natural Images](papers/2021/210324%20Can%20Vision%20Transformers%20Learn%20without%20Natural%20Images) #vision_transformer
1. [210402 Robust wav2vec 2.0](papers/2021/210402%20Robust%20wav2vec%202.0) #asr
1. [210407 Pushing the Limits of Non-Autoregressive Speech Recognition](papers/2021/210407%20Pushing%20the%20Limits%20of%20Non-Autoregressive%20Speech%20Recognition) #non-autoregressive #asr #ctc
1. [210413 Masked Language Modeling and the Distributional Hypothesis](papers/2021/210413%20Masked%20Language%20Modeling%20and%20the%20Distributional%20Hypothesis) #language_model #mlm
1. [210417 mT6](papers/2021/210417%20mT6) #language_model
1. [210418 Data-Efficient Language-Supervised Zero-Shot Learning with](papers/2021/210418%20Data-Efficient%20Language-Supervised%20Zero-Shot%20Learning%20with) #multimodal
1. [210422 ImageNet-21K Pretraining for the Masses](papers/2021/210422%20ImageNet-21K%20Pretraining%20for%20the%20Masses) #backbone
1. [210510 Are Pre-trained Convolutions Better than Pre-trained Transformers](papers/2021/210510%20Are%20Pre-trained%20Convolutions%20Better%20than%20Pre-trained%20Transformers) #nlp #convolution #transformer
1. [210606 On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation](papers/2021/210606%20On%20the%20Effectiveness%20of%20Adapter-based%20Tuning%20for%20Pretrained%20Language%20Model%20Adaptation) #finetuning #adapter
1. [210606 Rethinking Training from Scratch for Object Detection](papers/2021/210606%20Rethinking%20Training%20from%20Scratch%20for%20Object%20Detection) #object_detection
1. [210608 DETReg](papers/2021/210608%20DETReg) #detr
1. [210614 SAS](papers/2021/210614%20SAS)
1. [210615 BEiT](papers/2021/210615%20BEiT) #vit #bert
1. [210907 How much pretraining data do language models need to learn syntax](papers/2021/210907%20How%20much%20pretraining%20data%20do%20language%20models%20need%20to%20learn%20syntax) #bert
1. [210910 ReasonBERT](papers/2021/210910%20ReasonBERT) #bert #reasoning #qa
1. [210913 STraTA](papers/2021/210913%20STraTA) #finetuning #semi_supervised_learning #few_shot
1. [210914 Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](papers/2021/210914%20Performance-Efficiency%20Trade-offs%20in%20Unsupervised%20Pre-training%20for%20Speech%20Recognition) #asr
1. [210914 Task-adaptive Pre-training and Self-training are Complementary for Natural Language Understanding](papers/2021/210914%20Task-adaptive%20Pre-training%20and%20Self-training%20are%20Complementary%20for%20Natural%20Language%20Understanding) #finetuning #semi_supervised_learning #few_shot
1. [210927 BigSSL](papers/2021/210927%20BigSSL) #asr #semi_supervised_learning #unsupervised_training
1. [211005 Exploring the Limits of Large Scale Pre-training](papers/2021/211005%20Exploring%20the%20Limits%20of%20Large%20Scale%20Pre-training) #classificiation #scaling
1. [211018 Unsupervised Finetuning](papers/2021/211018%20Unsupervised%20Finetuning) #unsupervised_training #finetuning
1. [211026 WavLM](papers/2021/211026%20WavLM) #speech
1. [211103 VLMo](papers/2021/211103%20VLMo) #mixture_of_experts #vision-language
1. [211111 Masked Autoencoders Are Scalable Vision Learners](papers/2021/211111%20Masked%20Autoencoders%20Are%20Scalable%20Vision%20Learners) #vit
1. [211122 ExT5](papers/2021/211122%20ExT5) #multitask
1. [211122 Florence](papers/2021/211122%20Florence) #vision-language #transfer
1. [211201 Revisiting the Transferability of Supervised Pretraining](papers/2021/211201%20Revisiting%20the%20Transferability%20of%20Supervised%20Pretraining) #transfer
1. [211216 Masked Feature Prediction for Self-Supervised Visual Pre-Training](papers/2021/211216%20Masked%20Feature%20Prediction%20for%20Self-Supervised%20Visual%20Pre-Training) #self_supervised
1. [211220 Are Large-scale Datasets Necessary for Self-Supervised Pre-training](papers/2021/211220%20Are%20Large-scale%20Datasets%20Necessary%20for%20Self-Supervised%20Pre-training) #self_supervised #transfer
## probabilistic model
1. [200413 Einsum Networks](papers/2020/200413%20Einsum%20Networks)
1. [200419 Roundtrip](papers/2020/200419%20Roundtrip)
## prompt
1. [220118 ZeroPrompt](papers/2022/220118%20ZeroPrompt) #zero-shot
## pruning
1. [200130 Rethinking Pruning](papers/2020/200130%20Rethinking%20Pruning)
1. [200218 Picking Winning Tickets Before Training by Preserving Gradient Flow](papers/2020/200218%20Picking%20Winning%20Tickets%20Before%20Training%20by%20Preserving%20Gradient%20Flow) #lottery_ticket
1. [200224 HRank](papers/2020/200224%20HRank) #rank
1. [200305 Comparing Rewinding and Fine-tuning in Neural Network Pruning](papers/2020/200305%20Comparing%20Rewinding%20and%20Fine-tuning%20in%20Neural%20Network%20Pruning)
1. [200424 Convolution-Weight-Distribution Assumption](papers/2020/200424%20Convolution-Weight-Distribution%20Assumption)
1. [200514 Bayesian Bits](papers/2020/200514%20Bayesian%20Bits) #quantization #variational_inference
1. [200515 Movement Pruning](papers/2020/200515%20Movement%20Pruning)
1. [200518 Joint Multi-Dimension Pruning](papers/2020/200518%20Joint%20Multi-Dimension%20Pruning)
1. [200706 Lossless CNN Channel Pruning via Decoupling Remembering and Forgetting](papers/2020/200706%20Lossless%20CNN%20Channel%20Pruning%20via%20Decoupling%20Remembering%20and%20Forgetting)
1. [200710 To Filter Prune, or to Layer Prune, That Is The Question](papers/2020/200710%20To%20Filter%20Prune%2C%20or%20to%20Layer%20Prune%2C%20That%20Is%20The%20Question)
## qa
1. [200222 Unsupervised Question Decomposition for Question Answering](papers/2020/200222%20Unsupervised%20Question%20Decomposition%20for%20Question%20Answering)
## reasoning
1. [200129 Neural Arithmetic Units](papers/2020/200129%20Neural%20Arithmetic%20Units)
1. [200409 Injecting Numerical Reasoning Skills into Language Models](papers/2020/200409%20Injecting%20Numerical%20Reasoning%20Skills%20into%20Language%20Models)
## regularization
1. [200130 DropAttention](papers/2020/200130%20DropAttention) #dropout
1. [200219 Revisiting Training Strategies and Generalization Performance in Deep](papers/2020/200219%20Revisiting%20Training%20Strategies%20and%20Generalization%20Performance%20in%20Deep) #metric_learning
1. [200225 On Feature Normalization and Data Augmentation](papers/2020/200225%20On%20Feature%20Normalization%20and%20Data%20Augmentation) #normalization #mixup
1. [200228 The Implicit and Explicit Regularization Effects of Dropout](papers/2020/200228%20The%20Implicit%20and%20Explicit%20Regularization%20Effects%20of%20Dropout) #dropout
1. [200331 Regularizing Class-wise Predictions via Self-knowledge Distillation](papers/2020/200331%20Regularizing%20Class-wise%20Predictions%20via%20Self-knowledge%20Distillation) #distillation #consistency_regularization
1. [200409 Orthogonal Over-Parameterized Training](papers/2020/200409%20Orthogonal%20Over-Parameterized%20Training)
1. [200424 Dropout as an Implicit Gating Mechanism For Continual Learning](papers/2020/200424%20Dropout%20as%20an%20Implicit%20Gating%20Mechanism%20For%20Continual%20Learning)
1. [200427 Scheduled DropHead](papers/2020/200427%20Scheduled%20DropHead)
1. [200513 Implicit Regularization in Deep Learning May Not Be Explainable by Norms](papers/2020/200513%20Implicit%20Regularization%20in%20Deep%20Learning%20May%20Not%20Be%20Explainable%20by%20Norms) #training #optimization
1. [200707 RIFLE](papers/2020/200707%20RIFLE) #finetuning
1. [200707 Remix](papers/2020/200707%20Remix) #imbalanced
1. [200721 Improving compute efficacy frontiers with SliceOut](papers/2020/200721%20Improving%20compute%20efficacy%20frontiers%20with%20SliceOut) #efficient_training
1. [201122 Stable Weight Decay Regularization](papers/2020/201122%20Stable%20Weight%20Decay%20Regularization)
## reinforcement learning
1. [191120 Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model](papers/2019/191120%20Mastering%20Atari%2C%20Go%2C%20Chess%20and%20Shogi%20by%20Planning%20with%20a%20Learned%20Model)
1. [200130 Mastering Atari, Go, Chess, Shogi](papers/2020/200130%20Mastering%20Atari%2C%20Go%2C%20Chess%2C%20Shogi)
1. [200626 Critic Regularized Regression](papers/2020/200626%20Critic%20Regularized%20Regression)
1. [210929 Vision-Guided Quadrupedal Locomotion in the Wild with Multi-Modal Delay Randomization](papers/2021/210929%20Vision-Guided%20Quadrupedal%20Locomotion%20in%20the%20Wild%20with%20Multi-Modal%20Delay%20Randomization)
1. [211030 Mastering Atari Games with Limited Data](papers/2021/211030%20Mastering%20Atari%20Games%20with%20Limited%20Data)
## rendering
1. [200130 Textured Neural Avatars](papers/2020/200130%20Textured%20Neural%20Avatars)
## representation
1. [200412 Gradients as Features for Deep Representation Learning](papers/2020/200412%20Gradients%20as%20Features%20for%20Deep%20Representation%20Learning)
## resampling
1. [200512 Invertible Image Rescaling](papers/2020/200512%20Invertible%20Image%20Rescaling)
## restoration
1. [200402 Learning to See Through Obstructions](papers/2020/200402%20Learning%20to%20See%20Through%20Obstructions)
1. [200404 Deblurring by Realistic Blurring](papers/2020/200404%20Deblurring%20by%20Realistic%20Blurring)
1. [200406 Self-Supervised Scene De-occlusion](papers/2020/200406%20Self-Supervised%20Scene%20De-occlusion)
1. [200420 Bringing Old Photos Back to Life](papers/2020/200420%20Bringing%20Old%20Photos%20Back%20to%20Life) #vae
1. [201123 Cross-Camera Convolutional Color Constancy](papers/2020/201123%20Cross-Camera%20Convolutional%20Color%20Constancy)
1. [201123 Dissecting Image Crops](papers/2020/201123%20Dissecting%20Image%20Crops)
## retrieval
1. [210715 Internet-Augmented Dialogue Generation](papers/2021/210715%20Internet-Augmented%20Dialogue%20Generation) #dialog
1. [220124 Text and Code Embeddings by Contrastive Pre-Training](papers/2022/220124%20Text%20and%20Code%20Embeddings%20by%20Contrastive%20Pre-Training)
## review
1. [191210 Thoughts on recent papers](papers/2019/191210%20Thoughts%20on%20recent%20papers)
1. [200130 Filter Response Normalization](papers/2020/200130%20Filter%20Response%20Normalization)
1. [200227 A Primer in BERTology](papers/2020/200227%20A%20Primer%20in%20BERTology) #bert
1. [200306 What is the State of Neural Network Pruning](papers/2020/200306%20What%20is%20the%20State%20of%20Neural%20Network%20Pruning) #pruning
1. [200311 Improved Baselines with Momentum Contrastive Learning](papers/2020/200311%20Improved%20Baselines%20with%20Momentum%20Contrastive%20Learning) #contrastive_learning
1. [200318 A Metric Learning Reality Check](papers/2020/200318%20A%20Metric%20Learning%20Reality%20Check) #metric_learning
1. [200323 Thoughts on recent papers](papers/2020/200323%20Thoughts%20on%20recent%20papers)
1. [200324 A Systematic Evaluation](papers/2020/200324%20A%20Systematic%20Evaluation)
1. [200325 Rethinking Few-Shot Image Classification](papers/2020/200325%20Rethinking%20Few-Shot%20Image%20Classification) #meta_learning
1. [200326 Thoughts on recent papers](papers/2020/200326%20Thoughts%20on%20recent%20papers)
1. [200403 Thoughts on recent papers](papers/2020/200403%20Thoughts%20on%20recent%20papers)
1. [200408 State of the Art on Neural Rendering](papers/2020/200408%20State%20of%20the%20Art%20on%20Neural%20Rendering) #neural_rendering
1. [200409 EvoNorm](papers/2020/200409%20EvoNorm)
1. [200411 Thoughts on recent papers](papers/2020/200411%20Thoughts%20on%20recent%20papers)
1. [200428 Showing Your Work Doesn't Always Work](papers/2020/200428%20Showing%20Your%20Work%20Doesn%27t%20Always%20Work)
1. [200619 Augmentation for GANs](papers/2020/200619%20Augmentation%20for%20GANs)
1. [200627 Denoising Diffusion Probabilistic Models Implementation](papers/2020/200627%20Denoising%20Diffusion%20Probabilistic%20Models%20Implementation)
1. [200708 Thoughts on recent papers](papers/2020/200708%20Thoughts%20on%20recent%20papers)
1. [200717 Semantic factor of GANs](papers/2020/200717%20Semantic%20factor%20of%20GANs)
1. [200717 Thoughts on recent papers](papers/2020/200717%20Thoughts%20on%20recent%20papers)
1. [200725 Neighbor Embedding](papers/2020/200725%20Neighbor%20Embedding)
1. [200726 Thoughts on recent papers](papers/2020/200726%20Thoughts%20on%20recent%20papers)
1. [200802 Thoughts on recent papers](papers/2020/200802%20Thoughts%20on%20recent%20papers)
1. [200821 Virtual Try On](papers/2020/200821%20Virtual%20Try%20On)
1. [201016 Representation Learning via Invariant Causal Mechanisms](papers/2020/201016%20Representation%20Learning%20via%20Invariant%20Causal%20Mechanisms)
1. [201021 BYOL works even without batch statistics](papers/2020/201021%20BYOL%20works%20even%20without%20batch%20statistics)
1. [201108 Long Range Arena](papers/2020/201108%20Long%20Range%20Arena) #attention #efficient_attention
1. [201112 Learning Semantic-aware Normalization for Generative Adversarial Networks](papers/2020/201112%20Learning%20Semantic-aware%20Normalization%20for%20Generative%20Adversarial%20Networks)
1. [201112 When Do You Need Billions of Words of Pretraining Data](papers/2020/201112%20When%20Do%20You%20Need%20Billions%20of%20Words%20of%20Pretraining%20Data)
1. [201118 Thoughts on recent papers](papers/2020/201118%20Thoughts%20on%20recent%20papers)
1. [201120 Thoughts on recent papers](papers/2020/201120%20Thoughts%20on%20recent%20papers)
1. [201125 Thoughts on recent papers](papers/2020/201125%20Thoughts%20on%20recent%20papers)
1. [201126 Thoughts on recent papers 1](papers/2020/201126%20Thoughts%20on%20recent%20papers%201)
1. [201126 Thoughts on recent papers 2](papers/2020/201126%20Thoughts%20on%20recent%20papers%202)
1. [201204 Thoughts on recent papers](papers/2020/201204%20Thoughts%20on%20recent%20papers)
1. [210121 Thoughts on recent papers](papers/2021/210121%20Thoughts%20on%20recent%20papers)
1. [210227 Thoughts on recent papers](papers/2021/210227%20Thoughts%20on%20recent%20papers)
1. [210305 Thoughts on recent papers](papers/2021/210305%20Thoughts%20on%20recent%20papers)
1. [210319 Thoughts on recent papers](papers/2021/210319%20Thoughts%20on%20recent%20papers)
1. [210323 Thoughts on recent papers](papers/2021/210323%20Thoughts%20on%20recent%20papers)
1. [210324 A Broad Study on the Transferability of Visual Representations with Contrastive Learning](papers/2021/210324%20A%20Broad%20Study%20on%20the%20Transferability%20of%20Visual%20Representations%20with%20Contrastive%20Learning) #contrastive_learning
1. [210325 Contrasting Contrastive Self-Supervised Representation Learning Models](papers/2021/210325%20Contrasting%20Contrastive%20Self-Supervised%20Representation%20Learning%20Models) #contrastive_learning
1. [210326 Thoughts on recent papers](papers/2021/210326%20Thoughts%20on%20recent%20papers)
1. [210403 Thoughts on recent papers](papers/2021/210403%20Thoughts%20on%20recent%20papers)
1. [210412 Thoughts on recent papers](papers/2021/210412%20Thoughts%20on%20recent%20papers)
1. [210424 Thoughts on recent papers](papers/2021/210424%20Thoughts%20on%20recent%20papers)
1. [210429 Thoughts on recent papers](papers/2021/210429%20Thoughts%20on%20recent%20papers)
1. [210430 Thoughts on recent papers 1](papers/2021/210430%20Thoughts%20on%20recent%20papers%201)
1. [210430 Thoughts on recent papers 2](papers/2021/210430%20Thoughts%20on%20recent%20papers%202)
1. [210505 Thoughts on recent papers](papers/2021/210505%20Thoughts%20on%20recent%20papers)
1. [210508 Thoughts on recent papers](papers/2021/210508%20Thoughts%20on%20recent%20papers)
1. [210512 When Does Contrastive Visual Representation Learning Work](papers/2021/210512%20When%20Does%20Contrastive%20Visual%20Representation%20Learning%20Work) #contrastive_learning #self_supervised #transfer
## robustness
1. [200211 Fundamental Tradeoffs between Invariance and Sensitivity to Adversarial](papers/2020/200211%20Fundamental%20Tradeoffs%20between%20Invariance%20and%20Sensitivity%20to%20Adversarial) #adversarial_training
1. [200304 A Closer Look at Accuracy vs. Robustness](papers/2020/200304%20A%20Closer%20Look%20at%20Accuracy%20vs.%20Robustness) #adversarial_training
1. [200810 Informative Dropout for Robust Representation Learning](papers/2020/200810%20Informative%20Dropout%20for%20Robust%20Representation%20Learning)
## saliency
1. [200406 There and Back Again](papers/2020/200406%20There%20and%20Back%20Again)
## salient object detection
1. [200518 U$^2$-Net](papers/2020/200518%20U%24%5E2%24-Net)
## scale
1. [200712 Learning to Learn Parameterized Classification Networks for Scalable](papers/2020/200712%20Learning%20to%20Learn%20Parameterized%20Classification%20Networks%20for%20Scalable) #hypernetwork
1. [201130 Towards Better Accuracy-efficiency Trade-offs](papers/2020/201130%20Towards%20Better%20Accuracy-efficiency%20Trade-offs)
## score
1. [200319 GIQA](papers/2020/200319%20GIQA)
1. [200426 Evaluation Metrics for Conditional Image Generation](papers/2020/200426%20Evaluation%20Metrics%20for%20Conditional%20Image%20Generation)
## self supervised
1. [200213 Automatically Discovering and Learning New Visual Categories with Ranking Statistics](papers/2020/200213%20Automatically%20Discovering%20and%20Learning%20New%20Visual%20Categories%20with%20Ranking%20Statistics) #weak_supervision
1. [200218 MAST](papers/2020/200218%20MAST) #tracking
1. [200224 Self-Adaptive Training](papers/2020/200224%20Self-Adaptive%20Training) #noise #dataset
1. [200408 Improving BERT with Self-Supervised Attention](papers/2020/200408%20Improving%20BERT%20with%20Self-Supervised%20Attention) #bert #distillation
1. [200722 CrossTransformers](papers/2020/200722%20CrossTransformers) #few_shot
1. [201015 Representation Learning via Invariant Causal Mechanisms](papers/2020/201015%20Representation%20Learning%20via%20Invariant%20Causal%20Mechanisms) #causality
1. [201117 Neural Semi-supervised Learning for Text Classification Under](papers/2020/201117%20Neural%20Semi-supervised%20Learning%20for%20Text%20Classification%20Under) #nlp
1. [201125 Can Temporal Information Help with Contrastive Self-Supervised Learning](papers/2020/201125%20Can%20Temporal%20Information%20Help%20with%20Contrastive%20Self-Supervised%20Learning) #video #augmentation
1. [201224 Self-supervised Pre-training with Hard Examples Improves Visual](papers/2020/201224%20Self-supervised%20Pre-training%20with%20Hard%20Examples%20Improves%20Visual) #mixup
1. [210726 Continental-Scale Building Detection from High Resolution Satellite Imagery](papers/2021/210726%20Continental-Scale%20Building%20Detection%20from%20High%20Resolution%20Satellite%20Imagery)
1. [210927 Compressive Visual Representations](papers/2021/210927%20Compressive%20Visual%20Representations)
1. [211027 Neural Analysis and Synthesis](papers/2021/211027%20Neural%20Analysis%20and%20Synthesis) #audio_synthesis
1. [220124 data2vec](papers/2022/220124%20data2vec)
1. [220216 Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision](papers/2022/220216%20Vision%20Models%20Are%20More%20Robust%20And%20Fair%20When%20Pretrained%20On%20Uncurated%20Images%20Without%20Supervision)
## self supervised discovery
1. [200403 Self-Supervised Viewpoint Learning From Image Collections](papers/2020/200403%20Self-Supervised%20Viewpoint%20Learning%20From%20Image%20Collections) #viewpoint
1. [201127 Unsupervised part representation by Flow Capsules](papers/2020/201127%20Unsupervised%20part%20representation%20by%20Flow%20Capsules)
1. [210429 MarioNette](papers/2021/210429%20MarioNette)
## semantic factor
1. [200307 StyleGAN2 Distillation for Feed-forward Image Manipulation](papers/2020/200307%20StyleGAN2%20Distillation%20for%20Feed-forward%20Image%20Manipulation) #stylegan
1. [200308 PULSE](papers/2020/200308%20PULSE) #stylegan
1. [200406 GANSpace](papers/2020/200406%20GANSpace)
1. [201127 Navigating the GAN Parameter Space for Semantic Image Editing](papers/2020/201127%20Navigating%20the%20GAN%20Parameter%20Space%20for%20Semantic%20Image%20Editing) #image_editing
1. [201222 Time-Travel Rephotography](papers/2020/201222%20Time-Travel%20Rephotography) #restoration #stylegan
## semantic segmentation
1. [200323 Learning Dynamic Routing for Semantic Segmentation](papers/2020/200323%20Learning%20Dynamic%20Routing%20for%20Semantic%20Segmentation)
1. [200516 Single-Stage Semantic Segmentation from Image Labels](papers/2020/200516%20Single-Stage%20Semantic%20Segmentation%20from%20Image%20Labels)
1. [200826 EfficientFCN](papers/2020/200826%20EfficientFCN)
1. [210512 Segmenter](papers/2021/210512%20Segmenter)
## semi supervised learning
1. [200218 DivideMix](papers/2020/200218%20DivideMix) #mixup #noise #dataset
1. [200306 Semi-Supervised StyleGAN for Disentanglement Learning](papers/2020/200306%20Semi-Supervised%20StyleGAN%20for%20Disentanglement%20Learning) #stylegan #mixup
1. [200323 Meta Pseudo Labels](papers/2020/200323%20Meta%20Pseudo%20Labels) #meta_learning
1. [200627 Laplacian Regularized Few-Shot Learning](papers/2020/200627%20Laplacian%20Regularized%20Few-Shot%20Learning) #few_shot
1. [200724 Deep Co-Training with Task Decomposition for Semi-Supervised Domain](papers/2020/200724%20Deep%20Co-Training%20with%20Task%20Decomposition%20for%20Semi-Supervised%20Domain) #domain_adaptation
1. [201116 On the Marginal Benefit of Active Learning](papers/2020/201116%20On%20the%20Marginal%20Benefit%20of%20Active%20Learning) #active_learning #unsupervised_training
1. [201118 FROST](papers/2020/201118%20FROST)
## sgld
1. [200706 Kernel Stein Generative Modeling](papers/2020/200706%20Kernel%20Stein%20Generative%20Modeling) #svgd
## singing voice synthesis
1. [211008 KaraSinger](papers/2021/211008%20KaraSinger)
## single image
1. [200405 Structural-analogy from a Single Image Pair](papers/2020/200405%20Structural-analogy%20from%20a%20Single%20Image%20Pair)
## speech
1. [200129 Speech Recognition](papers/2020/200129%20Speech%20Recognition)
1. [200129 WaveFlow](papers/2020/200129%20WaveFlow) #conditional_generative_model
## state space model
1. [211031 Efficiently Modeling Long Sequences with Structured State Spaces](papers/2021/211031%20Efficiently%20Modeling%20Long%20Sequences%20with%20Structured%20State%20Spaces)
## structure learning
1. [200518 Large-scale empirical validation of Bayesian Network structure learning](papers/2020/200518%20Large-scale%20empirical%20validation%20of%20Bayesian%20Network%20structure%20learning)
## style transfer
1. [200318 A Content Transformation Block For Image Style Transfer](papers/2020/200318%20A%20Content%20Transformation%20Block%20For%20Image%20Style%20Transfer)
1. [200324 Deformable Style Transfer](papers/2020/200324%20Deformable%20Style%20Transfer)
1. [200710 Geometric Style Transfer](papers/2020/200710%20Geometric%20Style%20Transfer)
## stylegan
1. [200803 Encoding in Style](papers/2020/200803%20Encoding%20in%20Style) #gan_inversion
1. [210318 Labels4Free](papers/2021/210318%20Labels4Free) #unsupervised_segmentation
## super resolution
1. [200129 ESRGAN+](papers/2020/200129%20ESRGAN%2B)
1. [200323 Deep Unfolding Network for Image Super-Resolution](papers/2020/200323%20Deep%20Unfolding%20Network%20for%20Image%20Super-Resolution)
## table
1. [210906 Parsing Table Structures in the Wild](papers/2021/210906%20Parsing%20Table%20Structures%20in%20the%20Wild)
## text generation
1. [200130 Unlikelihood Training](papers/2020/200130%20Unlikelihood%20Training)
1. [200605 CoCon](papers/2020/200605%20CoCon)
## tokenizer
1. [211006 How BPE Affects Memorization in Transformers](papers/2021/211006%20How%20BPE%20Affects%20Memorization%20in%20Transformers)
## topic model
1. [200426 Neural Topic Modeling with Bidirectional Adversarial Training](papers/2020/200426%20Neural%20Topic%20Modeling%20with%20Bidirectional%20Adversarial%20Training)
## topology
1. [200413 Topology of deep neural networks](papers/2020/200413%20Topology%20of%20deep%20neural%20networks) #theory
## tracking
1. [200402 Tracking Objects as Points](papers/2020/200402%20Tracking%20Objects%20as%20Points) #keypoint
1. [200402 Tracking by Instance Detection](papers/2020/200402%20Tracking%20by%20Instance%20Detection) #meta_learning
1. [200403 FairMOT](papers/2020/200403%20FairMOT)
1. [200506 PeTra](papers/2020/200506%20PeTra)
1. [201215 Detecting Invisible People](papers/2020/201215%20Detecting%20Invisible%20People)
1. [211013 ByteTrack](papers/2021/211013%20ByteTrack)
## training
1. [200702 Beyond Signal Propagation](papers/2020/200702%20Beyond%20Signal%20Propagation)
## transducer
1. [200519 A New Training Pipeline for an Improved Neural Transducer](papers/2020/200519%20A%20New%20Training%20Pipeline%20for%20an%20Improved%20Neural%20Transducer)
## transfer
1. [200130 BiT ResNet](papers/2020/200130%20BiT%20ResNet) #resnet
1. [200512 Neural Architecture Transfer](papers/2020/200512%20Neural%20Architecture%20Transfer) #nas
1. [200711 Adversarially-Trained Deep Nets Transfer Better](papers/2020/200711%20Adversarially-Trained%20Deep%20Nets%20Transfer%20Better) #adversarial_training
1. [200716 Do Adversarially Robust ImageNet Models Transfer Better](papers/2020/200716%20Do%20Adversarially%20Robust%20ImageNet%20Models%20Transfer%20Better) #robust
1. [200721 Adversarial Training Reduces Information and Improves Transferability](papers/2020/200721%20Adversarial%20Training%20Reduces%20Information%20and%20Improves%20Transferability) #adversarial_training
1. [201122 Ranking Neural Checkpoints](papers/2020/201122%20Ranking%20Neural%20Checkpoints)
1. [211012 Rethinking supervised pre-training for better downstream transferring](papers/2021/211012%20Rethinking%20supervised%20pre-training%20for%20better%20downstream%20transferring) #classificiation #metric_learning
## transformer
1. [200129 Are Transformers universal approximator](papers/2020/200129%20Are%20Transformers%20universal%20approximator)
1. [200129 Product Key Memory](papers/2020/200129%20Product%20Key%20Memory) #attention
1. [200129 Reformer](papers/2020/200129%20Reformer) #attention
1. [200130 Sparse Transformer](papers/2020/200130%20Sparse%20Transformer) #generative_model
1. [200130 Structured Pruning for LM](papers/2020/200130%20Structured%20Pruning%20for%20LM) #pruning
1. [200207 Transformer Transducer](papers/2020/200207%20Transformer%20Transducer) #asr #transducer
1. [200211 On Layer Normalization in the Transformer Architecture](papers/2020/200211%20On%20Layer%20Normalization%20in%20the%20Transformer%20Architecture) #normalization
1. [200212 GLU Variants Improve Transformer](papers/2020/200212%20GLU%20Variants%20Improve%20Transformer) #activation
1. [200214 Transformer on a Diet](papers/2020/200214%20Transformer%20on%20a%20Diet) #efficient_attention
1. [200214 Transformers as Soft Reasoners over Language](papers/2020/200214%20Transformers%20as%20Soft%20Reasoners%20over%20Language) #language
1. [200215 Fine-Tuning Pretrained Language Models](papers/2020/200215%20Fine-Tuning%20Pretrained%20Language%20Models) #bert #finetuning
1. [200221 Addressing Some Limitations of Transformers with Feedback Memory](papers/2020/200221%20Addressing%20Some%20Limitations%20of%20Transformers%20with%20Feedback%20Memory) #recurrent
1. [200305 Talking-Heads Attention](papers/2020/200305%20Talking-Heads%20Attention) #attention
1. [200424 Lite Transformer with Long-Short Range Attention](papers/2020/200424%20Lite%20Transformer%20with%20Long-Short%20Range%20Attention) #lightweight
1. [200515 Finding Experts in Transformer Models](papers/2020/200515%20Finding%20Experts%20in%20Transformer%20Models)
1. [200515 JDI-T](papers/2020/200515%20JDI-T) #tts
1. [200516 Conformer](papers/2020/200516%20Conformer) #asr
1. [200518 Weak-Attention Suppression For Transformer Based Speech Recognition](papers/2020/200518%20Weak-Attention%20Suppression%20For%20Transformer%20Based%20Speech%20Recognition) #asr
1. [200605 Funnel-Transformer](papers/2020/200605%20Funnel-Transformer) #efficient_attention
1. [200707 Do Transformers Need Deep Long-Range Memory](papers/2020/200707%20Do%20Transformers%20Need%20Deep%20Long-Range%20Memory) #lm #attention
1. [200709 Fast Transformers with Clustered Attention](papers/2020/200709%20Fast%20Transformers%20with%20Clustered%20Attention) #attention
1. [200715 AdapterHub](papers/2020/200715%20AdapterHub) #nlp #finetuning
1. [200727 Big Bird](papers/2020/200727%20Big%20Bird) #attention
1. [200802 DeLighT](papers/2020/200802%20DeLighT) #nlp
1. [201217 Taming Transformers for High-Resolution Image Synthesis](papers/2020/201217%20Taming%20Transformers%20for%20High-Resolution%20Image%20Synthesis) #discrete_vae #generative_model #autoregressive_model
1. [201221 RealFormer](papers/2020/201221%20RealFormer) #attention
1. [201227 SG-Net](papers/2020/201227%20SG-Net) #syntax #attention
1. [210223 Do Transformer Modifications Transfer Across Implementations and](papers/2021/210223%20Do%20Transformer%20Modifications%20Transfer%20Across%20Implementations%20and)
1. [210225 Evolving Attention with Residual Convolutions](papers/2021/210225%20Evolving%20Attention%20with%20Residual%20Convolutions) #attention
1. [210318 HiT](papers/2021/210318%20HiT) #video #retrieval
1. [210318 Looking Beyond Two Frames](papers/2021/210318%20Looking%20Beyond%20Two%20Frames) #tracking
1. [210318 TFPose](papers/2021/210318%20TFPose) #pose
1. [210318 TransCenter](papers/2021/210318%20TransCenter) #tracking
1. [210318 Transformer Trackin](papers/2021/210318%20Transformer%20Trackin) #tracking
1. [210407 Seeing Out of tHe bOx](papers/2021/210407%20Seeing%20Out%20of%20tHe%20bOx) #multimodal #vision-language
1. [210409 Efficient Large-Scale Language Model Training on GPU Clusters](papers/2021/210409%20Efficient%20Large-Scale%20Language%20Model%20Training%20on%20GPU%20Clusters) #distributed_training
1. [210409 Not All Attention Is All You Need](papers/2021/210409%20Not%20All%20Attention%20Is%20All%20You%20Need)
1. [210410 UniDrop](papers/2021/210410%20UniDrop) #regularization
1. [210417 Demystifying the Better Performance of Position Encoding Variants for](papers/2021/210417%20Demystifying%20the%20Better%20Performance%20of%20Position%20Encoding%20Variants%20for) #positional_encoding
1. [210420 RoFormer](papers/2021/210420%20RoFormer) #positional_encoding
1. [210423 M3DeTR](papers/2021/210423%20M3DeTR) #3d
1. [210509 FNet](papers/2021/210509%20FNet) #efficient_attention #fourier
1. [210613 Thinking Like Transformers](papers/2021/210613%20Thinking%20Like%20Transformers)
1. [210617 Multi-head or Single-head](papers/2021/210617%20Multi-head%20or%20Single-head)
1. [210730 Perceiver IO](papers/2021/210730%20Perceiver%20IO)
1. [210809 Making Transformers Solve Compositional Tasks](papers/2021/210809%20Making%20Transformers%20Solve%20Compositional%20Tasks)
1. [210812 Mobile-Former](papers/2021/210812%20Mobile-Former) #backbone
1. [210830 A Battle of Network Structures](papers/2021/210830%20A%20Battle%20of%20Network%20Structures) #cnn #mlp #backbone
1. [210830 Shatter](papers/2021/210830%20Shatter) #bert
1. [210908 Panoptic SegFormer](papers/2021/210908%20Panoptic%20SegFormer) #panoptic_segmentation #detr
1. [210909 Bag of Tricks for Optimizing Transformer Efficiency](papers/2021/210909%20Bag%20of%20Tricks%20for%20Optimizing%20Transformer%20Efficiency) #nmt #lightweight
1. [210917 Primer](papers/2021/210917%20Primer) #lm #nas
1. [210922 Scale Efficiently](papers/2021/210922%20Scale%20Efficiently)
1. [211018 NormFormer](papers/2021/211018%20NormFormer)
1. [211026 Hierarchical Transformers Are More Efficient Language Models](papers/2021/211026%20Hierarchical%20Transformers%20Are%20More%20Efficient%20Language%20Models) #lm #efficient_attention
1. [211122 MetaFormer is Actually What You Need for Vision](papers/2021/211122%20MetaFormer%20is%20Actually%20What%20You%20Need%20for%20Vision) #vit
1. [211124 Sparse is Enough in Scaling Transformers](papers/2021/211124%20Sparse%20is%20Enough%20in%20Scaling%20Transformers) #sparsity #efficiency
1. [220221 Transformer Quality in Linear Time](papers/2022/220221%20Transformer%20Quality%20in%20Linear%20Time) #efficient_attention #linear_attention #local_attention
1. [220301 DeepNet](papers/2022/220301%20DeepNet) #normalization
1. [220330 Transformer Language Models without Positional Encodings Still Learn Positional Information](papers/2022/220330%20Transformer%20Language%20Models%20without%20Positional%20Encodings%20Still%20Learn%20Positional%20Information) #lm #positional_encoding
## tropical geometry
1. [200220 On the Decision Boundaries of Neural Networks](papers/2020/200220%20On%20the%20Decision%20Boundaries%20of%20Neural%20Networks)
## tts
1. [200512 Flowtron](papers/2020/200512%20Flowtron) #flow
1. [210617 WaveGrad 2](papers/2021/210617%20WaveGrad%202)
## uncertainty
1. [210727 A Tale Of Two Long Tails](papers/2021/210727%20A%20Tale%20Of%20Two%20Long%20Tails)
## unsupervised img2img
1. [200310 Unpaired Image-to-Image Translation using Adversarial Consistency Loss](papers/2020/200310%20Unpaired%20Image-to-Image%20Translation%20using%20Adversarial%20Consistency%20Loss)
1. [200611 Rethinking the Truly Unsupervised Image-to-Image Translation](papers/2020/200611%20Rethinking%20the%20Truly%20Unsupervised%20Image-to-Image%20Translation)
1. [201201 Unpaired Image-to-Image Translation via Latent Energy Transport](papers/2020/201201%20Unpaired%20Image-to-Image%20Translation%20via%20Latent%20Energy%20Transport)
## unsupervised nmt
1. [200422 When and Why is Unsupervised Neural Machine Translation Useless](papers/2020/200422%20When%20and%20Why%20is%20Unsupervised%20Neural%20Machine%20Translation%20Useless)
## vae
1. [200707 NVAE](papers/2020/200707%20NVAE)
1. [201119 Dual Contradistinctive Generative Autoencoder](papers/2020/201119%20Dual%20Contradistinctive%20Generative%20Autoencoder)
1. [201120 Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them](papers/2020/201120%20Very%20Deep%20VAEs%20Generalize%20Autoregressive%20Models%20and%20Can%20Outperform%20Them)
## video
1. [210325 An Image is Worth 16x16 Words, What is a Video Worth](papers/2021/210325%20An%20Image%20is%20Worth%2016x16%20Words%2C%20What%20is%20a%20Video%20Worth)
## video transformer
1. [210423 VidTr](papers/2021/210423%20VidTr)
## vision
1. [200305 Optimizing JPEG Quantization for Classification Networks](papers/2020/200305%20Optimizing%20JPEG%20Quantization%20for%20Classification%20Networks)
1. [201127 Field of Junctions](papers/2020/201127%20Field%20of%20Junctions)
## vision language
1. [201212 MiniVLM](papers/2020/201212%20MiniVLM)
1. [201222 Seeing past words](papers/2020/201222%20Seeing%20past%20words)
1. [210407 Multimodal Fusion Refiner Networks](papers/2021/210407%20Multimodal%20Fusion%20Refiner%20Networks)
1. [210727 Is Object Detection Necessary for Human-Object Interaction Recognition](papers/2021/210727%20Is%20Object%20Detection%20Necessary%20for%20Human-Object%20Interaction%20Recognition) #human-object-interaction
1. [220221 Vision-Language Pre-Training with Triple Contrastive Learning](papers/2022/220221%20Vision-Language%20Pre-Training%20with%20Triple%20Contrastive%20Learning)
## vision transformer
1. [201127 General Multi-label Image Classification with Transformers](papers/2020/201127%20General%20Multi-label%20Image%20Classification%20with%20Transformers)
1. [201223 A Survey on Visual Transformer](papers/2020/201223%20A%20Survey%20on%20Visual%20Transformer)
1. [201223 Training data-efficient image transformers & distillation through](papers/2020/201223%20Training%20data-efficient%20image%20transformers%20%26%20distillation%20through) #distillation
1. [210223 Pyramid Vision Transformer](papers/2021/210223%20Pyramid%20Vision%20Transformer)
1. [210318 CrossViT](papers/2021/210318%20CrossViT)
1. [210318 CvT](papers/2021/210318%20CvT)
1. [210318 Multi-Scale Vision Longformer](papers/2021/210318%20Multi-Scale%20Vision%20Longformer)
1. [210319 ConViT](papers/2021/210319%20ConViT)
1. [210319 Scalable Visual Transformers with Hierarchical Pooling](papers/2021/210319%20Scalable%20Visual%20Transformers%20with%20Hierarchical%20Pooling)
1. [210324 Vision Transformers for Dense Prediction](papers/2021/210324%20Vision%20Transformers%20for%20Dense%20Prediction) #fpn
1. [210325 Swin Transformer](papers/2021/210325%20Swin%20Transformer) #local_attention
1. [210331 Going deeper with Image Transformers](papers/2021/210331%20Going%20deeper%20with%20Image%20Transformers)
1. [210402 LeViT](papers/2021/210402%20LeViT)
1. [210421 Token Labeling](papers/2021/210421%20Token%20Labeling)
1. [210422 Multiscale Vision Transformers](papers/2021/210422%20Multiscale%20Vision%20Transformers)
1. [210422 So-ViT](papers/2021/210422%20So-ViT)
1. [210426 Improve Vision Transformers Training by Suppressing Over-smoothing](papers/2021/210426%20Improve%20Vision%20Transformers%20Training%20by%20Suppressing%20Over-smoothing)
1. [210426 Visformer](papers/2021/210426%20Visformer)
1. [210427 ConTNet](papers/2021/210427%20ConTNet)
1. [210428 Twins](papers/2021/210428%20Twins) #local_attention #positional_encoding
1. [210509 Conformer](papers/2021/210509%20Conformer)
1. [210515 Are Convolutional Neural Networks or Transformers more like human vision](papers/2021/210515%20Are%20Convolutional%20Neural%20Networks%20or%20Transformers%20more%20like%20human%20vision) #cnn #inductive_bias
1. [210517 Rethinking the Design Principles of Robust Vision Transformer](papers/2021/210517%20Rethinking%20the%20Design%20Principles%20of%20Robust%20Vision%20Transformer) #robustness
## visual grounding
1. [210401 Towards General Purpose Vision Systems](papers/2021/210401%20Towards%20General%20Purpose%20Vision%20Systems)
1. [210510 Visual Grounding with Transformers](papers/2021/210510%20Visual%20Grounding%20with%20Transformers)
## vit
1. [210521 Intriguing Properties of Vision Transformers](papers/2021/210521%20Intriguing%20Properties%20of%20Vision%20Transformers) #robustness
1. [210526 Aggregating Nested Transformers](papers/2021/210526%20Aggregating%20Nested%20Transformers) #local_attention
1. [210529 Less is More](papers/2021/210529%20Less%20is%20More)
1. [210603 DynamicViT](papers/2021/210603%20DynamicViT) #sparse_attention
1. [210603 When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations](papers/2021/210603%20When%20Vision%20Transformers%20Outperform%20ResNets%20without%20Pretraining%20or%20Strong%20Data%20Augmentations) #regularization
1. [210604 RegionViT](papers/2021/210604%20RegionViT) #local_attention
1. [210607 Shuffle Transformer](papers/2021/210607%20Shuffle%20Transformer)
1. [210608 Scaling Vision Transformers](papers/2021/210608%20Scaling%20Vision%20Transformers) #scale
1. [210609 CoAtNet](papers/2021/210609%20CoAtNet)
1. [210614 Delving Deep into the Generalization of Vision Transformers under Distribution Shifts](papers/2021/210614%20Delving%20Deep%20into%20the%20Generalization%20of%20Vision%20Transformers%20under%20Distribution%20Shifts) #robustness
1. [210615 Revisiting the Calibration of Modern Neural Networks](papers/2021/210615%20Revisiting%20the%20Calibration%20of%20Modern%20Neural%20Networks) #mlp #calibration
1. [210617 XCiT](papers/2021/210617%20XCiT) #efficient_attention
1. [210624 Exploring Corruption Robustness](papers/2021/210624%20Exploring%20Corruption%20Robustness) #robustness #mlp
1. [210624 VOLO](papers/2021/210624%20VOLO) #efficient_attention
1. [210624 Video Swin Transformer](papers/2021/210624%20Video%20Swin%20Transformer) #local_attention #video #video_transformer
1. [210701 CSWin Transformer](papers/2021/210701%20CSWin%20Transformer) #efficient_attention #local_attention
1. [210701 Focal Self-attention for Local-Global Interactions in Vision Transformers](papers/2021/210701%20Focal%20Self-attention%20for%20Local-Global%20Interactions%20in%20Vision%20Transformers) #local_attention
1. [210713 Visual Parser](papers/2021/210713%20Visual%20Parser) #local_attention
1. [210731 CrossFormer](papers/2021/210731%20CrossFormer)
1. [210811 ConvNets vs. Transformers](papers/2021/210811%20ConvNets%20vs.%20Transformers) #robustness #transfer
1. [210819 Do Vision Transformers See Like Convolutional Neural Networks](papers/2021/210819%20Do%20Vision%20Transformers%20See%20Like%20Convolutional%20Neural%20Networks) #resnet
1. [210908 Scaled ReLU Matters for Training Vision Transformers](papers/2021/210908%20Scaled%20ReLU%20Matters%20for%20Training%20Vision%20Transformers) #cnn
1. [211118 Swin Transformer V2](papers/2021/211118%20Swin%20Transformer%20V2)
1. [211202 Improved Multiscale Vision Transformers for Classification and Detection](papers/2021/211202%20Improved%20Multiscale%20Vision%20Transformers%20for%20Classification%20and%20Detection)
1. [211210 Deep ViT Features as Dense Visual Descriptors](papers/2021/211210%20Deep%20ViT%20Features%20as%20Dense%20Visual%20Descriptors) #self_supervised #semantic_segmentation
1. [211217 A Simple Single-Scale Vision Transformer for Object Localization and Instance Segmentation](papers/2021/211217%20A%20Simple%20Single-Scale%20Vision%20Transformer%20for%20Object%20Localization%20and%20Instance%20Segmentation) #multiscale
1. [220214 How Do Vision Transformers Work](papers/2022/220214%20How%20Do%20Vision%20Transformers%20Work) #cnn
1. [220414 DeiT III](papers/2022/220414%20DeiT%20III)
## vocoder
1. [200512 FeatherWave](papers/2020/200512%20FeatherWave)
1. [201118 Universal MelGAN](papers/2020/201118%20Universal%20MelGAN)
## weak supervision
1. [201126 SelfText Beyond Polygon](papers/2020/201126%20SelfText%20Beyond%20Polygon) #ocr
## uncategorized
1. [200211 fastai](papers/2020/200211%20fastai)
1. [210603 The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models](papers/2021/210603%20The%20Case%20for%20Translation-Invariant%20Self-Attention%20in%20Transformer-Based%20Language%20Models)
1. [210606 Referring Transformer](papers/2021/210606%20Referring%20Transformer)
1. [210607 ViTAE](papers/2021/210607%20ViTAE)
1. [210614 Non Gaussian Denoising Diffusion Models](papers/2021/210614%20Non%20Gaussian%20Denoising%20Diffusion%20Models)
1. [210909 PIMNet](papers/2021/210909%20PIMNet)
1. [211026 Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers](papers/2021/211026%20Combining%20Recurrent%2C%20Convolutional%2C%20and%20Continuous-time%20Models%20with%20Linear%20State-Space%20Layers)
1. [211215 Value Retrieval with Arbitrary Queries for Form-like Documents](papers/2021/211215%20Value%20Retrieval%20with%20Arbitrary%20Queries%20for%20Form-like%20Documents)
1. [220114 DeepSpeed-MoE](papers/2022/220114%20DeepSpeed-MoE)
1. [220203 AlphaCode, Formal Math](papers/2022/220203%20AlphaCode%2C%20Formal%20Math)
1. [220204 InstructGPT](papers/2022/220204%20InstructGPT)
1. [220323 Pathways](papers/2022/220323%20Pathways)
1. [220329 Few Could Be Better Than All](papers/2022/220329%20Few%20Could%20Be%20Better%20Than%20All)
1. [220405 Text Spotting Transformers](papers/2022/220405%20Text%20Spotting%20Transformers)
1. [220416 Benchmarking Generalization via In-Context Instructions on 1,600+ Language Tasks](papers/2022/220416%20Benchmarking%20Generalization%20via%20In-Context%20Instructions%20on%201%2C600%2B%20Language%20Tasks)