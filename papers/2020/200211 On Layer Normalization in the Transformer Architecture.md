https://arxiv.org/abs/2002.04745

On Layer Normalization in the Transformer Architecture (Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, Tie-Yan Liu)

openreview에 올라왔었던 논문. 많이들 궁금해했을 transformer에서 layer norm의 위치의 효과에 대한 논문 중 하나. 여기서는 warmup의 필요성이 layer norm의 위치와 연관되어 있다는 것에 중점을 두고 있음. 사실 이 주제에 대해선 Kaiming He의 Identity Mappings in Deep Residual Networks (https://arxiv.org/abs/1603.05027)를 생각하지 않을 수 없는데...post normalization이 좋지 않은 것 같다는 의견이 종종 있었는데 transformer에서는 post layer norm이 대세가 된 것이 좀 흥미롭기도.

#normalization #transformer