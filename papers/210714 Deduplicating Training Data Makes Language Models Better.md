https://arxiv.org/abs/2107.06499

Deduplicating Training Data Makes Language Models Better (Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini)

lm 학습용 코퍼스에서 중복 텍스트들을 삭제했더니 학습된 모델이 코퍼스에서 본 텍스트를 그대로 외워 생성하는 빈도가 크게 줄었다는 결과. 여기서 쓰인 파이프라인은 가지고 있는 게 좋겠네요.

#corpus #lm