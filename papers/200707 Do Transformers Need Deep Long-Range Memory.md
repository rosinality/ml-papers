https://arxiv.org/abs/2007.03356

Do Transformers Need Deep Long-Range Memory (Jack W. Rae, Ali Razavi)

트랜스포머-XL에는 모든 레이어에 long range attention이 들어가는데 반드시 그럴 필요는 없지 않을까? 일부 레이어만 long range attention을 사용하면 속도가 빨라질 뿐 아니라 성능도 오히려 더 나아질 수 있음. #transformer #lm #attention