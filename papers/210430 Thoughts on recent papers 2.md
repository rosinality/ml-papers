https://arxiv.org/abs/2104.14294 self supervision + vit 조건에서 self attention map이 거의 semantic segmentation 수준의 특성을 보여준다는 결과.
vision transformer 논문이 너무 많이 나오는 거 아니냐는 생각을 하면서도 관심을 갖게 되는 이유. 물론 swin transformer 같은 모델은 downstream task에 대한 수치 차원에서도 인상적인 결과를 보여주지만, 그 외에도 cnn과 크게 다른 구조에 힘입어 cnn과는 다른 특성을 보여주는 것으로 보인다. 동시에 그 다른 특성들이 상당히 바람직한 것으로 보인다는 것. 마찬가지로 참고할만한 결과(https://arxiv.org/abs/2104.04191)도 있다.