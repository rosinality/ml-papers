https://arxiv.org/abs/2007.06778

An Empirical Study on Robustness to Spurious Correlations using
  Pre-trained Language Models (Lifu Tu, Garima Lalwani, Spandana Gella, He He)

nlp 프리트레이닝의 효과가 무엇인가? 데이터셋에 없는 패턴으로 외삽을 가능하게 한다기보다는 데이터셋 내의 드물게 나타나는 패턴을 더 잘 활용하기 때문이라는 제안. 약간 까다로운 주장인 것 같긴 하지만. 그리고 역시 더 큰 모델! 더 많은 데이터! 더 강력한 계산능력! 이 더 나은 특성을 보여줌. #nlp #pretraining #multitask