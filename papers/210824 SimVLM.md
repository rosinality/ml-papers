https://arxiv.org/abs/2108.10904

SimVLM: Simple Visual Language Model Pretraining with Weak Supervision (Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, Yuan Cao)

심플한 (그렇지만 쓴 데이터의 규모는 심플하지 않은) vision language model. 이미지와 텍스트 시퀀스의 앞 부분을 트랜스포머어 넣어주고 텍스트의 나머지를 autoregressive decoding하게 하는 방식으로 학습시켰군요. 쓴 데이터를 보니까 다른 모델을 다 깨버리겠다는 각오가 눈에 띄네요.

이런 형태의 generative pretraining으로는 [[210318 All NLP Tasks Are Generation Tasks]]이 생각나는군요. 물론 이쪽이 더 심플하긴 합니다.

#vision-language #generative_model 

