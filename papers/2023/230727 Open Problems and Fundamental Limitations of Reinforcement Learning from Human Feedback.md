https://arxiv.org/abs/2307.15217

Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback (Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, Dylan Hadfield-Menell)

RLHF의 한계에 대해서 논하고 있는데...fundamental하다고 주장하고 있는 것은 주로 human preference라는 굉장히 복잡하고 다양한 문제 자체의 난점, 데이터셋을 구축하는 것의 어려움, 그리고 그 데이터셋을 reward model로 녹여내는 것의 문제가 중심이군요.

#alignment 