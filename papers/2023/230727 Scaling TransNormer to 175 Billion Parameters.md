https://arxiv.org/abs/2307.14995

Scaling TransNormer to 175 Billion Parameters (Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Fei Yuan, Xiao Luo, Yu Qiao, Yiran Zhong)

linear attention으로 multi billion 모델을 학습을 시켰다고?! 인 줄 알고 봤는데 딱히 그렇진 않고 그 규모로 scaling 할 수 있는지를 살펴봤다 정도네요. linear attention은 약간 한 물 간 접근이라는 느낌이었는데 retentive networks가 딱 linear attention처럼 생겨서 positional embedding과 잘 결합하면 이게 혹시 가능한 건가 하는 생각도 들긴 하네요.

#transformer #efficient_attention 