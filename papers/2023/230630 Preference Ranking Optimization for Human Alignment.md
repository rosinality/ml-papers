https://arxiv.org/abs/2306.17492

Preference Ranking Optimization for Human Alignment (Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, Houfeng Wang)

accepted vs rejected pair만 고려하는 것이 아니라 여러 prompt-response 샘플 사이의 preference ranking을 고려하는 ranking loss를 만들고 그걸로 policy 모델을 직접 학습시키는 방법이네요. 어쩐지 RRHF의 확장이라는 느낌이군요.

흥미로운데 평가 결과가 아주 확연하지는 않은 것 같네요.

#alignment 