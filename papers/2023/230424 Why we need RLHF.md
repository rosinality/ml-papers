https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81

왜 SFT만으로는 충분하지 않고 RL이 필요한가에 대한 흥미로운 설명이라서 공유합니다. Yoav Goldberg가 RLHF가 왜 필요한 것이냐라는 질문을 트위터에 종종 했었는데...Schulman의 강의에서 힌트를 얻어 내린 결론이네요. 저도 아직 강의 자체를 보지는 않았는데 한 번 꼭 봐야하지 않을까 싶습니다. (https://www.youtube.com/watch?v=hhiLw5Q_UFg)

전 일단 SFT로는 positive한 사례를 학습하는 것이기 때문에 negative한 시퀀스에 대한 피드백, 즉 나오지 않아야 할 시퀀스(반복 등)를 억제하는데 RL이 효과적이어서가 아닐까 정도로 생각했었습니다. 이 설명도 위 글에 한 가지 가능한 설명으로 나오기는 합니다. 그렇지만 위 글의 설명에서 중점적으로 보는 것은, LLM에 지식에 대한 쿼리를 요청하는 상황에서 발생하는, 보통 hallucination이라고 부르는 문제에 대해 RL이 도움을 줄 수 있다는 이야기를 합니다.

LLM은 어떤 질문에 대한 답에 대한 정보를 갖고 있을 수도 있고 아닐 수도 있습니다. SFT를 한 경우, 질문과 응답이 LLM에 이미 들어가 있는 정보에 대한 것이라면 그 정보를 활용해서 응답하도록 학습이 될 겁니다. 하지만 모델이 정보를 갖고 있지 않은 질문이라면? 어쨌든 SFT로 생성 학습이 되기 때문에 모델이 갖고 있는 지식과는 별개로 일단 말을 만들어내도록(= hallucination) 학습이 될 겁니다. 그런데 우리가 LLM에 어떤 정보가 들어있는지 아닌지를 알고 있진 않죠. 그러니 SFT로는 이 문제를 해결하기 어렵습니다.

그렇지만 RL을 사용한다면, 적절한 Reward를 제공할 수 있다면 아무렇게나 말을 해버린 경우에 대해 처벌을 할 수 있고, 따라서 장기적으로 모델이 자기가 갖고 있는 정보에 대해서는 그 정보를 활용해 대답하고 아닌 경우에는 I don't know라고 말하는 행동을 강화시킬 수 있습니다. 이것이 RLHF를 위시한 alignment가 hallucination을 억제하는 메커니즘이 될 수 있습니다.

추가적으로, 이건 ChatGPT나 GPT-4의 출력 결과를 사용해 alignment를 하려는 시도, 보통 distillation이라고 부르는 시도의 한계를 알려주기도 합니다. ChatGPT나 GPT-4가 갖고 있는 지식과 그것을 복제하려는 시도에서 사용하는 모델이 갖고 있는 지식은 다르죠. 그런 상황에서 ChatGPT를 복제하려고 한다면, ChatGPT가 갖고 있는 지식을 모델이 갖고 있지 못한 경우에는 hallucination을 촉진할 것이고 ChatGPT가 갖고 있지 않지만 모델은 지식을 갖고 있어 사실 대답을 할 수 있는 상황에서도 대답하지 않으려 하는 경향을 학습할 수 있습니다.

#alignment #rl #llm