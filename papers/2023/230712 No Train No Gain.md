https://arxiv.org/abs/2307.06440

No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models (Jean Kaddour, Oscar Key, Piotr Nawrot, Pasquale Minervini, Matt J. Kusner)

트랜스포머의 효율적인 학습을 위한 방법들을 비교. 최종적으로 결국 베이스라인과 비슷비슷하다는 건 어쩔 수 없고...그나마 학습 가속을 보이는 것은 layer stacking, 레이어 수가 작은 모델을 더 많은 모델로 복붙하는 접근이네요. 학습 속도를 한 두 배 정도 빠르게 만들 수 있으면 엄청날 것 같은데...과연 그런 방법이 나올 수 있을지.

#efficient_training 