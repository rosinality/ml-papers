https://arxiv.org/abs/2305.10425

SLiC-HF: Sequence Likelihood Calibration with Human Feedback (Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, Peter J. Liu)

이쪽은 RLHF의 대안적인 방법. 시퀀스 우도를 캘리브레이션하는 방법. RLHF를 위해서는 모델을 여러 개 올려야 하고 샘플링까지 병행해야 한다는 것이 부담스러워서 이런 방법이 매력적이긴 하다. 거기에 다른 모델로 수집한 데이터로 학습된 Reward 모델을 쓸 수 있다는 것도 장점으로 꼽고 있다.

다만 문제는 현재 RLHF의 의미나 효과에 대해 아는 것이 많지 않다는 것. 일단 RLHF와 대안적 방법을 비교해서 더 나은 방법을 채택하자면 양자에 대한 경험이 모두 있어야 할 것이고, 어느 것이 더 나은지를 평가할 수 있는 기준 또한 있어야 한다. 모두 만만한 일은 아니다. 한 쪽을 생략하고 간다면 편리하겠지만 일단 RLHF로 성공한 선례가 있기 때문에 그러기에는 영 걸리는 지점이 있고. 그 선례(OpenAI, Anthropic 등)들은 RLHF에서 다음 단계로 넘어갈 수도 있겠지만 그것도 이미 기존 방법에 대한 경험이 있기에 가능한 것일 것이다.

여하간 이쪽에서도 RLHF 튜닝은 부담스러웠는지 RLHF와의 비교는 논문에 공개된 샘플 텍스트를 끌어와서 했다.

#alignment 