https://arxiv.org/abs/2303.06865

High-throughput Generative Inference of Large Language Models with a Single GPU (Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gonzalez, Percy Liang, Christopher Ré, Ion Stoica, Ce Zhang)

flexgen 논문이 나왔군요. off loading으로 적은 gpu 메모리에 대해서도 llm inference를 하기 위한 방법입니다. 논문에서도 밝히고 있는 것처럼 일정 레이턴시만 달성할 수 있으면 스루풋을 극대화하는 것이 일반적인 시나리오일 것 같긴 합니다.

#llm 