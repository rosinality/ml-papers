https://arxiv.org/abs/2305.18565

PaLI-X: On Scaling up a Multilingual Vision and Language Model (Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic, Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut)

구글의 새 vision-language 모델이군요. vit-22B에 ul2 32B 모델의 결합입니다. 비전과 텍스트 모델의 규모를 비슷하게 가져가는 것이 좋은 것 같다고 말하고 있네요.

1단계 프리트레이닝에서는 224px로 다양한 과제에 대해서 학습시키고, 2단계 프리트레이닝은 이미지 입력 크기를 448, 672, 756px로 늘려가면서 OCR과 object detection에 대해 학습시킵니다.

여러모로 비전 모델도 큰 것을 사용하고 높은 해상도에서 학습시키는 것이 좋다는 결론이군요.

#vision-language #multimodal 