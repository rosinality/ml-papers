https://arxiv.org/abs/2304.05302

RRHF: Rank Responses to Align Language Models with Human Feedback without tears (Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, Fei Huang)

PPO 대신 reward로 ranking을 만들어 ranking loss를 적용하는 방법이군요. 세팅이 심플해지고 성능은 PPO를 쓰는 쪽과 비슷하다는 것이 요점입니다.

#alignment #llm 