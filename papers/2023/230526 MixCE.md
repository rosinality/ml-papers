https://arxiv.org/abs/2305.16958

MixCE: Training Autoregressive Language Models by Mixing Forward and Reverse Cross-Entropies (Shiyue Zhang, Shijie Wu, Ozan Irsoy, Steven Lu, Mohit Bansal, Mark Dredze, David Rosenberg)

forward KL의 mode coverage behavior가 생성 퀄리티에 해로울 수 있으니 reverse kl의 mode dropping behavior를 결합할 수 있다면 좀 더 낫지 않을까 하는 아이디어. 딱 gan이 유행하던 시기에 많이 하던 방식의 분석이긴 하네요. 여러모로 모델이 충분히 강력해지면 이 문제는 해소되는 경향이 크지 않은가 싶긴 한데요.

#lm 