https://arxiv.org/abs/2308.03303

LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning (Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, Bo Li)

lora의 AB 행렬에서 dimension reduction을 수행하는 A를 얼려버리고 B만 학습시키면 어떨까. 학습 파라미터도 줄어들지만 low dimension인 XA activation만 저장하면 되니 activation memory를 크게 줄일 수 있다는 아이디어네요. 꽤 흥미롭습니다.

#efficient_training 