https://arxiv.org/abs/2305.16960

Training Socially Aligned Language Models in Simulated Human Society (Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M. Dai, Diyi Yang, Soroush Vosoughi)

굉장히 흥미로운 alignment 방법이네요. 여러 llm agent들을 상호작용할 수 있게 해서 작은 사회를 만들고, llm이 생성한 응답을 다른 llm agent가 feedback하게 해서 그 결과로 align을 한다는 접근이군요. agent base model의 영향을 많이 받은 듯한 접근인데...모델을 지능적인 agent처럼 다루는 것이 통하는 시점이 왔네요.

다만 이런 방식의 alignment 절차에서 social network를 고려한다는 것이 어떤 의미일까 하는 생각은 들긴 하네요.

#alignment