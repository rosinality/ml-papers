https://arxiv.org/abs/2306.13649

GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models (Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, Olivier Bachem)

lm에 대한 distillation이 별로 성공적이지는 않았는데...최근 distillation 시도가 많이 나오는 군요. 단 perplexity를 맞추기보다는 샘플 퀄리티를 맞추는데 집중한다는 느낌입니다. reverse kl을 쓰고 student가 생성한 샘플을 사용해서 distillation을 하는 군요. reverse kl을 쓴다는 건 얼마 전 나온 https://arxiv.org/abs/2306.08543 이 연구와 통하네요.

샘플링 퀄리티를 맞춘다는 시도는 충분히 자연스럽게 나올만 하지만 고려하는 샘플의 도메인을 어디까지 커버할 수 있는가의 측면에서 불안하긴 하네요.

#distillation #llm 