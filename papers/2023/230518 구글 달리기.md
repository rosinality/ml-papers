구글이 달리기 시작했네.

[https://arxiv.org/abs/2305.10429](https://l.facebook.com/l.php?u=https%3A%2F%2Farxiv.org%2Fabs%2F2305.10429%3Ffbclid%3DIwAR35M7u-3D2uBXfC-RHEHQYuw7NoVuFLth2hN2e62Ti58qwBP8OHuIgl768&h=AT0loVvzbZO56lwmaRxoGDJiBKlWyJ1c_viq0oXQ55VS6LdLNpCEPwoz3GuUt8VsQneYvOHIJLftTA-QP-KrBeUm1RYen-Un20es_fFbAa5ACfgtD77dS8tDhv7ZFmoSNMlu&__tn__=-UK-R&c[0]=AT2KQc_vIfgFfegyLnY5AyIMnbeeWtEEY6V7GqBz4QqNdPwB6Qxcrf85MOL21Aqr-h5hTX5yU3IJdMQs5Z838QXA1FS6IMPR7tDdaxxp6Z_hkNe3E8fj9ZcTVdHekZ_jk8Uiv8Pp94EN9EydJ96KgPIER5SIs8L1AGDD4MCBpz9JZsGh7jBs)

LLM의 프리트레이닝 코퍼스의 도메인 비율을 결정하는 방법. 작은 모델을 균등 비율 샘플링으로 학습시킨 다음, 이 모델을 기준으로 특정 도메인의 loss가 과도하게 커지지 않도록 도메인 비율을 최적화하고, 이 최적화된 비율을 사용해 본격적으로 모델을 학습. 더 빠르게 더 고성능의 모델을 학습. Multilingual 상황도 도메인이라고 보고 다룰 수 있지 않을지.

[https://arxiv.org/abs/2305.10425](https://arxiv.org/abs/2305.10425?fbclid=IwAR3VlSvrgo9HA1o4Ro11N77wKW4GtYFNbUDKQEDitjyJkjA2tltNWin82RU)

이쪽은 RLHF의 대안적인 방법. 시퀀스 우도를 캘리브레이션하는 방법. RLHF를 위해서는 모델을 여러 개 올려야 하고 샘플링까지 병행해야 한다는 것이 부담스러워서 이런 방법이 매력적이긴 하다. 거기에 다른 모델로 수집한 데이터로 학습된 Reward 모델을 쓸 수 있다는 것도 장점으로 꼽고 있다.

다만 문제는 현재 RLHF의 의미나 효과에 대해 아는 것이 많지 않다는 것. 일단 RLHF와 대안적 방법을 비교해서 더 나은 방법을 채택하자면 양자에 대한 경험이 모두 있어야 할 것이고, 어느 것이 더 나은지를 평가할 수 있는 기준 또한 있어야 한다. 모두 만만한 일은 아니다. 한 쪽을 생략하고 간다면 편리하겠지만 일단 RLHF로 성공한 선례가 있기 때문에 그러기에는 영 걸리는 지점이 있고. 그 선례(OpenAI, Anthropic 등)들은 RLHF에서 다음 단계로 넘어갈 수도 있겠지만 그것도 이미 기존 방법에 대한 경험이 있기에 가능한 것일 것이다.

여하간 이쪽에서도 RLHF 튜닝은 부담스러웠는지 RLHF와의 비교는 논문에 공개된 샘플 텍스트를 끌어와서 했다.