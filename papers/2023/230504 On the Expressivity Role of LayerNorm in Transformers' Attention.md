https://arxiv.org/abs/2305.02582

On the Expressivity Role of LayerNorm in Transformers' Attention (Shaked Brody, Uri Alon, Eran Yahav)

layer norm이 attention에 미치는 영향. 이 논문에서는 layer norm의 효과를 1. 모든 키를 동일한 하이퍼플레인으로 projection. 2. 모든 키를 동일한 norm으로 scaling으로 보는군요. 1번의 효과는 모든 키들을 동등하게 선택할 수 있는 가능성을 부여하는 것이고 2번의 효과는 특정 키가 선택되지 않거나 못하는 문제를 해소하는 것으로 보는군요.

#transformer #attention #normalization 