https://arxiv.org/abs/2305.13245

GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints (Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, Sumit Sanghai)

multi head attention을 multi query attention으로 바꾸기. 체크포인트를 가져다 추가 튜닝을 해주면 괜찮게 된다고 합니다. multi query attention이 성능이 좀 떨어지니 n개 head를 사용하는 group query attention도 제안했군요.

T5에 대한 결과이긴 합니다. TPU에서는 multi query attention의 효과가 굉장히 큰 것으로 나타나는데 GPU에서는 어떨지 궁금하네요.

#transformer