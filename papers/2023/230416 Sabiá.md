https://arxiv.org/abs/2304.07880

Sabiá: Portuguese Large Language Models (Ramon Pires, Hugo Abonizio, Thales Rogério, Rodrigo Nogueira)

어쩌다 포르투갈어 LM 결과도 들여다보는 날이 왔네요. LLaMA weight를 가지고 10B 정도 되는 포르투갈어 토큰에 대해 학습. 공개 모델을 좀 더 프리트레이닝하는 것도 가능한 접근이 아닐까 싶었는데 증거가 하나 추가됐군요. 데이터 오염이 잘 체크되지는 않은 것 같은데 그걸 감안할 필요는 있겠습니다.

흥미로운 부분은 본래 포르투갈어로 제작된 데이터셋과 영어에서 포르투갈어로 번역한 데이터셋에서 나타나는 차이로 보이네요. 아예 다른 데이터셋들이기 때문에 수치를 비교하기 좀 그렇긴 하지만 포르투갈 네이티브 데이트셋들이 번역 데이터셋들에 비해 포르투갈어 튜닝에 대한 성능 향상폭이 더 큽니다. 논문에서의 추정은 포르투갈어 튜닝이 포르투갈어 문제를 푸는데 필요한 정보, 그러니까 문화나 국가에 대한 정보를 제공한다는 것이네요. 번역 데이터셋들은 영어권 문화나 국가적 배경 정보가 필요한 것이고, 그건 LLaMA에 이미 잘 포함되어 있는 것이라는 거죠. 반대로 말하면 영어로 습득한 정보를 포르투갈어로 구사하는 것에는 큰 문제가 없는 것이라고 할 수도 있겠네요.

물론 LLaMA가 기본적으로 포르투갈어를 꽤 하고, 두 언어의 유사성도 고려해야겠지만요.

더해서 LLaMA 65B까지 올리니 퍼포먼스가 GPT-3.5-turbo와 비슷해지는군요. 이것도 흥미로운 부분인 것 같습니다.

#llm #multilingual 