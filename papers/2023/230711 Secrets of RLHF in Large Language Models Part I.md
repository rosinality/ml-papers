https://arxiv.org/abs/2307.04964

Secrets of RLHF in Large Language Models Part I: PPO (Rui Zheng, Shihan Dou, Songyang Gao, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Limao Xiong, Lu Chen, Zhiheng Xi, Yuhao Zhou, Nuo Xu, Wenbin Lai, Minghao Zhu, Rongxiang Weng, Wensen Cheng, Cheng Chang, Zhangyue Yin, Yuan Hua, Haoran Huang, Tianxiang Sun, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang)

바이트댄스에서 RLHF 상황에서 PPO에 대해 정리하고 몇 가지 가능한 옵션들을 테스트해봤군요. 일반적인 세팅과 크게 다르지는 않는데 reward normalization이 주요한 차이일 것 같네요.

코드 공개도 했으니 코드 체크를 다시 한 번 해봐야겠습니다.

그나저나 Part I이라서 Part II에는 그러면 무슨 이야기를 하려고 하는 건가...싶었는데 좋은 reward model이 없는 상황에 대한 것이라고 하네요. 어쩐지 이쪽이 더 궁금하군요.

#alignment #reinforcement_learning 