https://arxiv.org/abs/2306.07052

Gradient Ascent Post-training Enhances Language Model Generalization (Dongkeun Yoon, Joel Jang, Sungdong Kim, Minjoon Seo)

pretraining objective에 대한 gradient ascent로 lm을 post training 하면 downstream task에 대한 성능이 향상된다는 결과. gradient ascent를 sharpness aware minimization과 연관해서 설명하고 있는데 (sam에서 loss를 상승시키는 eps를 찾는 부분을 가리키는 것 같긴 하네요.) 흥미롭군요.

#lm 