https://arxiv.org/abs/2304.09151

UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining (Hyung Won Chung, Noah Constant, Xavier Garcia, Adam Roberts, Yi Tay, Sharan Narang, Orhan Firat)

multilingual lm의 학습시 샘플링 방식과 관련된 방법이네요. oversampling을 하자니 low resource language는 지나치게 많은 에폭을 학습하게 된다는 문제에 대한 대응입니다. 문자 수 기준으로 학습 budget을 설정해서 모든 언어가 최대한 균등한 학습 문자 수 budget을 나눠가지도록 하는 동시에 일정 에폭 이상 반복되지는 않도록 하는 방식이네요.

그와 동시에 mC4 데이터셋을 업데이트했군요. 변화는 새로운 크롤링 데이터 반영, 언어 필터링 강화, bad word에 대한 필터링 약화군요. 데이터 양은 35% 증가했다고 합니다. 데이터 양은 35% 증가했다는데 한국어 양은 큰 변동이 없는 것 같기도 하고...그렇네요.

데이터 구축과 관련해서 첨언하자면 언어 필터로 자연어가 아닌 문서를 더 철저하게 걸러내는 것을 기대했고, bad word가 포함된 문서를 학습에 사용해 bad word에 대해 모델이 학습하도록 기대했군요. 그러니까 무해한 언어만 학습한 모델보다는 유해한 언어도 학습한 모델이 유해한 언어에 대한 대응을 하기에 더 낫다...는 느낌이네요. 또한 bad word filtering을 사용하는 것이 특정 언어에 대해 bias를 야기할 수 있다는 부분도 지적했는데 gopher에서 우려했던 것과 통하는 것이 아닐까 싶습니다.

#multilingual #llm 