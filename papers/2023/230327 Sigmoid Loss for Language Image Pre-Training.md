https://arxiv.org/abs/2303.15343

Sigmoid Loss for Language Image Pre-Training (Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer)

clip contrastive loss의 softmax를 sigmoid로 전환. softmax의 normalization factor가 사라지면서 data parallel 과정을 더 효율적으로 만들 수 있다는 것이 포인트.

#clip #contrastive_learning 