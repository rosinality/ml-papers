https://arxiv.org/abs/2305.10429

DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining (Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V. Le, Tengyu Ma, Adams Wei Yu)

LLM의 프리트레이닝 코퍼스의 도메인 비율을 결정하는 방법. 작은 모델을 균등 비율 샘플링으로 학습시킨 다음, 이 모델을 기준으로 특정 도메인의 loss가 과도하게 커지지 않도록 도메인 비율을 최적화하고, 이 최적화된 비율을 사용해 본격적으로 모델을 학습. 더 빠르게 더 고성능의 모델을 학습. Multilingual 상황도 도메인이라고 보고 다룰 수 있지 않을지.

#dataset #multitask #llm #pretraining 