https://arxiv.org/abs/2306.00978

AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration (Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Song Han)

weight only quantization에서 activation scale을 고려해 per channel scaling factor를 찾는 방법. tensor core를 사용하는 고속 커널까지 만들었군요. GPTQ와 유사하거나 더 나은 성능을 보이면서 더 고속으로 작동할 여지가 있어 보입니다. 중요한 결과로 보이네요.

#quantization 