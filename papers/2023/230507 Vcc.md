https://arxiv.org/abs/2305.04241

Vcc: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens (Zhanpeng Zeng, Cole Hawkins, Mingyi Hong, Aston Zhang, Nikolaos Pappas, Vikas Singh, Shuai Zheng)

long context length transformer. 논문을 특이하게 썼네요. 중요한 토큰들을 선정해서 사용한다는 것은 retrieval 기반 방법들과 비슷한데...이쪽에서는 토큰들을 압축해서 중요한 토큰을 만들어내는 방식이군요. 꽤 복잡하긴 합니다.

약간 다른 결이지만 사람은 그리 크지 않은 작업 기억과 장기 기억으로 책 읽고 이해하기 같은 과제를 어떻게 수행하는 것일까요? 습득한 정보를 장기 기억에 입력하고, 인출해서 새로운 정보와 결합하고, 이 결합된 정보를 다시 장기 기억에 입력하는 루프가 돌아가는 것이 아닐까 하는 생각을 해봅니다. 정교하게 구현하려면 까다롭겠지만 LLM을 이 루프의 부품으로 써서 비슷한 기능을 할 수 있게 하는 것은 꽤 가능하지 않을까 싶네요. 이 논문에서 중요 토큰들을 업데이트 하는 과정이 이런 추측을 연상하게 하는 지점이 있어서 써봤습니다.

#efficient_attention #transformer 