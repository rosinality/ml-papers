https://arxiv.org/abs/2307.03025

Style Over Substance: Evaluation Biases for Large Language Models (Minghao Wu, Alham Fikri Aji)

llm 평가에 슬슬 크라우드워커를 쓰는 경우가 생기고 있는데, 사람에게 평가를 시키면 사실 관계 같은 것보다는 스타일에 영향을 더 받아서 GPT-4만 못할 수 있다는 결과. 사실 관계 문제 이상으로 elo rating의 차이를 보면 크라우드워커의 평가가 썩 좋지 않았던 것으로 보이기도 합니다.

이건 평가의 문제이긴 하지만, OpenAI나 Anthropic이 괜히 데이터 구축에 엄청난 공을 들인 것이 아니겠죠.

사실 전반적으로 ML 논문에 사람의 평가 결과를 싣는 경우가 꽤 있는데 그 과정에 문제가 많다는 이야기는 계속 있었죠. 

#llm #evaluation 