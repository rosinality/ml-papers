https://arxiv.org/abs/2003.02436

Talking-Heads Attention (Noam Shazeer, Zhenzhong Lan, Youlong Cheng, Nan Ding, Le Hou)

multi head attention의 attention logit, weight에서 head들 사이에 linear map을 걸어본 시도. 뭔가 트랜스포머로 이것 저것 많이 해보고 있는 중인 듯...

#transformer #attention