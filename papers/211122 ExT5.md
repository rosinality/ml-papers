https://arxiv.org/abs/2111.10952

ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning (Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni, Jai Gupta, Kai Hui, Sebastian Ruder, Donald Metzler)

구글 사람들이 large scale multitask training에 맛을 들이더니 그만...107개의 supervised task를 모은 다음 t5 task와 섞어서 pretraining을 하는 것으로 벤치마크를 깨고 다녔네요.

다양한 task 중에 도움이 되는 것이 있고 안 되는 것이 있을 수 있으니 도움이 될 것 같은 것만 찾아서 써보기도 했는데 일단은 그냥 다 쓰는 것도 괜찮은 것 같다는 결론입니다.

#pretraining #multitask 