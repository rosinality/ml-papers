https://arxiv.org/abs/2205.01068

OPT: Open Pre-trained Transformer Language Models (Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer)

meta는 llm으로 뭔가 안 하나 싶었는데 이걸 하고 있었네요. 효율적인 학습을 위한 코드와 레시피, 로그를 공개했습니다. 작은 모델들은 weight를 공개했고 175B 모델은 요청하면 제공하는 모양입니다. (API인지 weight인지 모르겠네요.)

#lm