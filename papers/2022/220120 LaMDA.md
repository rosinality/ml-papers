https://arxiv.org/abs/2201.08239

LaMDA: Language Models for Dialog Applications (Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, Quoc Le)

구글은 매년 하나씩 왕창 큰 대화 모델을 내놓네요. 모델 생성 결과의 흥미로움, 안전성, 근거 여부 등을 평가하기 위한 지표와 데이터셋을 만들고 대화 생성, 대화문 평가, information retrieval에 대해 파인튜닝을 한 것이 메인인 것 같네요. 나머지는 1.5조 개 단어에 대해 1024개 TPUv3로 58일 동안 프리트레이닝한 137B 트랜스포머가 해줍니다.

#dialog 