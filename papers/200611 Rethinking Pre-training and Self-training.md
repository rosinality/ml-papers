https://arxiv.org/abs/2006.06882

Rethinking Pre-training and Self-training (Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin D. Cubuk, Quoc V. Le)

디텍션 학습에 데이터와 augmentation을 증가시키면 이미지넷 프리트레이닝이 도움이 되지 않음. 그러나 이미지넷 데이터를 noisy student와 같은 semi supervision에 사용하면 이 경우에도 성능 향상이 발생함. VirTex (https://arxiv.org/abs/2006.06666)도 그렇고 프리트레이닝 과정의 supervision에 대해 생각해보게 됨. NLP에서는 어떨지?