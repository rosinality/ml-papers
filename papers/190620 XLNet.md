https://arxiv.org/abs/1906.08237 새로운 프리트레이닝 알고리즘/모델. 성능이 예쁘게 나왔다. 핵심 아이디어는 Transformer-XL + BERT식의 masked language model에서 masking된 부분에 대한 정보를 활용하지 못하는 문제에 대한 해결이라고 볼 수 있겠다.
논문의 예제를 가져오면 예를 들어 New York is a city라는 문장이 있고 New York를 마스킹해서 예측 타겟으로 설정하면 BERT에서는 is a city를 사용해서 New York를 예측해야 한다. 그런데 사실 이건 좀 강한 제약이다.
New를 예측할 때 New 토큰만 차단하면 되고 York를 예측할 때는 York 토큰에 대한 정보만 차단하면 된다. 이상적으로는 York is a city를 사용해서 New를 예측하고 New is a city를 사용해서 York를 예측하면 된다.… 더 보기

#pretraining #language_model 