Rethinking Pre-training and Self-training (https://arxiv.org/abs/2006.06882)
Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition (https://arxiv.org/abs/2010.10504)
Self-training and Pre-training are Complementary for Speech Recognition (https://arxiv.org/abs/2010.11430)
On the Marginal Benefit of Active Learning: Does Self-Supervision Eat Its Cake? (https://arxiv.org/abs/2011.08121)
Neural Semi-supervised Learning for Text Classification Under Large-Scale Pretraining (https://arxiv.org/abs/2011.08626)
요즘 unsupervised pretraining + semi-supervised learning이 아주 날아오르는 중. (물론 Rethinking Pre-training and Self-training은 몇 달 되긴 했으나.) 레이블 효율성을 높여주는 것이지 데이터 효율성을 높여주는 것은 아니라는 점이 약간 마음에 걸리기는 하는데, 더 나은 prior를 찾는 것이 어렵다면 결국 데이터 밖에 답이 없을 것이긴 하다. 그리고 더 나은 prior를 찾는 문제는...수많은 연구들이 쏟아지고 있는 지금 시점에서도 성공적인 사례가 그렇게 많은 것 같지 않다.