https://arxiv.org/abs/1706.08224
생일 역설이라는 것은 사람이 367명이 있으면 반드시 생일이 같은 사람들이 존재한다는 것이다. 비둘기집 원리에 따라(그러니까 비둘기가 들어갈 집보다 비둘기의 수가 많으면 두 마리 비둘기가 들어간 비둘기집이 반드시 존재한다는 것) 모두에게 다른 생일을 부여해도 남는 사람이 있으니 중복되는 생일이 있을 수밖에 없다. (아, 물론 연도는 빼고.)
좀 더 재미있는 것은 생일이 같은 사람이 있을 확률이 50%가 되려면 사람이 몇 명이나 있어야 할지 추산해볼 수 있다는 것이다. 실제로는 23명이라고 하고, sqrt(N) (N은 가능한 경우의 수, support)로 근사할 수 있다고 한다. sqrt(366) ≈ 19니까 대충 비슷하다고 할 수 있겠다.
이걸 반대로 생각해보면 N개 샘플을 뽑았을 때 중복된 샘플이 50% 정도가 나온다면 가능한 경우의 수는 N^2이라고 할 수 있지 않은가? 라는 것이 이 논문의 접근이다. 그러니까 GAN에서 샘플을 뽑아보고 샘플 몇 개 정도에서 중복이 나오는지를 실험해봤다.
CelebA에 대해서 DCGAN과 ALI를 학습시켜본 결과 DCGAN은 N이 400이 나왔고 ALI에서는 1000이 나왔다고 한다. 그렇다면 DCGAN의 support는 160,000 정도가 되고 ALI는 1,000,000 정도가 된다고 할 수 있을 것이다. CelebA가 200K 정도가 되니까 그럭저럭 학습을 한 것이라고 할 수 있겠지만 분포를 학습했다고 하기에는 어렵지 않은가 하는 것이 요지다.
물론 이런 가정들은 discrete distribution에 대한 것이라 continuous distribution을 생각하면 문제가 꼬일 것이다. 그렇지만 GAN이 mode collapse가 강하게 나타난다는 것은 다들 알고 있으니까...
사실 generative model이 확률 분포를 학습한다고 표현하긴 하지만 그게 정말로 적절한 것인가 하는 생각은 좀 하게 된다. 확률 분포를 학습한다고 하면 확률 분포 사이의 차이를 측정하는 metric을 이용해 여러가지를 유도할 수 있고 여러 직관들을 도입할 수 있으니 상당히 유용하고, 또 실제로 많은 성과가 나오기도 했지만 여전히 확률 분포를 학습한다는 표현 뒤로 여러 가지 복잡한 문제와 특히 GAN과 같은 경우에는 GAN 특유의 학습 과정에서 작용하는 메커니즘 등등이 숨겨져 있는 것이 아닌가 하는 생각을 한다.
무엇보다 애초에 확률 분포를 학습한다는 것 자체가 몹시 어려운 일이 아닌가? 거기다 그 학습하는 대상이 그 정확한 특성에 대해 거의 알려진 것이 없는, support가 measure zero인 아주 자연스러우면서도 기괴한 데이터인데.
사물을 구분하는 것은 아주 쉽다. 그리고 사물의 세부적인 특징에 대한, 사진에 맞먹는 정보가 인간의 뇌에 저장되어 있다는 것도 분명해 보인다. 그렇지만 그런 세세한 정보를 다룰 수 있다고 하더라도 실제 분포에 대한 지극히 일부의 샘플로 실제 분포를 추정한다는 것은 아주 어려운 일일 것이라고 본다. 사람은 수많은 모습의 얼굴을 기억하고 있고 또 구분할 수 있지만 그를 기반으로 과연 새로운 모습의 얼굴들의 이미지를 얼마나 떠올릴 수 있고 또 그런 모습의 얼굴이 얼마나 흔한지를 충분히 잘 말할 수 있는가? 그게 그렇게 쉬운 일이었다면 그림을 그리기
 위해서 모델이 필요할 이유가 없었을 것이다.
 
 #gan 