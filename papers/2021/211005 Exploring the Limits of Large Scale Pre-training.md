https://arxiv.org/abs/2110.02095

Exploring the Limits of Large Scale Pre-training (Samira Abnar, Mostafa Dehghani, Behnam Neyshabur, Hanie Sedghi)

image classification 모델을 5000개 학습시켜서 pretraining/upstream 과제의 성능과 downstream 과제의 성능의 관계를 분석했네요. 핵심 결과는 upstream 과제의 성능 향상에 대해 downstream 과제의 성능은 bayes error rate보다 유의하게 낮은 지점에서 saturate 된다는 것입니다. pretraining 데이터와 downstream 데이터, 그리고 모델 세팅의 조합에 따라 성능이 달라진다는 자연스러운 결론이네요.

#pretraining #classificiation #scaling