https://arxiv.org/abs/2110.08207

Multitask Prompted Training Enables Zero-Shot Task Generalization (Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, Alexander M. Rush)

다양한 nlp 과제를 프롬프트 템플릿 형태로 변형시킨 다음 학습시켜서 학습에 쓰지 않은 과제에 대해서도 제로샷으로 대응할 수 있다는 아이디어. [[210903 Finetuned Language Models Are Zero-Shot Learners]]하고 많이 비슷한 아이디어라서 차이를 따로 기술했군요.

하여간 다들 llm으로 zero/few-shot 하는 걸 엄청나게 하고 있네요. llm도 없는 우린 뭘 할 수 있죠?

#lm #zero-shot