https://arxiv.org/abs/2107.11906

H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences (Zhenhai Zhu, Radu Soricut)

hierarchical low rank attention. 구조적으로는 local attention에 off diagonal을 low rank attention으로 채워넣은 느낌이네요. low rank attention은 avg pooling을 거친 시퀀스를 사용해서 계산한다는 느낌입니다. 실험의 다양성이 좀 아쉽긴 하네요.

#efficient_attention 