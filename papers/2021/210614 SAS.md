https://arxiv.org/abs/2106.07176

SAS: Self-Augmented Strategy for Language Model Pre-training (Yifei Xu, Jingqiao Zhang, Ru He, Liangzhu Ge, Chao Yang, Cheng Yang, Ying Nian Wu)

electra에서처럼 generator를 따로 두는 대신 인코더의 출력을 사용하는 접근이네요. 괜찮아 보입니다.

#pretraining 