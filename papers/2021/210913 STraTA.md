https://arxiv.org/abs/2109.06270

STraTA: Self-Training with Task Augmentation for Better Few-shot Learning (Tu Vu, Minh-Thang Luong, Quoc V. Le, Grady Simon, Mohit Iyyer)

pretrained lm으로 mnli 데이터셋에 대해 generative한 모델을 만들고, 이 모델을 사용해서 개별 과제들에 대해서 nli 트레이닝 데이터를 만들고(즉 in domain 텍스트에 대한 nli 데이터 생성), nli에 학습시킨 모델을 만든 다음 pseudo label을 만들어서 self training을 한다, 이런 few shot training 절차네요.

결과적으로는 nli라는 개별 과제에 학습시키기 전에 사용되는 중간 단계의 과제가 성능을 끌어올릴 수 있다는 것을 다시 한 번 확인한 것이 될 것 같은데, 이런 과제를 설계하는 것도 그 자체로 흥미로운 일이 아닌가 싶긴 하네요. 물론 요즘 같은 대 unsupervised pretraining 시대에는 좀 안 맞는 것 같긴 합니다만.

어쨌든 sst-2는 이제 샘플 16개만 있으면 풀리는 군요.

#pretraining #finetuning #semi_supervised_learning #few_shot