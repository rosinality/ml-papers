https://arxiv.org/abs/2105.11115

Self-Attention Networks Can Process Bounded Hierarchical Languages (Shunyu Yao, Binghui Peng, Christos Papadimitriou, Karthik Narasimhan)

self attention이 형식 언어 dyck을 모델링할 수 없다는 결과가 있었는데...dyck에서 recursion depth를 제한한 경우에는 모델링이 가능하고 이 케이스에서 lstm에 비해 강점을 보이는 것을 검증했네요. 그러니까 self attention은 자연어에서도 표현력은 대체로 충분할 것(이라고 기대합니다).

#attention #nlp 