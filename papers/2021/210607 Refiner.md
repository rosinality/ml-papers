https://arxiv.org/abs/2106.03714

Refiner: Refining Self-attention for Vision Transformers (Daquan Zhou, Yujun Shi, Bingyi Kang, Weihao Yu, Zihang Jiang, Yuan Li, Xiaojie Jin, Qibin Hou, Jiashi Feng)

트랜스포머 self attention의 attention map을 linear projection으로 확장하고 그 위에 dw conv를 옮긴 형태의 모델. 평소라면 자잘한 모듈을 붙였네 ㅎㅎ 했을 텐데 성능이 굉장하네요. 이미지넷 86%!

#vit #attention 