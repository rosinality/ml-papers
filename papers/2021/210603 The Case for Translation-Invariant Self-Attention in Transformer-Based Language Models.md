https://arxiv.org/abs/2106.01950

The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models (Ulme Wennberg, Gustav Eje Henter)