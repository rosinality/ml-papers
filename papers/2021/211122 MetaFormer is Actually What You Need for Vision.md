https://arxiv.org/abs/2111.11418

MetaFormer is Actually What You Need for Vision (Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, Shuicheng Yan)

요즘 나오는 이야기인 self attention이 아니라 transformer의 모델 선택들이 성능에 도움이 되고 있는 것이 아니냐는 이야기. 그래서 self attention을 빼버리고 average pooling으로 대체해서 돌려봤는데 성능이 꽤 잘 나오더라...입니다. object detection까지 넘어가도 성능이 나오네요. resnet 정도이긴 합니다만.

#vit #transformer 