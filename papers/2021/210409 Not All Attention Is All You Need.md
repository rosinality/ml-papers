https://arxiv.org/abs/2104.04692

Not All Attention Is All You Need (Hongqiu Wu, Hai Zhao, Min Zhang)

트랜스포머의 attention layer pruning인데...과제별 튜닝이기는 하지만 상당 수의 레이어를 날릴 수 있군요. 이런 결과가 이전에도 있었을 법 한데 기억은 안 나네요.

#transformer