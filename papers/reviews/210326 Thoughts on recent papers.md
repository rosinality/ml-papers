# 210326 최근 논문들에 대한 생각

역시나 efficient vision transformer는 몇 편 더 나왔다. local attention을 밀어붙인 사례 두 편과(https://arxiv.org/abs/2103.12731, https://arxiv.org/abs/2103.14030) vit에 fpn을 붙인 사례 한 편. (https://arxiv.org/abs/2103.13413) swin transformer는 roll로 쉽게 구현할 수 있어서 괜찮을 것 같다. 여러 트레이닝 레시피가 들어갔지만 이 백본으로 mAP 58을 넘겼다는 것도 흥미롭다.

물론 세상에 흥미로운 게 비전 트랜스포머만 있는 것은 아니고.

레이 대신 콘을 날려서 안티앨리어싱을 한 nerf (https://arxiv.org/abs/2103.13415)

요즘 많이 나오는 nerf 최적화 (700x: https://arxiv.org/abs/2103.13744, 3000x: https://arxiv.org/abs/2103.14024)

self supervised learning의 transfer/task 성능 측정 (https://arxiv.org/abs/2103.14005, https://arxiv.org/abs/2103.13517)

유명한 일본의 디텍션 장인의 universenet과 벤치마크 논문 (https://arxiv.org/abs/2103.14027)

효율적인 비디오 트랜스포머 (https://arxiv.org/abs/2103.13915)

비전에서 cnn을 트랜스포머로 대체하는 시점에 도리어 nlp에서 트랜스포머를 rnn으로 교체해보려고 하는 논문 (https://arxiv.org/abs/2103.13076)

프랙탈 매니아들의 프랙탈로 비전 트랜스포머 프리트레이닝시키기 (https://arxiv.org/abs/2103.13023)

minimax 대신 duality gap으로 gan 학습시키기 (https://arxiv.org/abs/2103.12685)

semantic segmentation을 위해 spinenet을 다시 서치한다거나 (https://arxiv.org/abs/2103.12270)

등등등.



#review