https://arxiv.org/abs/2004.10102

Attention is Not Only a Weight: Analyzing Transformers with Vector Norms (Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, Kentaro Inui)

Attention Module is Not Only a Weight: Analyzing Transformers with Vector Norms (Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, Kentaro Inui)

트랜스포머를 뜯어볼 때 어텐션 가중치만이 아니라 가중치가 곱해진 임베딩의 놈을 봐야 한다는 연구. 이 놈을 봤을 때 모델이 더 의미있는 방식으로 거동한다는 것이 나타남. 사실 임베딩의 놈이 균일할 것이라고 생각할 필요는 없긴 함. 어텐션 네 이놈!

#attention #bert